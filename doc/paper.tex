\documentclass[a4paper]{elsarticle}
% vim: tw=0 wm=0

\setcounter{tocdepth}{3}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{comment}
\usepackage{placeins}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
 \usepackage{booktabs}
\usepackage{array}
\usepackage[pdfencoding=auto,psdextra]{hyperref}
\usepackage{booktabs}
\usepackage{bookmark}% faster updated bookmarks
\usepackage{hypcap} % fix the links
\evensidemargin\oddsidemargin
\usepackage{graphicx}
\pagestyle{plain}
\usepackage{xcolor}
\newcommand\ToDo[1]{\textcolor{red}{#1}}
 
\usepackage{xspace}
\usepackage{color}
\usepackage{epsfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{changes}
  
\usepackage{blindtext}

\usepackage{numprint}
\npdecimalsign{.}
\npthousandsep{}

\usepackage[draft,nomargin,inline]{fixme}
\fxsetface{inline}{\itshape}
\fxsetface{env}{\itshape}
%\fxuselayouts{margin}
%\fxuselayouts{inline}
\fxusetheme{color}

\usepackage{url}
\newcommand{\keywords}[1]{\par\aDSvspace\baselineskip
	\noindent\keywordname\enspace\ignorespaces#1}

\usepackage{tikz}
\usetikzlibrary{positioning}
\definecolor{canaryyellow}{rgb}{1.0, 0.94, 0.0}
\definecolor{brightgreen}{rgb}{0.4, 1.0, 0.0}
\definecolor{jazzberryjam}{rgb}{0.65, 0.04, 0.37}

%defining of command

\newcommand\floor[1]{\lfloor#1\rfloor}
\newcommand\ceil[1]{\lceil#1\rceil}
\newcommand\str[1]{\texttt{#1}}
\newcommand\pL[1][]{\ensuremath{p^{\mathrm{L}#1}}}
\newcommand\pR[1][]{\ensuremath{p^{\mathrm{R}#1}}}
\newcommand\qL{\ensuremath{q^\mathrm{L}}}
\newcommand\qR{\ensuremath{q^\mathrm{R}}}
\newcommand\pLH{\ensuremath{\hat{p}^\mathrm{L}}}
\newcommand\pRH{\ensuremath{\hat{p}^\mathrm{R}}}
\newcommand{\Vext}{\ensuremath{V_\mathrm{{ext}}}}
\newcommand\UB{\ensuremath{\mathrm{UB}}}
\newcommand\Sigmand{\ensuremath{\Sigma^\mathrm{nd}}}
\newcommand{\mdmwnpp}{MDMWNPP\xspace}

\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\setlength{\leftmarginii}{1.8ex}
\raggedbottom
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

% scaling factor for tables
\newcommand\tabscale{0.8}
\newtheorem{Theorem}{Theorem}
\newtheorem{Lemma}{Lemma}

\begin{document}
	
	%\setlength{\parindent}{0pt}  % disallow indentations
	%\numberwithin{table}{1}
	%\mainmatter  % start of an individual contribution
	
	% first the title is needed
	\title{RILS-ROLS: Robust Symbolic Regression via Iterated Local Search and Ordinary Least Squares}
	
\author[1]{Aleksandar Kartelj}
\author[2]{Marko Djukanovi\'c}
	\address[1]{$kartelj@matf.bg.ac.rs$, \\  Faculty of Mathematics, University of Belgrade, Serbia}
    \address[2]{$ marko.djukanovic@pmf.unibl.org$,\\   Faculty of Natural Sciences and Mathematics, University of Banja Luka, Bosnia and Herzegovina}
	\begin{abstract}
		
	\end{abstract}
	\maketitle
	
	
\section{Introduction}\label{sec:introduction}
	
	The problem of symbolic regression (SR)~\cite{billard2002symbolic} has attracted many researchers over the last decade to study it intensively. SR can be seen as a generalization of the well known  concept of linear regression, i.e., polynomial regression~\cite{stimson1978interpreting}. All regression models in principle have the same task: given a set of a $n$-dimensional input data and the output data, the aim is to find a  mathematical expression (function) consisting of $n$ (input) variables that fits to the output data w.r.t. some in advance known measure. When aiming a model to be a linear combination of input variables, it represents the problem of linear regression. However, when there are some nonlinear relations between variables, which are relevant, linear regression models are not enough. This is the point where symbolic regression comes into the play. Unlike linear regression, it allows the search over the space of all possible mathematical formulas in order to find the best fitting ones able to  predict the output variable from the input variables. The base of constructing the explicit formula are the basis operations like addition and multiplication, as well as polynomial, trigonometric, exponential, and other elementary functions.  
	%Application: https://towardsdatascience.com/real-world-applications-of-symbolic-regression-2025d17b88ef
	
	
	 Concrete examples of application of SR include having a less black--box tool, as it may say much more how SR model achieve predictions, that is the coefficients and functions that build the model may indicate on larger importance of some variables over the others. Moreover, in that way we may grasp why the variables are related in the obtained way. As an example, appearance of an exponential function may associate  to  some physical phenomenon such as intensity of radiation or acceleration over time, see~\cite{udrescu2020ai}. Additionally, the SR models, if they are correct, may have    posses  larger extrapolation power than other mathematical models, and especially than neural network--based methods. SR models may maximize obtaining physically more reasonable equations that could serve as insight towards  establishing  physical theory behind as the final goal.  Practical applications of SR in chemical and biological sciences are listed in~\cite{weng2020simple}. In particular,  the discovery of a series of new oxide perovskite catalysts with improved activities is presented there. Applications of SR to discovering physical laws from distorted video is studied in~\cite{udrescu2021symbolic} by means of a unsupervised learning method. Revealing complex ecological dynamics by SR is presented in~\cite{chen2019revealing}, tackled by  a machine learning method. Application of SR to model the mutations effects on protein  stability, in the domain of fundamental and applied biology,  is shown in~\cite{louis2021reviewing}. One of the recent work studies of  auto-discovering conserved quantities using trajectory data from unknown dynamical systems, where SR is solved by a machine learning algorithm, see~\cite{liu2021machine}. Last but not least, we mention the paper~\cite{liang2019phillips} discovers  the application of  SR to model analytic representations of the exciton binding energy. The use of solving the SR in material science is described in~\cite{wang2019symbolic,wang2022symbolic,burlacu2022symbolic}. 
	 
	 
	 
	 There are many different ways to tackle the SR by achieving the right analytic expressions, some of them are more related to machine learning techniques, others exploiting the technique of genetic programming (GP) and various approximate methods such as meta-heuristics. Among the first GP methods to tackle SR is the one of Raidl~\cite{raidl1998hybrid} based on a hybrid variant of genetic programming. A differential evolution algorithm was proposed by Cerny et al.~\cite{cerny2008using}. Age-fitness Pareto Optimization approach is proposed by Smidt and Lipson~\cite{schmidt2010age}.  
	 Application of local--based heuristics on solving SR is discusses by Commenda in his Ph.D. thesis~\cite{kommenda2018local}. A  GP-based approach, the gene-pool optimal mixing evolutionary algorithm (GOMEA) is studied by Virgolin et al.~\cite{virgolin2021improving}.  Another evolutionary algorithm, the interaction-transformation EA (ITEA) has been proposed by de Franca et al.~\cite{de2021interaction}. Simulated annealing to solve SR is proposed by Kantor~\cite{kantor2021simulated}. 
	 
	 
	 Machine learning--based methodologies to solve SR are presented in~\cite{}... 
	
	
	Short descriptions of the SR methods used in our experiments are given in Table~\ref{} \fxnote{TODO}
\subsection{Contributions of this work}

The main contributions of this work can be summarized as follows:

\begin{enumerate}
	\item The proposed method outperforms all state-of-the-art methods for ground-truth problems from literature, considered in the SRBench benchmark. 
	
	\item It shows high robustness, which is proved by testing methods under different levels of Gaussian white noise. 
	
	\item The method is very efficient, taking in average around xyz seconds to reach exact solution -- when exact solution is found. 
	
	\item The new set of unbiased instances is introduced -- randomly generated formula of various sizes. All the other methods were tested against this new test-bed. The proposed RILS-ROLS method showed to be dominant here as well. 
\end{enumerate}


\section{Problem definition and search space}
\label{sec:search-space}

\section{Literature review}\label{sec:lit-rev}


\section{Proposed iterated local search method}\label{sec:rils}
   

\section{Experimental evaluation}\label{sec:experiments}

\subsection{Parameter tuning}
SAMO JEDAN PARAMETAR!!!

\subsection{Comparison with other methods}

\subsection{Statistical evaluation}

\section{Conclusions and future work}\label{sec:conclusions}
  
 \newpage
 \appendix
 
 \section{Complete results for a single random seed}\label{sec:appendix-1}
 
 \section{Overview of RILS-ROLS python package}\label{sec:appendix-2}
 
 \section{Analysis within SRBench}
 
 
\newpage
%\section*{References}
\bibliographystyle{abbrv}	
\bibliography{bib}	

	
\end{document}
