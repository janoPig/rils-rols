\documentclass[a4paper,12pt]{elsarticle}
% vim: tw=0 wm=0

\setcounter{tocdepth}{3}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{comment}
\usepackage{placeins}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
 \usepackage{booktabs}
\usepackage{array}
\usepackage[pdfencoding=auto,psdextra]{hyperref}
\usepackage{booktabs}
\usepackage{bookmark}% faster updated bookmarks
\usepackage{hypcap} % fix the links
\evensidemargin\oddsidemargin
\usepackage{graphicx}
\pagestyle{plain}
\usepackage{xcolor}
\newcommand\ToDo[1]{\textcolor{red}{#1}}
 \usepackage{tabularx}
\usepackage{xspace}
\usepackage{color}
\usepackage{epsfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{changes}
\usepackage{tikz}
\usepackage{fullpage}
\usepackage{calc}
\usetikzlibrary{positioning,shadows,arrows,trees,shapes,fit}
\usepackage{blindtext}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}

\usepackage{numprint}
\npdecimalsign{.}
\npthousandsep{}

\usepackage[draft,nomargin,inline]{fixme}
\fxsetface{inline}{\itshape}
\fxsetface{env}{\itshape}
%\fxuselayouts{margin}
%\fxuselayouts{inline}
\fxusetheme{color}

\usepackage{url}
\newcommand{\keywords}[1]{\par\aDSvspace\baselineskip
	\noindent\keywordname\enspace\ignorespaces#1}

\usepackage{tikz}
\usetikzlibrary{positioning}
\definecolor{canaryyellow}{rgb}{1.0, 0.94, 0.0}
\definecolor{brightgreen}{rgb}{0.4, 1.0, 0.0}
\definecolor{jazzberryjam}{rgb}{0.65, 0.04, 0.37}

%defining of command

\newcommand\floor[1]{\lfloor#1\rfloor}
\newcommand\ceil[1]{\lceil#1\rceil}
\newcommand\str[1]{\texttt{#1}}
\newcommand\pL[1][]{\ensuremath{p^{\mathrm{L}#1}}}
\newcommand\pR[1][]{\ensuremath{p^{\mathrm{R}#1}}}
\newcommand\qL{\ensuremath{q^\mathrm{L}}}
\newcommand\qR{\ensuremath{q^\mathrm{R}}}
\newcommand\pLH{\ensuremath{\hat{p}^\mathrm{L}}}
\newcommand\pRH{\ensuremath{\hat{p}^\mathrm{R}}}
\newcommand{\Vext}{\ensuremath{V_\mathrm{{ext}}}}
\newcommand\UB{\ensuremath{\mathrm{UB}}}
\newcommand\Sigmand{\ensuremath{\Sigma^\mathrm{nd}}}
\newcommand{\mdmwnpp}{MDMWNPP\xspace}

\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\setlength{\leftmarginii}{1.8ex}
\raggedbottom
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

% scaling factor for tables
\newcommand\tabscale{0.8}
\newtheorem{definition}{Definition}
\newtheorem{Lemma}{Lemma}

\begin{document}
	
	%\setlength{\parindent}{0pt}  % disallow indentations
	%\numberwithin{table}{1}
	%\mainmatter  % start of an individual contribution
	
	% first the title is needed
	\title{RILS-ROLS: Robust Symbolic Regression via Iterated Local Search and Ordinary Least Squares}
	
\author[1]{Aleksandar Kartelj}
\author[2]{Marko Djukanovi\'c}
	\address[1]{$kartelj@matf.bg.ac.rs$, \\  Faculty of Mathematics, University of Belgrade, Serbia}
    \address[2]{$ marko.djukanovic@pmf.unibl.org$,\\   Faculty of Natural Sciences and Mathematics, University of Banja Luka, Bosnia and Herzegovina}
	\begin{abstract}
		  \fxnote{TODO}
	\end{abstract}
	\maketitle
	
	
\section{Introduction}\label{sec:introduction}
	
	The problem of symbolic regression (SR)~\cite{billard2002symbolic} has attracted many researchers over the last decade to study it intensively. SR can be seen as a generalization of the well known  concept of linear regression, i.e., polynomial regression~\cite{stimson1978interpreting}. All regression models in principle have the same task: given a set of a $n$-dimensional input data and the output data, the aim is to find a  mathematical expression (function) consisting of $n$ (input) variables that fits to the output data w.r.t. some in advance known measure.  This computationally intensive task is in general  provenly NP--hard~\cite{virgolin2022symbolic}. When aiming a model to be a linear combination of input variables, it represents the problem of linear regression. However, when there are some nonlinear relations between variables, which are relevant, linear regression models are not enough. This is the point where symbolic regression comes into the play. Unlike linear regression, it allows the search over the space of all possible mathematical formulas in order to find the best fitting ones able to  predict the output variable from the input variables. The base of constructing the explicit formula are the basis operations like addition and multiplication, as well as polynomial, trigonometric, exponential, and other elementary functions.  
	%Application: https://towardsdatascience.com/real-world-applications-of-symbolic-regression-2025d17b88ef
	
	
	 Concrete examples of application of SR include having a less black--box tool, as it may say much more how SR model achieve predictions, that is the coefficients and functions that build the model may indicate on larger importance of some variables over the others. Moreover, in that way we may grasp why the variables are related in the obtained way. As an example, appearance of an exponential function may associate  to  some physical phenomenon such as intensity of radiation or acceleration over time, see~\cite{udrescu2020ai}. Additionally, the SR models, if they are correct, may have    posses  larger extrapolation power than other mathematical models, and especially than neural network--based methods. SR models may maximize obtaining physically more reasonable equations that could serve as insight towards  establishing  physical theory behind as the final goal.  Practical applications of SR in chemical and biological sciences are listed in~\cite{weng2020simple}. In particular,  the discovery of a series of new oxide perovskite catalysts with improved activities is presented there. Applications of SR to discovering physical laws from distorted video is studied in~\cite{udrescu2021symbolic} by means of a unsupervised learning method. Revealing complex ecological dynamics by SR is presented in~\cite{chen2019revealing}, tackled by  a machine learning method. Application of SR to model the mutations effects on protein  stability, in the domain of fundamental and applied biology,  is shown in~\cite{louis2021reviewing}. One of the recent work studies of  auto-discovering conserved quantities using trajectory data from unknown dynamical systems, where SR is solved by a machine learning algorithm, see~\cite{liu2021machine}. Last but not least, we mention the paper~\cite{liang2019phillips} discovers  the application of  SR to model analytic representations of the exciton binding energy. The use of solving the SR in material science is described in~\cite{wang2019symbolic,wang2022symbolic,burlacu2022symbolic,kabliman2021application}. Yet another application to wind speed forecasting is given in~\cite{abdellaoui2021symbolic}. 
	 
	 
	 
	 There are many different ways to tackle the SR by achieving the right analytic expressions, most of them are related to machine learning techniques or to the technique of genetic programming (GP) and various approximate methods, such as meta-heuristics. Among the first GP methods to tackle SR is the one of Raidl~\cite{raidl1998hybrid} based on a hybrid variant of genetic programming. A differential evolution algorithm was proposed by Cerny et al.~\cite{cerny2008using}. Age-fitness Pareto Optimization approach is proposed by Smidt and Lipson~\cite{schmidt2010age}.  Application of artificial bee colony programming to solve SR is proposed by Karaboga et al.~\cite{karaboga2012artificial}. 
	 Influence of local--based heuristics on solving SR is reported by Commenda in his Ph.D. thesis~\cite{kommenda2018local}. A  GP-based approach, the gene-pool optimal mixing evolutionary algorithm (GOMEA) is studied by Virgolin et al.~\cite{virgolin2021improving}.  Another evolutionary algorithm, the interaction-transformation EA (ITEA) has been proposed by de Franca et al.~\cite{de2021interaction}. Simulated annealing to solve SR is proposed by Kantor~\cite{kantor2021simulated}. A Variable neighborhood programming approach to solve SR  is proposed by Elleurich et al.~\cite{elleuch2020variable}; this technique is initially proposed in~\cite{elleuch2016variable}. 
	 Kommenda et al.~\cite{kommenda2020parameter} proposed a method called OPERON algorithm, which uses nonlinear least squares for parameter identification of SR models further integrated into a local search mechanism in
	 tree-based GP. The C++ implementation of OPERON is discussed in~\cite{burlacu2020operon}. The method that uses Taylor polynomial to approximate the symbolic equation that fits the dataset, called Taylor genetic programming, is proposed in~\cite{he2022taylor}. A GP approach that uses the idea of semantic back-propagation (SBP-GP) is proposed in~\cite{virgolin2019linear}.   Empirical analysis of variance between many GP--based methods for SR is discussed in~\cite{kammerer2021empirical}. It is also worth to mention the GP--based Eurequa commercial solver~\cite{schmidt2009distilling, schmidt2011machine} that uses age-fitness pareto optimization  with co-evolved fitness estimation. This solver is nowadays accepted for the gold standard of symbolic regression.   
	 
	 
	 Machine learning--based method   based on Bayesian symbolic regression (BSR) is proposed in~\cite{jin2019bayesian},  belongs to the family of Markov Chain Monte Carlo algorithms. Deep Symbolic Regression (DSR), a RNN approach, is proposed in~\cite{petersen2019deep} utilizing the policy gradient search. This mechanism of search is further investigated in~\cite{landajuela2021improving}. A fast neural network approach, called OccamNet,  is proposed in~\cite{costa2020fast}.  A deep reinforcement learning approach enhanced with genetic programming is given in~\cite{mundhenk2021symbolic}. 
	
	Powerful hybrid techniques to solve SR are also well studied; among them, we emphasize the EPLEX solver from~\cite{la2019probabilistic,la2016epsilon} and AI Feynman algorithm from~\cite{udrescu2020ai}, a physics-inspired divide-and-conquer method that  combines neural network
	fitting. The later one is one of the most efficient method for physically inspired models. We also mention the Fast Function Extraction (FFX) algorithm developed by McConaghy~\cite{mcconaghy2011ffx},  a non-evolutionary method in combination with a machine learning technique---path-wise regularized learning--- which   quickly prune a huge set of candidate basis functions down to compact models.
	
	A short chronological overview of the most important  literature methods to solve SR are given in Table~\ref{tab:gp-based}. 
	
	
	 \begin{table}[!ht]
		\centering
		\scalebox{0.7}{
		\begin{tabularx}{550pt}{l  l  X}  
			Algorithm          &   Paper/year &   Short details   \\ \hline
			GP         &    \cite{koza1994genetic} (1994)    & Application of GP to SR \\ 
	        Hybrid GP  &   \cite{raidl1998hybrid} (1998)   & GP--based method; solutions locally optimizes by method of least squares to find optimum coefficients for top-level terms \\ 
	        DFE        & \cite{cerny2008using} (2008)   & Differential evolution algorithm \\
	        Eurequa                & \cite{schmidt2009distilling, schmidt2011machine} (2009, 2011) & Age-fitness pareto optimization approach using co-evolved fitness estimation   \\
        	APF            &      \cite{schmidt2010age} (2010)        &  Age-fitness pareto optimization approach                        \\
        	FFX  & \cite{mcconaghy2011ffx} (2011)  & The Fast Function Extraction algorithm --  non-evolutionary technique based on a machine learning technique called path-wise regularized learning \\
             ABCP &  \cite{karaboga2012artificial} (2012)  & Artificial bee colony programming  approach \\
			EPLEX          &      \cite{la2016epsilon} (2016)         &   A parent selection method called $\epsilon$--lexicase selection  \\ 
	       Local optimization     & \cite{kommenda2018local} (2018)   & Constants Optimization in GP by Nonlinear Least Square \\
	       SBP-GP &  \cite{virgolin2019linear} (2019)  & The idea of semantic back-propagation utilized in GP \\
	       BSR & \cite{jin2019bayesian} (2019) & ML--based approach; Bayesian symbolic regression  \\ 
	       DSR  & \cite{petersen2019deep} (2019) & Deep Symbolic Regression based on a RNN approach further utilizing the policy gradient search \\
	       OPERON & \cite{kommenda2020parameter} (2020) &  Utilizing nonlinear least squares for parameter identification of SR models with LS \\
	       VNP    & \cite{elleuch2020variable} (2020) & VNS based GP approach \\
	       OccamNet & \cite{costa2020fast} (2020) &   A fast neural network approach; the model defines a probability distribution over a non-differentiable function space; it samples functions and updates the weights with back-propagation  based on cross-entropy matching in an EA strategy	 \\
	       
	       AI Feynman  & \cite{udrescu2020ai} (2020) & A physics-inspired divide-and-conquer method; it  combines neural network fitting \\
	             GOMEA  & \cite{virgolin2021improving} (2021)    & A model-based
	       EA framework called Gene-pool optimal mixing evolutionary algorithm \\
	       ITEA & \cite{de2021interaction} (2021)   & EA based approach called the interaction-transformation EA   \\
	       SA & \cite{kantor2021simulated} (2021) &  Simulated annealing approach \\
	       
         DRLA & \cite{mundhenk2021symbolic} (2021)  &    A deep reinforcement learning approach enhanced with genetic programming \\
            Taylor GP &  \cite{he2022taylor} (2022)  &  Taylor polynomials approximations  \\ \hline
         

	\end{tabularx} }
		\caption{Overview of methods to solve SR.}
		\label{tab:gp-based}
	\end{table}
 
  
	The progress in the SR field suffered from a lack of uniform, robust, and transparent
	benchmarking standards over the last two decade. Recently, La Cava et al.~\cite{la2021contemporary} proposed an
	open-source, reproducible benchmarking platform for SR, called SRBench. The authors extended  PMLB~\cite{olson2017pmlb},  a repository of standardized regression, by 130 SR datasets for which model forms are known; see more in the aforementioned paper. In the extensive experimental evaluation  where 14
	symbolic regression methods and 7 machine learning methods are compared on the  set of 252 diverse
    regression problems. The general conclusions derived from there are: ($i$) when comes to real-world problem, the best performing algorithms are those which combine genetic algorithm with parameter estimation; ($ii$) when come to the instances with the presence of noise, the GP--based and deep learning techniques are performing similarly. 
    
    In this work we present a novel approach to solve SR, which combines the popular iterated local search meta-heuristic (ILS)~\cite{lourencco2003iterated,lourencco2019iterated} with ordinary least squares method (OLS)~\cite{leng2007ordinary}. Briefly, ILS handles combinatorial (discrete) aspects of search space, while OLS helps in the process of coefficient determination, so it handles continuous parts of the search space. As will be shown later, the proposed method seems to be very robust w.r.t. introduced noise, therefore we called the method RILS-ROLS (with letters R corresponding to regression and robust).  \fxnote{TODO: maybe expand the core of the algorithm... }
 
The main contributions of this work may be summarized as follows:

\begin{enumerate}
	\item The proposed method outperforms all state-of-the-art methods for ground-truth problems from literature, considered in the SRBench benchmark. 
	
	\item It shows high robustness, which is proved by comparison to other algorithms under different levels of Gaussian white noise. 
	
	\item The method is very efficient, taking in average around xyz seconds to reach exact solution -- when exact solution is found. 
	
	\item The new set of unbiased instances is also introduced -- randomly generated formula of various sizes and number of input variables. This set was employed to understand the influence of formula size, number of input variables and noise on the solving difficulty. 
\end{enumerate}


\section{Problem definition and search space}
\label{sec:search-space}
In this section we formally define the SR problem, followed by defining the search space framework of the problem. 

\begin{definition}
  Given is a dataset $D = \{(x_i, y_i)\}_{i=1}^n$, where $\textbf{x} \in \mathbb{R}^d$ represents input variables (features), and $y$ target variable. Suppose that there exists an analytical model of the form $f(\textbf{x})= g^*(\textbf{x}, \theta^*) + \epsilon $ that is a generator of all observations from $D$.  
   The goal of SR is to learn a mapping $\tilde{f}(\textbf{x})=  \tilde{g}(\textbf{x}, \tilde{\theta})  \colon \mathbb{R}^d \mapsto \mathbb{R}$  estimated by searching through the space of (mathematical) expressions  $\tilde{g}$ and parameters $\tilde{\theta}$ where  $\epsilon$ is the presented  white noise using given input data. 
  
\end{definition}

Koza~\cite{koza1994genetic} introduced the problem of SR as a specific application of genetic programming. GP deals with the object called programs, which need to be optimized. In particular, the programs are represented by syntax trees consisting of functions/operations over input features and constants; as an example of a function represented by a syntax tree, see Figure~\ref{fig:syntax-tree-example} In essence, syntax trees are elements of the search space of SR. That is, each sample model $\tilde{f}$ may be seen as a point in the search space, represented by respective syntax tree. How accurate is this mapping, we compute in the basis of the historical data $D$ and the chosen error   measure (such as $MSE$, or $R^2$). Interestingly, the search space of SR consists of its discrete and continuous part. More precisely, the model we are seeking for might be obtained as a composition of some functions from a (finite) set of mathematical functions. It is common to use the set of the following elementary mathematical functions in the experiments: $\sqrt{x}, x^2 $, $\sin$, $\cos$, $\log$, $\exp$, $\arcsin$, $\arccos$, and $a^x$. It is allowed to link functions will all four operators: $+$, $-$, $\times$, and $/$. After fixing functions that build models, the set of optimal coefficients that  fits to this model must be provided. For the most physical models, these coefficients may receive any real value, that is they are continuous in its nature. 


\fxnote{TODO: needs for extension...}

\begin{figure}[!ht]
	\centering
\begin{tikzpicture}
	[font=\small, edge from parent, 
	every node/.style={top color=white, bottom color=blue!25, 
		rectangle,rounded corners, minimum size=6mm, draw=blue!75,
		very thick, drop shadow, align=center},
	edge from parent/.style={draw=blue!50,thick},
	level 1/.style={sibling distance=3cm},
	level 2/.style={sibling distance=1.2cm}, 
	level 3/.style={sibling distance=1cm}, 
	level distance=2cm,
	]
	\node (A) {$\times$} 
	child { node (B) {$\sin$}
		%child { node {x} 
			%edge from parent node[left=.5em,draw=none] {} }
		child { node {$x$}}
	}
	child {node (C) {$\times$}
		%child { node {x}
		%	child { node {C1a}}
		%}
		child { node {$x$}}
		child { node {$/$}
			child { node {$x$}}
			child { node {$y$} %edge from parent node[right=.5em,draw=none] {$\frac{a}{b}$}
			}
		}
	};
%	child { node {+} 
	%	child { node {2}}
	%	child { node {$x$}}
	%};
	
\end{tikzpicture}

\caption{Syntax tree representation for the expression $\sin x \times   ( x \times  x / y  )  = \frac{x^2 \sin x }{y}$}
\label{fig:syntax-tree-example}
\end{figure}

%\section{Literature review}\label{sec:lit-rev}


\section{The proposed method}\label{sec:rils}
   Before we give details on our RILS-ROLS method for solving SR, we will explain the basics of two main techniques incorporated into our algorithm. These are ILS meta-heuristic and OLS methods. 
   
  \subsection{Iterated local search}
    Iterated local search (ILS)~\cite{lourencco2003iterated} is an efficient meta-heuristics, whose essential idea is given as follow. Iteratively generate a sequence of solutions produced by the (embedded) heuristic, such as local search or randomized greedy heuristics. When the search gets \emph{stuck} in local optimum, a \emph{perturbation} is performed -- this step is usually non-deterministic. This simple idea appeared in the literature by Baxter~\cite{baxter1981local} in early 1980s, and, since then, has been  re-invented by many researches under different names; some of the following names were used: iterated descent search~\cite{baum1998iterated}, large-step
    Markov chains~\cite{martin1991large}, chained local optimization~\cite{martin1996combining}, or, in some cases, combinations of these~\cite{applegate2003chained}. 
  
   The most popular version of ILS is shown in Algorithm~\ref{alg:ils}. Note that we use this version of ILS as the backbone of our RILS-ROLS algorithm.
   
   \begin{algorithm}
  		\begin{algorithmic}[1] 
  			 \State \textbf{Input}: problem instance
  	 		\State \textbf{Output}: an approximate (feasible) solution 
  	 		\State $s \gets$ \texttt{Initialize}$()$
  	 		\State  $s \gets$ \texttt{LocalSearch}($s$)
   	 		\While{\emph{stopping criteria is not met}}
   	    		 \State  $s' \gets$ \texttt{Perturbation}($s$)
   	    		 \State  $s' \gets$ \texttt{LocalSearch}($s'$)
   	    		 \State  $ s \gets$ \texttt{AcceptanceCriterion}($s, s'$)
   	 		\EndWhile
   	 		\State \Return $s$
   		\end{algorithmic}
    	\caption{An ILS framework from   literature.}
    	   	     \label{alg:ils}
    \end{algorithm}  
   
An initial solution may be generated randomly or by using a greedy heuristic, which is afterwards, improved by local search. At each iteration, ILS applies three steps. First, the current incumbent solution is perturbed -- it is partially randomized. Next, the perturbed solution is improved with LS procedure. Thirdly, the LS outcome is thereafter compared to the incumbent solution and the search us possibly moved to the new solution, with accordance to the chosen acceptance criterion. Sometimes, ILS incorporates a mechanism of tabu list, which prevents the search getting back to already visited solutions. 
   
 \subsection{Ordinary least square method}
   Ordinary least square method (OLS) is a linear regression technique. It is based on applying the least-square method to minimize the square residual (error) sum  between actual and predicted values (given by the model). More precisely, given data $({X}, y)$, the task is to determine linear mapping $\tilde{y} = k \textbf{x} + b$, that is coefficients (line slope) $k = (k_1, \ldots, k_d)$ and $b$ (intercept), so that $ \sum_{i} |\tilde{y}_i - y_i|^2 $ is minimized. This sum is also know as the sum of squared error (SSE). There are many methods to minimize SSE. One of the analytical approaches is the calculus-oriented -- it takes into account   partial derivation w.r.t. $k$ of the cost function $ \sum_{i} |\tilde{y}_i - y_i|^2 $, getting 
   
   $$  \frac{\partial}{\partial k} \sum_{i} |\tilde{y}_i - y_i|^2 = \frac{\partial}{\partial k} \sum ( kx_i+b  - y_i)^2 =  \sum -2x_i(kx_i + b - y_i)  =  \sum -2x_i (\tilde{y}_i - y_i ) . $$
   
   Taking into account   partial derivation w.r.t.  $b$, we have 
    $$  \frac{\partial}{\partial b} \sum_{i} (\tilde{y}_i - y_i)^2  = \frac{\partial}{\partial b} \sum ( kx_i+b  - y_i)^2 = \sum -2  (\tilde{y}_i - y_i). $$
    
    Thus, we get the system of equation: 
    \begin{align*}
    	  &\sum -2x_i (\tilde{y}_i - y_i )  = 0 \\
    	  &\sum -2  (\tilde{y}_i - y_i ) = 0.
    \end{align*}
     The above system can be transformed to 
     \begin{align*}
     	 & \sum y_i \cdot x_i   = b\sum x_i \cdot x_i  +  k \cdot \sum x_i \\
     	 & \sum y_i  = k \sum x_i + b \cdot n. 
     \end{align*}
     By solving this system of equations, one can easily obtain vector of coefficients $k=(k_1, ..., k_d)$ and intercept $b$. \fxnote{TODO: Aca -- ovde je pisalo da je to sistem sa dve jednacine i dve nepoznate k i b -- medjutim, k je vektor. Ako mozes dopisi jos nesto, tipa koliko je podataka potrebno i slicno -- mozda veremenska slozenost.) }
   \subsection{RILS-ROLS method}
   Now we will explain the proposed RILS-ROLS method in detail. The overall method scheme is given in Algorithm~\ref{alg:rilsrols}.  
   
    \begin{algorithm}
   	\begin{algorithmic}[1] 
   		\State \textbf{Input}: input training dataset $D_{tr}$
   		\State \textbf{Output}: best symbolic formula solution $bs$
   		\State $n_{tr} \gets |D_{tr}|$
   		\State $sample_{size} \gets max(100, 0.01 \times n_{tr})$
   		\State $D_{tr}' \gets$ \texttt{Sample}($D_{tr}, sample_{size}$)
   		\State $s \gets NodeConstant(0)$ 
   		\State $s_{fit} \gets$ \texttt{Fitness}($s, D_{tr}'$)
   		\State $bs, bs_{fit} \gets s, s_{fit}$ 
   		\State $start_{tried}, perturbations_{tried} \gets \emptyset, \emptyset$
   		\While{\emph{stopping criteria is not met}}
   			\State $start_{tried} \gets start_{tried} \cup \{s\}$
   			\State $s_{perturbations} \gets $ \texttt{GeneratePerturbations}($s$, 1)
   			\State $s_{perturbations} \gets $ \texttt{FitOLS}($s_{perturbations}$)
   			\State $s_{perturbations} \gets $ \texttt{OrderByR2}($s_{perturbations}$)
   			\State $improved \gets false $
   			\For{$p \in s_{perturbations}$}
   				\If{$p \in perturbations_{tried}$}
   					\State \textbf{continue}
   				\EndIf
  				\State $p \gets $ \texttt{Simplify}($p$)
  				\State $p \gets $ \texttt{LocalSearch}($p$)
  				\State $p_{fit} \gets$ \texttt{Fitness}($p$)
  				\If{$p_{fit} < bs_{fit}$}
  					\State $bs, bs_{fit}, improved \gets p, p_{fit}, true$ // new best solution
  					\State \textbf{break}
  				\EndIf
  				\State $perturbations_{tried} \gets perturbations_{tried} \cup \{p\}$
   			\EndFor
   			\If{not improved}
   				\State $start_{candidates} \gets $ \texttt{GeneratePerturbations}($bs$, 1)
   				\If{$start_{candidates} \cap start_{tried} = \emptyset$} // all 1-perturbations around $bs$ tried
   					\State $start_{candidates} \gets $ \texttt{GeneratePerturbations}($bs$, 2) 
   				\EndIf
   				\State $s \gets $ \texttt{RandomPick}($start_{candidates} \setminus start_{tried}$)
   				\If{not improved for too many iterations}
   					\State $sample_{size} \gets$ \texttt{IncreaseSampleSize}($sample_{size}, n_{tr}$)
   					\State $D_{tr}' \gets$ \texttt{Sample}($D_{tr}, sample_{size}$)
   				\EndIf
   			\EndIf
   			\If{$R^2$ almost 1 and RMSE almost 0}
   				\State \textbf{break} // early exit
   			\EndIf
   		\EndWhile
   		\State \Return $bs$
   	\end{algorithmic}
   	\caption{RILS-ROLS algorithm.}
   	\label{alg:rilsrols}
   \end{algorithm}  
   
\section{Experimental evaluation}\label{sec:experiments}

\subsection{Parameter tuning}
SAMO JEDAN PARAMETAR!!!

\subsection{Comparison with other methods}

\begin{table}[!htb]
	\caption{Comparison without noise}\label{tab:comp_noise0}
	\centering
		\begin{tabular}{l|rrr|rrr} \hline
			& \multicolumn{3}{c|}{Exact model percentage} & \multicolumn{3}{c}{$R^2 > 0.999$ percentage}\\ \hline
			Method & Feynman & Strogatz & Total & Feynman & Strogatz & Total \\ \hline
			RILS-ROLS&\bf{57.84}&\bf{83.57}&\bf{60.62}&82.59&90&83.38\\
			AIFeynman&55.78&27.14&52.69&78.51&35.71&73.9\\
			GP-GOMEA&26.83&29.46&27.12&71.55&71.43&71.54\\
			AFP\_FE&26.98&20&26.23&59.05&28.57&55.77\\
			ITEA&22.41&7.14&20.77&27.59&21.43&26.93\\
			AFP&21.12&15.18&20.48&44.83&25&42.69\\
			DSR&19.72&19.64&19.71&25&14.29&23.85\\
			Operon&16.55&11.43&16&86.21&\bf{92.86}&86.93\\
			gplearn&16.27&8.93&15.48&32.76&7.14&30\\
			SBP-GP&12.72&11.61&12.6&73.71&78.57&74.23\\
			EPLEX&12.39&8.93&12.02&46.98&21.43&44.23\\
			BSR&2.48&0.89&2.31&10.78&21.43&11.93\\
			FEAT&0&0.89&0.1&39.66&42.86&40\\
			FFX&0&0&0&0&0&0\\
			MRGP&0&0&0&\bf{93.1}&89.29&\bf{92.69}\\
			\hline
		\end{tabular}
\end{table}

\begin{table}[!htb]
	\caption{Comparison with noise level 0.001}\label{tab:comp_noise0001}
	\centering
		\begin{tabular}{l|rrr|rrr} \hline
			& \multicolumn{3}{c|}{Exact model percentage} & \multicolumn{3}{c}{$R^2 > 0.999$ percentage}\\ \hline
			Method & Feynman & Strogatz & Total & Feynman & Strogatz & Total \\ \hline
			RILS-ROLS&\bf{42.93}&\bf{35}&\bf{42.08}&80.26&65&78.62\\
			DSR&16.81&18.75&17.02&26.29&14.29&25\\
			gplearn&15.84&8.93&15.1&24.14&7.14&22.31\\
			AFP\_FE&15.95&6.43&14.92&57.76&28.57&54.62\\
			AFP&14.87&7.14&14.04&44.83&25&42.69\\
			EPLEX&10.02&2.68&9.23&48.71&21.43&45.77\\
			AIFeynman&1.49&13.73&2.81&9.73&35.71&12.53\\
			ITEA&1.94&4.46&2.21&26.72&21.43&26.15\\
			GP-GOMEA&1.62&0.89&1.54&72.84&71.43&72.69\\
			Operon&0.34&0&0.31&\bf{81.9}&\bf{92.86}&\bf{83.08}\\
			FFX&0.11&0&0.1&19.4&14.29&18.85\\
			FEAT&0&0.89&0.1&13.79&50&17.69\\
			SBP-GP&0&0&0&62.07&53.57&61.15\\
			BSR&0&0&0&7.76&14.29&8.46\\
			MRGP&0&0&0&0.86&17.86&2.69\\
			\hline
		\end{tabular}
\end{table}

\begin{table}[!htb]
	\caption{Comparison with noise level 0.01}\label{tab:comp_noise001}
	\centering
	\begin{tabular}{l|rrr|rrr} \hline
		& \multicolumn{3}{c|}{Exact model percentage} & \multicolumn{3}{c}{$R^2 > 0.999$ percentage}\\ \hline
		Method & Feynman & Strogatz & Total & Feynman & Strogatz & Total \\ \hline
		RILS-ROLS&\bf{36.29}&\bf{22.14}&\bf{34.77}&\bf{64.91}&\bf{46.43}&\bf{62.92}\\
		gplearn&13.21&8.93&12.75&14.22&7.14&13.46\\
		DSR&8.41&16.07&9.23&17.24&14.29&16.92\\
		EPLEX&8.77&0&7.83&14.22&3.57&13.07\\
		AFP&7.11&0.89&6.44&8.19&7.14&8.08\\
		AFP\_FE&6.12&3.57&5.85&5.6&0&5\\
		ITEA&0.11&0.89&0.19&1.72&10.71&2.69\\
		AIFeynman&0&0.79&0.09&0&0&0\\
		Operon&0.09&0&0.08&0&0&0\\
		GP-GOMEA&0&0&0&0&0&0\\
		SBP-GP&0&0&0&0&0&0\\
		BSR&0&0&0&0&0&0\\
		FEAT&0&0&0&0&14.29&1.54\\
		FFX&0&0&0&0&0&0\\
		MRGP&0&0&0&0&0&0\\
		\hline
	\end{tabular}
\end{table}

\begin{center}
 \begin{tikzpicture}
 	\begin{axis}[
		xlabel=$size$,
		ylabel=$exact\;percentage$,
		xmin=0, xmax=4,
		ymin=0, ymax=100,
		xtick={1,2,3},
		xticklabels={small,medium,large},   % <---
		ytick={0,10,...,100}
		]
		\addplot[smooth,mark=*,blue] plot coordinates {
			(1,85)
			(2,42.5)
			(3,21.05)
		};
		\addlegendentry{No noise}
		
		\addplot[smooth,color=red,mark=x]
		plot coordinates {
			(1,51.67)
			(2,13.75)
			(3,5.26)
		};
		\addlegendentry{Noise level 0.001}
		
		\addplot[smooth,color=green,mark=o]
		plot coordinates {
			(1,31.67)
			(2,5)
			(3,4.21)
		};
		\addlegendentry{Noise level 0.01}
	\end{axis}
\end{tikzpicture}
\captionof{figure}{Exact solution percentages for varying levels of noise and formulae sizes}
\label{fig:compExact_noise_size}
\end{center}

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel=$size$,
			ylabel=$R^2 > 0.999\;percentage$,
			xmin=0, xmax=4,
			ymin=20, ymax=100,
			xtick={1,2,3},
			xticklabels={small,medium,large},   % <---
			ytick={20,30,...,100}
			]
			\addplot[smooth,mark=*,blue] plot coordinates {
				(1,98.33)
				(2,62.5)
				(3,58.95)
			};
			\addlegendentry{No noise}
			
			\addplot[smooth,color=red,mark=x]
			plot coordinates {
				(1,93.33)
				(2,53.75)
				(3,40)
			};
			\addlegendentry{Noise level 0.001}
			
			\addplot[smooth,color=green,mark=o]
			plot coordinates {
				(1,65)
				(2,31.25)
				(3,20)
			};
			\addlegendentry{Noise level 0.01}
		\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Percentages of solutions having $R^2 > 0.999$ for varying levels of noise and formulae sizes}
	\label{fig:compR2_noise_size}
\end{center}

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel=$variable\;count$,
			ylabel=$exact\;percentage$,
			xmin=0, xmax=5,
			ymin=0, ymax=100,
			xtick={1,2,3,4},
			xticklabels={1,2,3,4},   % <---
			ytick={0,10,...,100}
			]
			\addplot[smooth,mark=*,blue] plot coordinates {
				(1,43.33)
				(2,43.33)
				(3,46.67)
				(4,46.67)
			};
			\addlegendentry{No noise}
			
			\addplot[smooth,color=red,mark=x]
			plot coordinates {
				(1,3.33)
				(2,3.33)
				(3,26.67)
				(4,26.67)
			};
			\addlegendentry{Noise level 0.001}
			
			\addplot[smooth,color=green,mark=o]
			plot coordinates {
				(1,3.33)
				(2,3.33)
				(3,3.33)
				(4,16.67)
			};
			\addlegendentry{Noise level 0.01}
		\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Exact solution percentages for varying levels of noise and variable counts (only sizes ranging from 7 to 12)}
	\label{fig:compExact_noise_varcnt}
\end{center}

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel=$variable\;count$,
			ylabel=$exact\;percentage$,
			xmin=0, xmax=5,
			ymin=0, ymax=100,
			xtick={1,2,3,4},
			xticklabels={1,2,3,4},   % <---
			ytick={0,10,...,100}
			]
			\addplot[smooth,mark=*,blue] plot coordinates {
				(1,83.33)
				(2,66.67)
				(3,73.33)
				(4,46.67)
			};
			\addlegendentry{No noise}
			
			\addplot[smooth,color=red,mark=x]
			plot coordinates {
				(1,56.67)
				(2,63.33)
				(3,66.67)
				(4,46.67)
			};
			\addlegendentry{Noise level 0.001}
			
			\addplot[smooth,color=green,mark=o]
			plot coordinates {
				(1,23.33)
				(2,43.33)
				(3,53.33)
				(4,26.67)
			};
			\addlegendentry{Noise level 0.01}
		\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Percentages of solutions having $R^2 > 0.999$ for varying levels of noise and variable counts (only sizes ranging from 7 to 12)}
	\label{fig:compR2_noise_varcnt}
\end{center}


\subsection{Statistical evaluation}

\section{Conclusions and future work}\label{sec:conclusions}
  
 \newpage
 \appendix
 
 \section{Complete results for a single random seed}\label{sec:appendix-1}
 
 \section{Overview of RILS-ROLS python package}\label{sec:appendix-2}
 
 \section{Analysis within SRBench}
  
  
  
 
\newpage
%\section*{References}
\bibliographystyle{abbrv}	
\bibliography{bib}	

	
\end{document}
