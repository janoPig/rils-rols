%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% https://www.biomedcentral.com/getpublished                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% https://miktex.org/                                             %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins

%%% loading packages, author definitions

%\documentclass[twocolumn]{bmcart}% uncomment this for twocolumn layout and comment line below
\documentclass{bmcart}

%%% Load packages
\usepackage{amsthm,amsmath}
\usepackage{tabularx}
\usepackage{xspace}
\usepackage{color}
\usepackage{epsfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{tikz}
%\usepackage{fullpage}
\usepackage{calc}
\usetikzlibrary{positioning,shadows,arrows,trees,shapes,fit}
\usepackage{blindtext}
\usepackage{pgfplots}
%\pgfplotsset{compat=1.16}
\usepackage{pythonhighlight}
\usepackage{fixme}
\fxsetup{status=draft} % <====== add this line
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
  
\usepackage{url}
\raggedbottom
%\algnewcommand\algorithmicforeach{\textbf{for each}}
%\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

% scaling factor for tables
\newcommand\tabscale{0.8}
\newtheorem{definition}{Definition}
\newtheorem{Lemma}{Lemma}


\usepackage{fixme}

  
%\RequirePackage[numbers]{natbib}
%\RequirePackage[authoryear]{natbib}% uncomment this for author-year bibliography
%\RequirePackage{hyperref}
\usepackage[utf8]{inputenc} %unicode support
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\def\includegraphic{}
%\def\includegraphics{}

%%% Put your definitions there:
\startlocaldefs
\endlocaldefs

%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{ {RILS}-{ROLS} : Robust Symbolic Regression via Iterated Local Search and Ordinary Least Squares}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
  addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
  corref={aff1},                       % id of corresponding address, if any
% noteref={n1},                        % id's of article notes, if any
  email={kartelj@matf.bg.ac.rs}   % email address
]{\inits{A.K.}\fnm{Aleksandar} \snm{Kartelj}}
\author[
  addressref={aff2},
  email={marko.djukanovic@pmf.unibl.org}
]{\inits{M.DJ.}\fnm{Marko} \snm{DjukanoviÄ‡}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{%                           % unique id
  \orgdiv{Department of Informatics, Faculty of Mathematics},             % department, if any
  \orgname{University of Belgrade},          % university, etc
  \city{Belgrade},                              % city
  \cny{Serbia}                                    % country
}
\address[id=aff2]{%
  \orgdiv{Faculty of Sciences and Mathematics},
  \orgname{University of Banja Luka},
  %\street{},
  %\postcode{}
  \city{Banja Luka},
  \cny{Bosnia and Herzegovina}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{artnotes}
%%\note{Sample of title note}     % note to the article
%\note[id=n1]{Equal contributor} % note, connected to author
%\end{artnotes}

\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                           %%
%% The Abstract begins here                  %%
%%                                           %%
%% Please refer to the Instructions for      %%
%% authors on https://www.biomedcentral.com/ %%
%% and include the section headings          %%
%% accordingly for your article type.        %%
%%                                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract} % abstract
In this paper, we solve the well-known symbolic regression problem. This problem has been intensively studied in the literature and has a wide range of applications. The symbolic regression models could provide insights towards establishing physical theory behind experimentally observed data. Practical applications include revealing complex ecological dynamics in ecology, discovering mutations effects on protein stability in biology, modeling analytic representations of the exciton binding energy in energy science, finding models for band gaps of NaCl-type compounds
in material science, just to name a few. 

We propose an efficient meta-heuristic-based approach, called \textsc{RILS}-\textsc{ROLS} to solve this problem. \textsc{RILS}-\textsc{ROLS} is based on the following elements: ($i$) iterated local search is the method backbone, mainly solving combinatorial and some continuous aspects of the problem; ($ii$) ordinary least square method is focused on the continuous aspect of the search space -- it efficiently determines the best--fitting coefficients of linear combinations within solution equations;  ($iii$) a novel fitness function combines three important model quality measures: $R^2$ score, RMSE score, and the size of the model (or model complexity). 

The experiments are conducted on the two well-known ground-truth benchmark sets called \textsc{Feynman} and \textsc{Strogatz}. \textsc{RILS}-\textsc{ROLS} was compared to 14 other competitors from the literature. Our method outperformed all 14 competitors with respect to the true symbolic model accuracy percentage under varying levels of noise. It also showed to be the most efficient considering the average running time of reaching the exact model.  
In addition to evaluation on known ground-truth datasets, a new randomly generated set of problem instances is introduced. The goal of \textsc{Random} dataset was to test the scalability of our method with respect to incremental equation sizes and different levels of noise. \textsc{RILS}-\textsc{ROLS} can be considered as a new state-of-the-art method for solving the problem of symbolic regression when ground-truth equations are known.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{Symbolic regression}
\kwd{Iterated local search}
\kwd{Ordinary least square}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
%\end{fmbox}% uncomment this for two column layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                            %%
%% The Main Body begins here                  %%
%%                                            %%
%% Please refer to the instructions for       %%
%% authors on:                                %%
%% https://www.biomedcentral.com/getpublished %%
%% and include the section headings           %%
%% accordingly for your article type.         %%
%%                                            %%
%% See the Results and Discussion section     %%
%% for details on how to create sub-sections  %%
%%                                            %%
%% use \cite{...} to cite references          %%
%%  \cite{koon} and                           %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}      %%
%%                                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%
	\section{Introduction}\label{sec:introduction}

The problem of symbolic regression (SR)~\cite{billard2002symbolic} has attracted many researchers over the last decade. SR can be seen as a generalization of more specific variants of regression in which the functional form is fixed, e.g. the well-known linear regression, polynomial regression~\cite{stimson1978interpreting}, etc. All regression models have the same goal: given a set of $n$-dimensional input data and its corresponding continuous output target variable, the aim is to find a  mathematical expression (function) of $n$ (input) variables that   \emph{fits} best to the target variable.  %This computationally intensive task is in general provenly NP--hard~\cite{virgolin2022symbolic}. 
In the case of linear regression, the model is always a linear combination of input variables. This is in general not good enough, since target variable might be dependent on some nonlinear function among input variables. Unlike linear regression, SR allows the search over much larger space of possible mathematical formulas to find the best-fitting ones, i.e. those able to predict the target variable from the input variables. The basis of constructing explicit formula is in elementary operations like addition and multiplication, as well as polynomial, trigonometric, exponential, and others.  
%Application: https://towardsdatascience.com/real-world-applications-of-symbolic-regression-2025d17b88ef

SR models tend to be interpretable, unlike artificial neural networks. Coefficients inside SR formulas might indicate the absolute or relative importance of certain input variables. Moreover, relations among variables might be described in an elegant way. For example, the appearance of an exponential function can be associated with some physical phenomenon such as the intensity of radiation or acceleration over time, see~\cite{udrescu2020ai}. Additionally, the SR models often have high generalization power unlike some models with fixed functional forms such as polynomial regression. 

Practical applications of SR in chemical and biological sciences are listed in~\cite{weng2020simple}. In particular, this paper describes the discovery of a series of new oxide perovskite catalysts with improved activities. The application of SR to discovering physical laws from a distorted video is studied in~\cite{udrescu2021symbolic}. Revealing complex ecological dynamics by SR is presented in~\cite{chen2019revealing}. Paper \cite{louis2021reviewing} provides the application of SR to model the effects of the mutations on protein stability, in the domain of fundamental and applied biology. A recent study in \cite{liu2021machine} is concerned of auto-discovering conserved quantities using trajectory data from unknown dynamical systems. The application of SR to model analytic representations of the exciton binding energy is shown in~\cite{liang2019phillips}. The use of SR in material science is described in~\cite{wang2019symbolic,wang2022symbolic,burlacu2022symbolic,kabliman2021application}. SR application to wind speed forecasting is given in~\cite{abdellaoui2021symbolic}. 

	There are many different ways to tackle the SR. Most of them are based on   machine learning techniques, genetic programming (GP) or some other meta-heuristics. Among the first GP methods to tackle SR is the one of Raidl~\cite{raidl1998hybrid} based on a hybrid variant of genetic programming. A differential evolution algorithm was proposed by Cerny et al.~\cite{cerny2008using}. Age-fitness Pareto Optimization approach is proposed by Smidt and Lipson~\cite{schmidt2010age}.  Application of the artificial bee colony programming to solve SR is proposed by Karaboga et al.~\cite{karaboga2012artificial}. 
Application of local-based heuristics for solving SR is reported by Commenda in his PhD thesis~\cite{kommenda2018local}. A GP-based approach, the gene-pool optimal mixing evolutionary algorithm (\textsc{GOMEA}) is studied by Virgolin et al.~\cite{virgolin2021improving}.  Another evolutionary algorithm, the interaction-transformation EA (\textsc{ITEA}) has been proposed by de Franca et al.~\cite{de2021interaction}. Simulated annealing to solve SR is proposed by Kantor~\cite{kantor2021simulated}. A variable neighbourhood programming approach  is proposed by Elleurich et al.~\cite{elleuch2020variable}; this technique is initially proposed in~\cite{elleuch2016variable}. 
Kommenda et al.~\cite{kommenda2020parameter} proposed a method called \textsc{Operon} algorithm, which uses nonlinear least squares for parameter identification of SR models further integrated into a local search mechanism in tree-based GP. The C++ implementation of \textsc{Operon} is discussed in~\cite{burlacu2020operon}. The method that utilizes the Taylor polynomials to approximate the symbolic equation that fits the dataset, called Taylor genetic programming, is proposed in~\cite{he2022taylor}. A GP approach that uses the idea of semantic back-propagation (SBP-GP) is proposed in~\cite{virgolin2019linear}.  Empirical analysis of variance between many GP-based methods for SR is discussed in~\cite{kammerer2021empirical}. It is also worth to mention the GP-based Eurequa commercial solver~\cite{schmidt2009distilling, schmidt2011machine} that uses age-fitness pareto optimization with co-evolved fitness estimation. This solver is nowadays accepted as the gold standard of symbolic regression.   

	Method based on Bayesian symbolic regression (\textsc{BSR}) is proposed in~\cite{jin2019bayesian} -- this method belongs to a family of Markov Chain Monte Carlo algorithms (MCMC). Deep Symbolic Regression (\textsc{DSR}), an RNN approach, is proposed in~\cite{petersen2019deep} -- it utilizes the policy gradient search. This mechanism of search is further investigated in~\cite{landajuela2021improving}. A fast neural network approach, called \textsc{OccamNet},  is proposed in~\cite{costa2020fast}.  A deep reinforcement learning approach enhanced with genetic programming is proposed in~\cite{mundhenk2021symbolic}. 

Powerful hybrid techniques to solve SR are also well studied; among them, we emphasize the \textsc{Eplex} solver from~\cite{la2019probabilistic,la2016epsilon} and \textsc{AI-Feynman} algorithm from~\cite{udrescu2020ai}, a physics-inspired divide-and-conquer method combined with the neural network fitting. The latter is one of the most efficient methods for physically-inspired models. We also mention the Fast Function Extraction (\textsc{FFX}) algorithm developed by McConaghy~\cite{mcconaghy2011ffx}, which is a non-evolutionary method combined with a machine learning technique called path-wise-regularized learning, which quickly prunes a huge set of candidate basis functions down to compact models.

A short overview of the most important literature methods to solve SR is given in Table~\ref{tab:gp-based}.  


	\begin{table}[!h]
	\centering
	\scalebox{0.7}{
		\begin{tabularx}{550pt}{l  l  X}  
			Algorithm          &   Paper/year &   Short details   \\ \hline
			\textsc{GP}        &    \cite{koza1994genetic} (1994)    & Application of GP to SR \\ 
			\textsc{Hybrid-GP}  &   \cite{raidl1998hybrid} (1998)   & GP-based method; solutions are locally optimized with OLS to find optimum coefficients for top-level terms \\ 
			\textsc{DFE}        & \cite{cerny2008using} (2008)   & Differential evolution algorithm \\
			\textsc{APF-FE}                & \cite{schmidt2009distilling, schmidt2011machine} (2009, 2011) & Age-fitness Pareto optimization approach using co-evolved fitness estimation   \\
			APF            &      \cite{schmidt2010age} (2010)        &  Age-fitness Pareto optimization approach                        \\
			\textsc{FFX}  & \cite{mcconaghy2011ffx} (2011)  & The fast function extraction algorithm --  non-evolutionary technique based on a machine learning technique called path-wise regularized learning \\
			\textsc{ABCP} &  \cite{karaboga2012artificial} (2012)  & Artificial bee colony programming  approach \\
			\textsc{EPLEX}          &      \cite{la2016epsilon} (2016)         &   A parent selection method called $\epsilon$-lexicase selection  \\ 
			\textsc{MRGP} & \cite{arnaldo2014multiple} (2014) & It decouples and
			linearly combines a program's subexpressions via multiple regression on the target variable  \\
			Local optimization NLS    & \cite{kommenda2018local} (2018)   & Constants optimization in GP by nonlinear least squares \\
			\textsc{FEAT} & \cite{la2018learning} (2018) & Features are represented as networks of multi-type expression trees comprised of activation functions; differentiable features are trained via gradient descent \\
			\textsc{SBP-GP} &  \cite{virgolin2019linear} (2019)  & The idea of semantic back-propagation utilized in GP \\
			\textsc{BSR} & \cite{jin2019bayesian} (2019) & ML-based approach; Bayesian symbolic regression  \\ 
			\textsc{DSR}  & \cite{petersen2019deep} (2019) & Deep symbolic regression based on a RNN approach further utilizing the policy gradient search \\
			\textsc{Operon} & \cite{kommenda2020parameter} (2020) &  Utilizing nonlinear least squares for parameter identification of SR models with LS \\
			\textsc{VNP}    & \cite{elleuch2020variable} (2020) & VNS based GP approach \\
			\textsc{OccamNet} & \cite{costa2020fast} (2020) &   A fast neural network approach; the model defines a probability distribution over a non-differentiable function space; it samples functions and updates the weights with back-propagation  based on cross-entropy matching in an EA strategy	 \\
			
			\textsc{AI-Feynman} & \cite{udrescu2020ai} (2020) & A physics-inspired divide-and-conquer method; it also uses neural network fitting \\
			\textsc{GOMEA}  & \cite{virgolin2021improving} (2021)    & A model-based
			EA framework called gene-pool optimal mixing evolutionary algorithm \\
			\textsc{ITEA} & \cite{de2021interaction} (2021)   & EA based approach called the interaction-transformation EA   \\
			\textsc{SA} & \cite{kantor2021simulated} (2021) &  Simulated annealing approach \\
			
			\textsc{DRLA} & \cite{mundhenk2021symbolic} (2021)  &    A deep reinforcement learning approach enhanced with genetic programming \\
			\textsc{Taylor-GP} &  \cite{he2022taylor} (2022)  &  Taylor polynomials approximations  \\ \hline
			
			
	\end{tabularx} }
	\caption{SR methods overview.}
	\label{tab:gp-based}
\end{table}

	
The SR researches lack uniform, robust, and transparent
benchmarking standards. Recently, La Cava et al.~\cite{la2021contemporary} proposed an
open-source, reproducible benchmarking platform for SR, called \texttt{SRBench}. The authors extended  PMLB~\cite{olson2017pmlb},  a repository of standardized regression problems, with 130 SR datasets for which exact (\emph{ground-truth}) models are known; see more in the aforementioned paper. In their extensive experimental evaluation, 14
symbolic regression methods and 7 machine learning methods are compared on the set of 252 diverse regression problems. The remaining 122 SR datasets belong to a class of so-called \emph{black-box} problems for which the \emph{ground-truth} is unknown. One of the most interesting conclusions from La Cava et al. research is that algorithms specialize in either solving \emph{ground-truth} problems, or \emph{black-box} problems, but not both. 

In this work we present a novel approach to solve \emph{ground-truth}-based SR, which combines the popular iterated local search meta-heuristic (ILS)~\cite{lourencco2003iterated,lourencco2019iterated} with the ordinary least square method (OLS)~\cite{leng2007ordinary}. ILS mostly handles combinatorial (discrete) aspects of search space, while OLS helps in the process of a coefficient determination, so it handles some continuous parts of the search space. As will be shown later, the proposed method has shown its robustness w.r.t. introduced noise, therefore we called the method \textsc{RILS}-\textsc{ROLS}  (with letters R corresponding to regression and robust). Additionally, in order to navigate the search toward exact model (and not only accurate one) the algorithm is equipped with a carefully constructed fitness function that combines three important model characteristics: RMSE and $R^2$ scores which participate into models accuracy, and a weighted solution size that penalizes too complex models. 

The summary of main contributions is as follows: 

\begin{enumerate}
	\item The proposed method outperforms 14 comparison methods on two \emph{ground-truth} benchmark sets from literature -- these methods and benchmarks are used in \texttt{SRBench} platform.  
	
	\item The method shows its high robustness, which is proved by its comparison to the other algorithms in the presence of different levels of Gaussian white noise in the input data. 
	
	\item The method is very efficient, taking in average less than 4 minutes to reach the exact solution, on the problem instances where it was possible. 
	
	\item A new set of unbiased instances is introduced -- it consists of randomly generated formulae of various sizes and number of input variables. This set was employed to analyze the effect of the model size and the level of noise on the solving difficulty. 
\end{enumerate}
 
	\section{Problem definition and search space}
\label{sec:search-space}
In this section we formally define the SR problem.

\begin{definition}
	Given is a dataset $D = \{(\mathbf{x_i}, y_i)\}_{i=1}^n$, where $\mathbf{x_i} \in \mathbb{R}^d$ represents i-th feature (input) vector while $y_i \in \mathbb{R}$ is its corresponding target (output) variable. Suppose that there exists an analytical model of the form $f(\mathbf{x})= g^*(\mathbf{x}, \theta^*) + \epsilon $ that is a generator of all observations from $D$.  
	The goal of SR is to learn a mapping $\tilde{f}(\mathbf{x})=  \tilde{g}(\mathbf{x}, \tilde{\theta})  \colon \mathbb{R}^d \mapsto \mathbb{R}$  estimated by searching through the space of (mathematical) expressions  $\tilde{g}$ and parameters $\tilde{\theta}$ where  $\epsilon$ is observed white noise within the given input data. 
	
\end{definition}

Koza~\cite{koza1994genetic} introduced the problem of SR as a specific application of genetic programming. GP can be used to optimize nonlinear structures such as computer programs. In particular, the programs are represented by syntax trees consisting of functions/operations over input features and constants. As an example of a function represented by a syntax tree, see Figure~\ref{fig:syntax-tree-example}. In essence, all valid syntax trees form the solution search space of SR. That is, each sample model $\tilde{f}$ may be seen as a point in the search space, represented by respective syntax tree. The solution accuracy can be computed on the basis of historical data $D$ and the chosen error measure such as $MSE$, $RMSE$, $R^2$, some combination of them, etc. Interestingly, the SR search space nature is twofold: discrete and continuous. It is primarily modeled as a problem of discrete (combinatorial) optimization, since the number of possible solution functional forms is countable. However, it may also include elements solved by means of continuous (global) optimization, e.g., constants (coefficients) fitting. It is common to use the set of the following elementary mathematical functions: $\sqrt{x}, x^2 $, $\sin$, $\cos$, $\log$, $\exp$, $\arcsin$, $\arccos$, $a^x$, and a set of standard arithmetic operators: $+$, $-$, $\cdot$, and $/$. 

	\begin{figure}[!ht]
	\centering
	\begin{tikzpicture}
		[font=\small, edge from parent, 
		every node/.style={top color=white, bottom color=blue!25, 
			rectangle,rounded corners, minimum size=6mm, draw=blue!75,
			very thick, drop shadow, align=center},
		edge from parent/.style={draw=blue!50,thick},
		level 1/.style={sibling distance=3cm},
		level 2/.style={sibling distance=1.2cm}, 
		level 3/.style={sibling distance=1cm}, 
		level distance=2cm,
		]
		\node (A) {$\cdot$} 
		child { node (B) {$\sin$}
			%child { node {x} 
				%edge from parent node[left=.5em,draw=none] {} }
			child { node {$x$}}
		}
		child {node (C) {$\cdot$}
			%child { node {x}
				%	child { node {C1a}}
				%}
			child { node {$x$}}
			child { node {$/$}
				child { node {$x$}}
				child { node {$y$} %edge from parent node[right=.5em,draw=none] {$\frac{a}{b}$}
				}
			}
		};
		%	child { node {+} 
			%	child { node {2}}
			%	child { node {$x$}}
			%};
		
	\end{tikzpicture}
	
	\caption{Syntax tree representation for the expression $\sin{x} \cdot   ( x \cdot  x / y  )  = \frac{x^2 \sin x }{y}$}
	\label{fig:syntax-tree-example}
\end{figure}

	\section{The proposed RILS-ROLS method}\label{sec:rils}

Our method relies on the following operations: $+$, $-$, $\cdot$, $/$, $\sqrt{x}$, $x^2 $, $\sin$, $\cos$, $\log$, $\exp$, $a^x$, where $a^x$ is only consequently used -- it is never explicitly introduced in the search space. Beside this, the following set of constants enters the search space explicitly: $-1$, $0$, $1$, $2$, $\pi$ and $10$. 
Before we give details on our   method for solving SR, we will explain two building blocks of \textsc{RILS-ROLS}: 1) iterated local search (ILS) and 2) ordinary least squares (OLS). 

\subsection{Iterated local search}
ILS~\cite{lourencco2003iterated} is an efficient meta-heuristics that iteratively generates a sequence of solutions produced by the (embedded) heuristic, such as local search (LS) or randomized greedy heuristics. When the search gets \emph{stuck} in local optimum, a \emph{perturbation} is performed -- this step is usually non-deterministic. This simple idea was proposed by Baxter~\cite{baxter1981local} in early 1980s, and, since then, has been  re-invented by many researches under different names. Some of the following names were used: iterated descent search~\cite{baum1998iterated}, large-step
Markov chains~\cite{martin1991large}, chained local optimization~\cite{martin1996combining}, or, in some cases, combinations of previous~\cite{applegate2003chained}. The most popular version of ILS is shown in Algorithm~\ref{alg:ils}. (Note that we use this version of ILS as the backbone of our \textsc{RILS}-\textsc{ROLS} algorithm.)

\begin{algorithm}
	\begin{algorithmic}[1] 
		\State \textbf{Input}: problem instance
		\State \textbf{Output}: feasible solution 
		\State $s \gets$ \texttt{Initialize}$()$
		\State  $s \gets$ \texttt{LocalSearch}($s$)
		\While{\emph{stopping criteria are not met}}
		\State  $s' \gets$ \texttt{Perturbation}($s$)
		\State  $s' \gets$ \texttt{LocalSearch}($s'$)
		\State  $ s \gets$ \texttt{AcceptanceCriterion}($s, s'$)
		\EndWhile
		\State \Return $s$
	\end{algorithmic}
	\caption{General ILS method.}
	\label{alg:ils}
\end{algorithm}  

An initial solution may be generated randomly or by using a greedy heuristic, which is afterwards, improved by local search. At each iteration, ILS applies three steps. First, the current incumbent solution $s$ is perturbed -- it is partially randomized, yielding new solution $s'$. Next, the solution $s'$ is potentially improved by a LS procedure. Thirdly, the newly obtained solution $s'$  possibly becomes new incumbent -- this is decided upon the acceptance criterion. Sometimes, ILS incorporates a mechanism of tabu list, which prevents the search getting back into already visited solutions.  		 


	\subsection{Ordinary least square method}\label{sec:ols}
Ordinary least square method (OLS) is a linear regression technique. It is based on applying the least-square method to minimize the square residual (error) sum  between actual and predicted values (given by the model). More precisely, given   dataset $D$ of $n$ points $(\mathbf{x_i}, y_i)$ where each $\mathbf{x_i}$ is $d$-dimensional, the task is to determine linear mapping $\tilde{y} = \mathbf{k} \mathbf{x} + b$, that is coefficients (line slope) $\mathbf{k} = (k_1, \ldots, k_d)$ and $b$ (intercept), so that $ \sum_{i}^{n} (\tilde{y}_i - y_i)^2 $ is minimized. This sum is also know as the sum of squared errors (SSE). There are many methods to minimize SSE. One of the analytical approaches is the calculus-oriented -- it takes into account partial derivatives of SSE w.r.t. $k_j,\; j \in \{1, ..., d\}$: 

$$  \frac{\partial}{\partial k_j} \sum_{i}^{n} (\tilde{y}_i - y_i)^2 = \frac{\partial}{\partial k_j} \sum_{i=1}^{n} ( \mathbf{k}\mathbf{x_i}+b  - y_i)^2 =  \sum_{i=1}^{n} 2x_{ij}(\mathbf{k}\mathbf{x_i} + b - y_i).$$ 

Similarly, by taking into account  partial derivation w.r.t. $b$, we have: 
$$  \frac{\partial}{\partial b} \sum_{i=1}^{n} (\tilde{y}_i - y_i)^2  = \frac{\partial}{\partial b} \sum_{i=1}^{n} ( \mathbf{k}\mathbf{x_i}+b  - y_i)^2 = \sum_{i=1}^{n} -2  (y_i - \tilde{y}_i). $$

Finally, by setting each partial derivative to zero, we get the system of $d+1$ linear equations with $d+1$ unknowns ($\mathbf{k}=(k_1, ..., k_d)$ and $b$): 
 \begin{align*}
	& \sum_{i=1}^{n} -2x_{i1} (y_i - \tilde{y}_i) = 0 \\
	& \vdots \\
	& \sum_{i=1}^{n} -2x_{id} (y_i - \tilde{y}_i) = 0 \\
	&\sum_{i=1}^{n} -2  (y_i - \tilde{y}_i) = 0.
\end{align*} 
 

%https://www.thekerneltrip.com/machine/learning/computational-complexity-learning-algorithms/
%https://math.stackexchange.com/questions/84495/computational-complexity-of-least-square-regression-operation %https://levelup.gitconnected.com/train-test-complexity-and-space-complexity-of-linear-regression-26b604dcdfa3
To solve this system one can use Cholesky  factorization or LU decomposition for matrix inversion, and Winograd or Strassen algorithm for matrix multiplication, depending on a solver of linear equations chosen as well as the sparsity of the matrix associated to the linear system~\cite{krishnamoorthy2013matrix}. For example, by using Cholesky decomposition, solving the system requires a matrix inversions and matrix multiplications. In that case the computation cost of $O(d^3)$ is required to find an inverse of the matrix associated to the linear system, which is the most consuming part of the algorithm. Thus, OLS using close-form matrix techniques performs in $O(d^3)$ time complexity.  %, where $d$ stands for the number of input features. 
It is worth mentioning that when the number of input features becomes large, these closed-form matrix techniques are usually replaced by iterative ones, such as gradient descent (GD)~\cite{andrychowicz2016learning}. When using GD the complexity becomes $O(n^2d )$.
% The learning rate in GD has to be carefully chosen, as it dictates how quick the algorithm is and how fast it converges. 


\subsection{\textsc{RILS}-\textsc{ROLS}  method}
Now we will explain the proposed \textsc{RILS}-\textsc{ROLS} method in detail. The overall method scheme is given in Algorithm~\ref{alg:rilsrols}.   

\begin{algorithm}
	\footnotesize
	\hspace*{\algorithmicindent} \textbf{Input}: input training dataset $D_{tr}$ \\
	\hspace*{\algorithmicindent} \textbf{Control parameters}: size penalty $penalty_{size}$, error tolerance $tolerance_{error}$ \\
	\hspace*{\algorithmicindent} \textbf{Output}: best symbolic formula solution $bs$
	\begin{algorithmic}[1] 
		\Procedure{\textsc{RILS}-\textsc{ROLS} }{$D_{tr}$}
		\State $n_{tr} \gets |D_{tr}|$
		\State $sample_{size} \gets $ \texttt{InitialSampleSize}($n_{tr}$)
		\State $D_{tr}' \gets$ \texttt{Sample}($D_{tr}, sample_{size}$)
		\State $s \gets NodeConstant(0)$ 
		\State $s_{fit} \gets$ \texttt{Fitness}($s, D_{tr}'$)
		\State $bs, bs_{fit} \gets s, s_{fit}$ \label{line:solSet}
		\State $start_{tried}, perturbations_{tried} \gets \emptyset, \emptyset$
		\While{\emph{stopping criteria is not met}}
		\State $start_{tried} \gets start_{tried} \cup \{s\}$
		\State $s_{perturbations} \gets $ \texttt{All1Perturbations}($s$) \label{line:sPert}
		\State $s_{perturbations} \gets $ \texttt{FitOLS}($s_{perturbations}$, $D_{tr}'$)
		\State $s_{perturbations} \gets $ \texttt{OrderByR2}($s_{perturbations}$) \label{line:orderR2}
		\State $improved \gets false $
		\For{$p \in s_{perturbations}$}
		\If{$p \in perturbations_{tried}$}
		\State \textbf{continue}
		\EndIf
		\State $p \gets $ \texttt{Simplify}($p$) \label{line:simp}
		\State $p \gets $ \texttt{LocalSearch}($p$, $D_{tr}'$) \label{line:ls}
		\State $p_{fit} \gets$ \texttt{Fitness}($p$, $D_{tr}'$)
		\If{$p_{fit} < bs_{fit}$} \label{line:avoid1}
		\State $bs, bs_{fit}, improved \gets p, p_{fit}, true$ // new best solution
		\State \textbf{break}
		\EndIf \label{line:avoid2}
		\State $perturbations_{tried} \gets perturbations_{tried} \cup \{p\}$
		\EndFor
		\If{improved}
		\State $s \gets bs$ \label{line:improved}
		\Else
		\State $start_{candidates} \gets $ \texttt{All1Perturbations}($bs$)
		\If{$start_{candidates} \setminus start_{tried} = \emptyset$} // all 1-perturbations around $bs$ tried
		\State $s' \gets $ \texttt{RandomPick}($start_{candidates}$)  \label{line:pert21}
		\State $start_{candidates} \gets $ \texttt{All1Perturbations}($s'$) \label{line:pert22} // 2-permutations of \textit{bs}
		\EndIf
		\State $s \gets $ \texttt{RandomPick}($start_{candidates} \setminus start_{tried}$) \label{line:randPick}
		\If{not improved for too many iterations}
		\State $sample_{size} \gets$ \texttt{IncreaseSampleSize}($sample_{size}, n_{tr}$) \label{line:sampleAdj1}
		\State $D_{tr}' \gets$ \texttt{Sample}($D_{tr}, sample_{size}$)\label{line:sampleAdj2}
		\EndIf
		\EndIf
		\If{$R^2$ almost 1 and RMSE almost 0 w.r.t. $tolerance_{error}$}
		\State \textbf{break} // early exit
		\EndIf
		\EndWhile
		\State $bs \gets $ \texttt{Simplify}($bs$)
		\State $bs \gets $ \texttt{RoundModelCoefficients}($bs$)
		\State \Return $bs$
		\EndProcedure
	\end{algorithmic}
	\caption{\textsc{RILS}-\textsc{ROLS}  method.}
	\label{alg:rilsrols}
\end{algorithm}  

\textsc{RILS}-\textsc{ROLS}  algorithms receives training dataset $D_{tr}$ as input. In addition, it has two control parameters: $penalty_{size}$ and $tolerance_{error}$. The first one quantifies the importance of solution expression complexity in the overall solution quality measure (more on this in Section~\ref{sec:fitness}). The second parameter is related to the expected noise level in data -- higher noise means that tolerance to errors should be higher.
$D_{tr}$ can have very large number of records, so the first step of \textsc{RILS}-\textsc{ROLS}  is to choose a random sample $D_{tr}' \subseteq D_{tr}$ of size $sample_{size}$. Initial the sample size is chosen as $\max(0.01 \cdot n_{tr}, 100)$. The size of sample is later dynamically adjusted through the algorithm's iterations -- when there are no solution improvements for some number of iterations, the sample size is doubled (lines~\ref{line:sampleAdj1}-\ref{line:sampleAdj2} of Algorithm~\ref{alg:rilsrols}).

As previously stated, solution is usually represented by means of a tree. We use a simple solution initialization -- tree root node is set to zero constant. We interchangeably use two solution variables: ($i$) $s$ denotes starting (or working) solution and ($ii$) $bs$ stands for the best solution so far, also known as the incumbent solution. Solution quality is measured by evaluating the fitness function (more about it in the subsequent Section~\ref{sec:fitness}). Before entering the main loop, the best solution \emph{bs} is set to the initial solution (line~\ref{line:solSet}). 


Main loop iterates as long as none of termination criteria is met: ($i$) maximal running time has been reached; ($ii$) maximal number of fitness calculations has been made; ($iii$) best solution is sufficiently good, 
w.r.t. its $R^2$ and $RMSE$ scores. More precisely, if $R^2$ is sufficiently close to 1 and, at the same time, $RMSE$ is sufficiently close to 0, the algorithm stops prematurely -- this significantly reduces the running times for the majority of tested instances.
The sufficiency is controlled with parameter $tolerance_{error}$ --  smaller value means that $R^2$ and $RMSE$ should be closer to 1 and 0, respectively. 


One of the first steps in the main loop is to generate perturbations near the starting solution $s$ (line~\ref{line:sPert}). 
As the name of this procedure (\texttt{All1Perturbations}) suggests, perturbation step is local -- meaning the closeness of starting solution $s$ and any of perturbations is 1 (we call it 1-perturbation sometimes). The precise way of generating perturbation is described separately, in Section~\ref{sec:pertGen}. 


Candidate perturbations are being improved by performing OLS coefficient fitting (procedure \texttt{FitOLS}). This means that coefficients in any of linear combinations of current solution are being set by applying  ordinary least square method, already described in Section~\ref{sec:ols}. 
After this step, perturbations are usually better suited to given sample data $D_{tr}'$. 
Further, these perturbations are sorted w.r.t. $R^2$ metric in the descending order (line~\ref{line:orderR2}). 

Now, the algorithm enters the internal loop -- it iterates over the ordered perturbations and aims to find the one which improves the best solution $bs$. But, before comparing candidate perturbation solution $p$ with $bs$, $p$ is first simplified (line~\ref{line:simp}), after which the local search is performed (line~\ref{line:ls}).
Solution simplification is done in a symbolical fashion by popular  Python package \texttt{SymPy}~\cite{sympy}.
Local search tries to find local optima expressions close to the given $p$ -- explained in detail in Section~\ref{sec:ls}.  
Finally, the fitness function value of $p$ is compared to the fitness function value of $bs$. If fitness of $p$ is better, $bs$ is updated correspondingly and the internal loop, that goes across ordered perturbations, is immediately terminated (it works in a  \emph{first-improvement} strategy). Otherwise, the next perturbation is probed. 
Note that the probed perturbations are stored in a set denoted by $perturbations_{tried}$. The goal is to avoid checking the same perturbation multiple times (lines \ref{line:avoid1}-\ref{line:avoid2}), i.e. $perturbations_{tried}$ serves as a kind of tabu list, which is known from the Tablu search meta-heuristic, see~\cite{glover1998tabu}.    


If some of $s_{perturbations}$ around starting solution $s$ yielded an improvement, $bs$ becomes the starting solution $s$ in the next iteration of the main loop (line~\ref{line:improved}). 
Otherwise, it makes no sense to set starting solution to $bs$ -- the search is being  \emph{trapped} in a local optimum. Randomness is introduced in order to avoid this undesired situation. First, a set of local perturbations around $bs$ is generated ($start_{candidates}$) in the same manner as before (procedure \texttt{All1Perturbations}). If at least one of these was not previously used as a starting solution ($start_{tried}$), a single perturbation from the $start_{candidates} \setminus start_{tried}$ is randomly picked (line~\ref{line:randPick}). There is a minor chance that $start_{candidates} \setminus start_{tried} = \emptyset$. When that happens, the set of starting solution candidates is equal to perturbations of some randomly selected perturbation of $bs$ (lines \ref{line:pert21}-\ref{line:pert22}) -- which effectively means that the perturbations with distance 2 from $bs$ are used. There is a very small chance that these 2-perturbations will have an empty set difference with $start_{tried}$ set of starting solutions. Therefore, 3-perturbations are not considered in our algorithm. 

Before returning the final symbolical model, \textsc{RILS}-\textsc{ROLS}  performs the final symbolical simplification and rounding of model coefficients. The latter is sensitive to control parameter $tolerance_{error}$ -- a smaller value means the information loss during rounding is smaller, i.e. a higher number of significant digits is preserved. Note that confidence in the symbolical model is reduced when expected noise in data is higher -- the rule of thumb is: for higher expected noise, $tolerance_{error}$ should be higher. 


\subsubsection{Fitness function}\label{sec:fitness}


Symbolic regression objective is to determine the expression fitting available data. It is also allowed to obtain some equivalent expression, since there are multiple ways to express some symbolical equation. Although logically sound and intuitive, this objective is not quantifiable during the solution search/training phase, because the goal expression is not known at that point -- only target values for some of the input data are known. Thus, various numerical metrics are being used in literature to guide the symbolic regression search process. The most popular are the coefficient of determination, also known as $R^2$, and the mean squared error (MSE) or root mean squared error (RMSE). Also, the important aspect of the solution quality is the solution expression complexity, which may correspond to the model size of its tree representation. This follows the Occam's razor principle~\cite{costa2020fast}~ that simpler solution  is more likely to be a correct one. 
The search process of \textsc{RILS}-\textsc{ROLS}  is guided by the non-linear combination of $R^2$, RMSE and solution expression size (complexity) presented in Equation~(\ref{eq:fitness}). 

\begin{equation}
	\label{eq:fitness}
	fit(s) = (2-R^2(s)) \cdot (1+RMSE(s)) \cdot (1+penalty_{size} \cdot size(s))
\end{equation}

Since the presented fitness function needs to be minimized, the following conclusions may be drawn:
\begin{itemize}
	\item higher $R^2$ is preferred -- ideally, when $R^2(s)=1$, the effect of term $2-R^2(s)$ is neutralized; %since 1 is neutral for multiplication;
	\item lower RMSE is preferred -- ideally, when $RMSE(s)=0$, the whole term $(1+RMSE(s))$ becomes 1;
	\item since $penalty_{size} > 0$, larger expressions tend to have higher fitness (which follows the Occam's razor principle); therefore, simpler solutions are favorable. 
\end{itemize}

The size of expression is calculated by counting all nodes in the expression tree -- this includes leaves (variables and constants) and internal nodes (operations). 

\subsubsection{Expression caching}

In order to speed-up the fitness evaluation, we employ \emph{expression caching}. This means that values attached to expression trees or subtrees are persisted in key-value structure, such that the key is tree (subtree) textual representation, while the value is the $|D_{tr}'|$-size  vector of corresponding expression values on the sample training dataset $D_{tr}'$. (Of course, once the $D_{tr}'$ changes, which happens not that frequently, the whole cache is cleared.) 
Caching is performed in a partial way -- when determining the value of a given expression $T$, it is not required to find the exact \emph{hit} inside the cache. So, if some subexpression (subtree) of $T$ is present in the cache, its value will be reused and further combined to calculate the whole fitness function value. 

For example, let  $T = y^2 \cdot (x+y)/z - \sin(x+y)$ be an expression, and $D_{tr}'=\{([1, 2, 5], 7), ([3, 4, 3], 5), ([4, 5, 3], 6), ([6, 7, 4], 3), ([3, 3, 6], 2)\}$ be a sample training dataset (here, input feature vector is labeled by $[x, y, z]$); let expression cache consisting of the following key-value entries $cache=\{(x+y, [3, 7, 9, 13, 6]), (y^2, [4, 16, 25, 49, 9])\}$.  
Expression $T$ does not need to be fully evaluated since some of its parts are inside the cache: $y^2$ and $x+y$. (Note that single variables do not need to enter the cache, since they are already available as columns of  $D_{tr}'$.)
Each newly evaluated (sub)expression (except constant or variable alone) enters the cache. In this example, the new entries will correspond to keys $(x+y)/z$, $y^2 \cdot (x+y)/z$, $\sin(x+y)$ and $y^2 \cdot (x+y)/z - \sin(x+y)$.  
The maximal number of cache entries is set to 5000. Once this number is reached, the cache is cleared. 

\subsubsection{Perturbations}\label{sec:pertGen}

Perturbations allow the algorithm to escape from local optima. As previously described, perturbations are performed in two occasions: ($i$) during the exhaustive examination of neighboring solutions around the starting solution, ($ii$) during selection of the next starting solution, a non-exhaustive case.  
In both cases, the same Algorithm~\ref{alg:pert} is used. 

\begin{algorithm}
     \textbf{Input}: solution $s$ \\
     \textbf{Output}: local perturbations (1-perturbations) of solution $s$ -- $s_{perturbations}$ \\
	\begin{algorithmic}[1] 
		\Procedure{All1Perturbations}{$s$}
		\State $s_{perturbations} \gets \emptyset$ \label{line:pertInit}
		\State $s \gets$ \texttt{NormalizeConstants}($s$)
		\State $s \gets$ \texttt{Simplify}($s$)
		\State $s_{subtrees} \gets$ \texttt{SubTrees}($s$)
		\For{$n \in s_{subtrees}$} \label{line:forSSStart}
		\State $n_{perturbations} \gets$ \texttt{All1PerturbationsAroundNode}($s$, $n$)
		\State $s_{perturbations} \gets s_{perturbations} \cup n_{perturbations}$	
		\EndFor \label{line:forSSEnd}
		\State \Return $s_{perturbations}$
		\EndProcedure
	\end{algorithmic}
	\caption{Generation of all 1-perturbations of a given solution.}
	\label{alg:pert}
\end{algorithm}  

Initially, the set of perturbations $s_{perturbations}$ is empty (line~\ref{line:pertInit} of Algorithm~\ref{alg:pert}).


This is followed by constant normalization during which coefficients that enter multiplication, division, addition or subtraction are set to 1, while those entering the power function are rounded to integer, with exception of square root, which is kept intact. For example, for expression $3.3\cdot(x+45.1\cdot y^{3.2})\cdot 81\cdot x/\sqrt{y}$ the normalized version is $1\cdot (x+1\cdot y^3)\cdot 1\cdot x/\sqrt{y}$. The reason for performing normalization is reducing the search space of possible perturbations. This reduction is reasonable, since normalization preserves the essential functional form. (Note that coefficients get tuned later: the linear coefficient during the OLS phase, and the remaining during local search.)


After performing the normalization process, the expression is simplified -- getting compact expression is more likely after normalization than before. The previous expression will take form $(x+y^3)\cdot x/\sqrt{y}$. In this particular case, the simplification will usually only remove unnecessary coefficients, but in general it can also perform some non-trivial symbolic simplification. 


Perturbations are generated by making simple changes on the per-node level of $s$ expression tree.
Depending on the structure of the expression tree (note that expression does not have to have unique tree representation), the set of subtrees of the previous expression $(x+y^3)\cdot x/\sqrt{y}$ might be $\{(x+y^3)\cdot x/\sqrt{y}, (x+y^3), x/\sqrt{y}, x, y^3, \sqrt{y}, y\}$. %This set of subtrees is obtained if left subtree of the whole expression is $(x+y^3)$ while the right one is $x/\sqrt{y}$.  
Further, the set of perturbations around each subtree $n$ is generated (lines \ref{line:forSSStart}-\ref{line:forSSEnd} in Algorithm~\ref{alg:pert}).   


Algorithm~\ref{alg:pertNode} shows how perturbations around given subtree are generated.

\begin{algorithm}	
	\hspace*{\algorithmicindent} \textbf{Input}: solution $s$, node $n$\\
	\hspace*{\algorithmicindent} \textbf{Output}: 1-perturbations of solution $s$ around node $n$
	\begin{algorithmic}[1]
		\Procedure{All1PerturbationsAroundNode}{$s$, $n$}
		\State $n_{perturbations} \gets \emptyset$
		\If{$n = s$} \label{alg:alg4-case-1}
		\State $n_{perturbations} \gets n_{perturbations} \cup$ \texttt{NodeChanges}($n$)
		\EndIf
		\If{$n.arity \geq 1$}\label{alg:alg4-case-2}
		\State $n_{changes} \gets$ \texttt{NodeChanges}($n.left$)
		\For{$nc \in n_{changes}$}
		\State $new \gets$ \texttt{Replace}($s$, $n.left$, $nc$)
		\State $n_{perturbations} \gets n_{perturbations} \cup \{new\}$
		\EndFor
		\EndIf
		\If{$n.arity = 2$}\label{alg:alg4-case-3}
		\State $n_{changes} \gets$ \texttt{NodeChanges}($n.right$)
		\For{$nc \in n_{changes}$}
		\State $new \gets$ \texttt{Replace}($s$, $n.right$, $nc$)
		\State $n_{perturbations} \gets n_{perturbations} \cup \{new\}$
		\EndFor
		\EndIf
		\State \Return $n_{perturbations}$
		\EndProcedure
	\end{algorithmic}
	\caption{Generation of 1-perturbations of a given solution around given node.}
	\label{alg:pertNode}
\end{algorithm}

It can be seen that there are three possibly overlapping cases when performing perturbations on the per-node level. 

\emph{Case 1}. 
Observed node $n$ is the whole tree $s$ (see line ~\ref{alg:alg4-case-1} in Algorithm~\ref{alg:pertNode}). Following on  the previous exemplary expression tree, this means that multiplication node that connects $(x+y^3)$ and $x/\sqrt{y}$ is to be changed. For example, multiplication can be replaced by addition, which forms a 1-perturbation expression (tree) $(x+y^3)+ x/\sqrt{y}$. 


\emph{Case 2}. 
Node $n$ has arity of at least 1 (see line ~\ref{alg:alg4-case-2} in Algorithm~\ref{alg:pertNode}). This means that the left subtree exists, so the left subtree node is to be changed. 
For example, if $n=x+y^3$, the overall perturbation might be $(x/y^3)+x/\sqrt{y}$ (addition is replaced by division). 
Another example would be the case of unary operation, e.g., when $n=\sqrt{y}$. In that case, some of possible perturbations could be $(x+y^3)\cdot x/\sqrt{\ln{y}}$ (application of logarithm to left subtree $y$) or $(x+y^3)\cdot x/\sqrt{x}$ (changing variable $y$ to $x$), etc.

\emph{Case 3}. 
Node $n$ is binary operation, meaning the right subtree must exist (Line ~\ref{alg:alg4-case-3} in Algorithm~\ref{alg:pertNode}). 
The analogous idea is applied as in \emph{Case 2}. 

The algorithm allows the following set of carefully chosen per-node changes (method named \texttt{NodeChanges} in Algorithm~\ref{alg:pertNode}): 

\begin{enumerate}
	\item Any node to any of its subtrees (excluding itself). For example, if $(x+y^3)$ is changed to $x$, the perturbation is $(x+y^3)\cdot x/\sqrt{y} \rightarrow x\cdot x/\sqrt{y}$. 
	\item Constant to variable. For example,  $(1+y^3)\cdot x/\sqrt{y} \rightarrow (x+y^3)\cdot x/\sqrt{y}$.
	\item Variable to unary operation applied to that variable. For example,  $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+y^3)\cdot \ln{x}/\sqrt{y}$.
	\item Unary operation to another unary operation. For example,  $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+y^3)\cdot x/\sin{y}$.
	\item Binary operation to another binary operation. For example,  $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+y^3)\cdot (x + \sqrt{y})$. 
	\item Variable or constant enter the binary operation with arbitrary variable. For example,  $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+x/y^3)\cdot x/\sqrt{y}$. 
\end{enumerate}

Method named \texttt{Replace}$(s, n, nc)$ inside Algorithm~\ref{alg:pertNode} is simply used to replace the node $n$ with node $nc$ inside the given expression tree $s$.   

\subsubsection{Local search}\label{sec:ls}

Perturbations are further improved by means of local search procedure (Algorithm~\ref{alg:ls}). 

\begin{algorithm}
	\hspace*{\algorithmicindent} \textbf{Input}: perturbation $p$, sample training dataset $D_{tr}'$ \\
	\hspace*{\algorithmicindent} \textbf{Output}: local optimum $bp$ in the vicinity of perturbation $p$
	\begin{algorithmic}[1] 
		\Procedure{LocalSearch}{$p$, $D_{tr}'$}
		\State $bp, bp_{fit} \gets p,\ $\texttt{Fitness}$(p,D_{tr}')$ 
		\State $improved \gets true$
		\While{$improved$}
		\State $improved \gets false$
		\State $bp_{candidates} \gets \emptyset$
		\State $bp_{subtrees} \gets $\texttt{SubTrees}($bp$)
		\For{$n \in bp_{subtrees}$}
		\State $n_{candidates} \gets $ \texttt{All1PerturbationsAroundNodeExtended}($bp$, $n$)
		\For{$new \in n_{candidates}$}
		\State $new \gets$ \texttt{FitOLS}($new$, $D_{tr}'$)
		\State $new_{fit} \gets$ \texttt{Fitness}($new$, $D_{tr}'$)
		\If{$new_{fit} < bp_{fit}$}
		\State $bp, bp_{fit} \gets new, new_{fit}$
		\State $improved \gets true$
		\EndIf
		\EndFor
		\EndFor
		\EndWhile
		\State \Return $bp$
		\EndProcedure
	\end{algorithmic}
	\caption{Local search procedure.}
	\label{alg:ls}
\end{algorithm}  

For a given perturbation $p$, local search systematically explores extended set of 1-perturbations around $p$. It relies on the \emph{best-improvement} strategy, meaning that all 1-perturbations (for all subtrees) are considered. Before checking if the candidate solution ($new$) is better than the actual best $bp$, the OLS coefficient fitting (\texttt{FitOLS}) takes place.  


The set of used 1-perturbations is expanded in comparison to those used previously. Namely, in addition to those six possible types of node changes, the following four are added:

\begin{enumerate}
	\setcounter{enumi}{6}
	\item Any node to any constant or variable. For example, $(x+y^3)\cdot x/\sqrt{y} \rightarrow y \cdot x / \sqrt{y}$ or  $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+\pi) \cdot x / \sqrt{y}$.
	\item Any node to unary operation applied to it. For example, $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+y^3)\cdot x/\ln{\sqrt{y}}$.
	\item Any node to binary operation applied to that node and some variable or constant. For example, $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+y^3)\cdot x/\sqrt{y} - x$.
	\item Constant to its multiple, where possible multipliers are $\{$0.01, 0.1, 0.2, 0.5, 0.8, 0.9, 1.1, 1.2, 2, 5, 10, 20, 50, 100$\}$. For example, $(1+y^3)\cdot x/\sqrt{y} \rightarrow (1.2+y^3)\cdot x/\sqrt{y}$.
\end{enumerate}

Change under point 10 is very important -- it performs the general coefficient tuning, unlike OLS that considers only coefficients in linear combinations.  

\section{Experimental evaluation}\label{sec:experiments}

Our \textsc{RILS}--\textsc{ROLS} algorithm is implemented in Python 3.9.0. All experiments concerning our method are conducted in the single-core mode, on a PC with Intel i9-9900KF CPU @3.6GHz, 64GB RAM, under Windows 10 Pro OS. The RAM consumption was very small (up to few hundred megabytes) -- beside memory space needed to load input dataset, the only considerable amount is reserved for expression cache. 


The following 14 algorithms are compared to our approach: \textsc{AI-Feynman}, \textsc{GOMEA}, \textsc{Afp-Fe}, \textsc{Itea}, \textsc{Afp}, \textsc{Dsr}, \textsc{Operon}, the \textsc{gplearn} from python package \textsc{gplearn}~\cite{stephens2016genetic}, \textsc{Sbp-Gp}, \textsc{Eplex}, \textsc{Bsr}, \textsc{Feat}, \textsc{Ffx}, \textsc{Mrgp}. 

For the (14) competitors, the results are exported from \texttt{SRBench} \url{https://cavalab.org/srbench/results/}, reported in~\cite{la2021contemporary}. 
The maximum computation time allowed for each run of \textsc{RILS}-\textsc{ROLS} is set to 1 hour, while the maximal number of fitness function evaluations is set to 1 million. All other algorithms from \texttt{SRBench} used the time limit that was equal or bellow 8 hours runtime and the same fitness evaluation limit of 1 million (see~\cite{la2021contemporary}). We did not use the full time limit of 8 hours because we empirically concluded that there were almost no improvements after 1 hour \textsc{RILS}-\textsc{ROLS} runtime.

All non-deterministic algorithms (including \textsc{RILS}-\textsc{ROLS}) were run ten times per each problem instance, where each run used different setting of random number generator seed. 

\subsection{Datasets and \texttt{SRBench}}\fxnote{Marko: Dosao dovde!}

\texttt{SRBench} (\url{https://cavalab.org/srbench/}) is an open-source benchmarking project which merges a large set of diverse benchmark datasets, contemporary SR methods as well as ML methods around a shared model evaluation and an environment for analysis, see~\cite{la2021contemporary}. \texttt{SRBench} considers two types of symbolic regression problems: 1) \emph{ground-truth} problems, for which the exact model is known, and 2) \emph{black-box} problems, for which the exact model is not known. In our work, we consider only the former one, since \textsc{RILS-ROLS} was not designed to solve \emph{black-box} problems. There are two groups of instances in \texttt{SRBench} \emph{ground-truth} problem set:
\begin{itemize}
	\item \textsc{Feynman} instances are inspired by physics and formulas/models that describes various natural laws.  
	There are 116 instances, where each one consists of $10^5$  samples (see \cite{udrescu2020ai} for more details). Some exact models (equations) of \textsc{Feynman} instances are listed in Table~\ref{tab:Feynamn-Eq}.  
	
	\begin{table}[!ht]
		\centering
		\begin{tabular}{l}   \hline
			$x = \sqrt{x_1^2 + x_2^2 - 2 x_1 x_2 \cos(\theta_1 - \theta_2)}$ \\
			$ \theta_1 = \arcsin(n \sin \theta_2)$ \\
			$E =  \frac{m c^2 }{1 - \frac{v^2}{c^2}}$ \\
			$\omega = \frac{1 + \frac{v}{c}}{ \sqrt{1 - \frac{v^2}{c^2}}} \omega_0$ \\ \hline
			
		\end{tabular}
		\caption{Some \textsc{Feynman} instances.}
		\label{tab:Feynamn-Eq}
	\end{table}
	
	
	\item \textsc{Strogatz} instances are introduced in \cite{la2016inference}. 
	Each instance represents a 2-state system of first-order, ordinary differential equations. 
	The aim of each problem is to predict the rate of change of the subsequent state. These equations describe natural processes modeled by non-linear dynamics exhibiting chaos.  The equations for some of the datasets that belong \textsc{Strogatz} are shown in Table~\ref{table:strogatz-ODEs}. In overall, there are 14 \textsc{Strogatz} instances. 
	
	%table, an instance -- example
	
	\begin{table}
		\centering
		\begin{tabular}{ll} \\ \hline
			Bacterial representation &   $x' = 20 - x - \frac{x \cdot y}{1 + 0.5 x^2 }$ \\ 
			&   $y' = 10 - \frac{x \cdot y}{1 + 0.5 x^2  }$ \\ \hline
			Shear Flow               &  $\theta' = \cot(\phi)\cos(\theta)$ \\
			&  $ \phi'  = ( \cos^2(\phi) + 0.1 \cdot \sin^2 (\phi)) \sin(\theta) $ \\ \hline
		\end{tabular}
		\caption{Some \textsc{Strogatz} ODE instances.}
		\label{table:strogatz-ODEs}
	\end{table}
	
\end{itemize}

The above-mentioned benchmark sets may be biased since the models which describe physical laws usually impose   various symmetries, periodicity w.r.t. some variables, internal separability on some variables, etc. In order to test \textsc{RILS-ROLS} robustness and scalability in unbiased setting, we generated a set of random SR problem instances called \textsc{Random}. These instances have a varying size (total number of expression nodes) and number of variables. In total, there are 235 random instances, or 5 randomly generated instances for each of the following 47 (size, number of variables) combinations $\{(3, 1), \{4, 5\} \times \{1, 2\}, \{6\} \times \{1, 2, 3\}, \{7, 8, 9, 10, 11, 12\} \times \{1, 2, 3, 4\}, \{13, 14, 15\} \times \{1, 2, 3, 4, 5\}\}$. Each of \textsc{Random} instances has 10 thousands randomly generated samples. 

Some \textsc{Random} instances are shown in Table~\ref{table:random}, while all instances can be found at \textsc{RILS-ROLS} GitHub repository: \url{https://github.com/kartelj/rils-rols}. 

\begin{table}
	\centering
	\begin{tabular}{ll} \\ \hline
		random\_04\_02\_0010000\_00 &	$\ln{(x_0 + x_1)}$\\
		random\_07\_02\_0010000\_00 &	$(x_0 + 10)\cdot (x_1 + 1)$\\
		random\_08\_02\_0010000\_00 &	$(x_0 + \ln{x_0})*(x_1 + 10)$\\
		random\_10\_03\_0010000\_02 &	$\ln{(\sqrt{x_0} \cdot x_1/\cos{x_2})}$\\
		random\_12\_04\_0010000\_04 &	$\sqrt{x_3 \cdot \exp{x_1} + \cos{(x_2 \cdot \sin{x_0})}}$\\
		random\_15\_03\_0010000\_04	&   $x_0 \cdot x_1/2 + (\cos{(x_2)} - \pi^2)^2$\\
		\hline
	\end{tabular}
	\caption{Some \textsc{Random} instances.}
	\label{table:random}
\end{table}

\subsection{Parameter tuning}

The \textsc{RILS}-\textsc{ROLS} algorithm employs two control parameters, $penalty_{size}$ and $tolerance_{error}$. The first parameter is empirically tuned for considered benchmarks to 0.001 -- this is also used as a default value in the corresponding Python package (package is described in the Appendix). The second parameter is positively correlated with the expected level of noise in input data, i.e., for higher noise the $tolerance_{error}$ should take higher values and vice-versa. More precisely, in case there is no noise, we used the default setting $tolerance_{error}=10^{-16}$, while in noisy scenarios we set $tolerance_{error}$ to the corresponding noise level (0.001 or 0.01). 
For setting $penalty_{size}$ and $tolerance_{error}$ on new datasets one can either keep the default settings or use some wrapper tuning algorithm around \textsc{RILS-ROLS} regressor, e.g., grid-search w.r.t. training or validation accuracy. 

\subsection{Comparison to other methods}
In this section, we evaluate our \textsc{RILS-ROLS} algorithm and compare it to 13 other competitors from the literature. The numerical results are reported in Tables~\ref{tab:comp_noise0}-\ref{tab:comp_noise001}. The results for \textsc{Feynman} and \textsc{Storgatz} are given w.r.t.\  the following levels of noise: 0.0 (no-noisy data), 0.001 (low-level noise), and 0.01 (high-level noise). More precisely, the white Gaussian noise is added to the target variable the same way as in~\cite{la2021contemporary}:
$$ y_{noise} = y + \epsilon, \epsilon \sim \mathcal{N}\left(0, \alpha \sqrt{\frac{1}{n} \sum _{i=1}^n{y_i^2}}\right),$$
where $n$ relates to the number of items in the input data.

Each table consists of two blocks, where the first one gives the exact model accuracy, while the second is related to the less relevant statistical accuracy based on the $R^2$ metric. The exact model accuracy shows how often the method reaches true formula (ground-truth). Regarding statistical accuracy, the solution is considered accurate if it produced an $R^2$ score larger than 0.999.  Note that for each instance there were 10 runs, so in total there were at most 1160 successful runs for \textsc{Feynman} and 140 for \textsc{Strogatz}.  

The following conclusions may be drawn from the numerical results: 

\begin{itemize}
	\item Concerning the exact percentages in case when no noise is utilized in the input data, the best performing algorithm is our \textsc{RILS-ROLS}. It found the exact solution in 57.84\% of runs for \textsc{Feynman} instances. It was even more impressive in solving \textsc{Strogatz} instances, being successful in 83.57\% runs. The second best performing  approach is \textsc{AI-Feynman}, successful in 55.78\% runs for \textsc{Feynman} set, and 27.14\% runs for the problem instances from benchmark set \textsc{Strogatz}. All other approaches perform significantly worse, and none of them is able to deliver the exact solution for more than 30\% runs over the instances of any of the two considered ground-truth benchmark sets. The overall results are in favor of \textsc{RILS-ROLS} with 60.62\%, while the second best is \textsc{AI-Feynman} with 52.65\%. 
	\item  Concerning the exact results in presence of the small noise (0.001), our \textsc{RILS-ROLS} is still performing very well, finding an exact model in 42.08\% of runs considering all problem instances. The second best approach is again \textsc{AI-Feynman} being successful in 31.89\% runs. As in no-noise scenario, there is a significant difference in \textsc{AI-Feynman} performances among two sets of instances -- it performs much better on \textsc{Feynman} instances than on \textsc{Strogatz}. It is also worth mentioning that \textsc{AFP-FE} shows to be robust w.r.t. noise so its performances only slightly deteriorate in comparison to the no-noise scenario. This puts it on the third place with overall accuracy of 21.23\%. Other approaches reach accuracy bellow 20\%.  
	\item  Concerning the scenario with high noise (level of 0.01), our \textsc{RILS-ROLS} is still performing quite well -- it finds and exact model in 34.77\% runs. The second best is \textsc{AFP-FE} reaching 20\% correct solutions. \textsc{AI-Feynman} performances degrade significantly under this level of noise, so it is ranked only 6th overall, reaching the accuracy of 12.61\%. 
	
	\item One more interesting method is \textsc{DSR}. It shows the perfect robustness to noise and agnosticism to benchmark dataset -- it has near 19\% exact model accuracy in all 6 combinations of benchmark dataset (\textsc{Feynman} or \textsc{Strogatz}) and noise level (0, 0.001 or 0.01).     
	
	\item When comparing performances of the algorithms in terms of $R^2 > 0.999$ accuracy, in no-noise scenario, our \textsc{RILS}-\textsc{ROLS} is successful with the rate of success of 83.38\% (third place). Better methods in terms of this accuracy are \textsc{MRGP}, and \textsc{Operon}, with 92.69\%, and 86.92\%, respectively. On the other hand, the exact model percentages of these two approaches are rather low (0\% and 16\%, respectively). These two algorithms are therefore more appropriate in scenarios when the underlying (ground-truth) model is unknown, so-called \emph{black-box} regression. 
	
	\item  In the presence of the noise level of 0.001, our \textsc{RILS-ROLS} method has 78.62\% of a success rate (ranked third, again).  As before, \textsc{MRGP} and \textsc{Operon} are best two here. 
	
	\item   When noise level is 0.01, the situation is almost the same, \textsc{MRGP} and \textsc{Operon} are the best and second best with 88.46\% and 86.54\% accuracy, respectively. \textsc{GP-GOMEA} now reaches third place with 73.46\%, while the proposed \textsc{RILS-ROLS} is fifth with 62.92\% $R^2$ accuracy.
	
	\item We can conclude that \textsc{RILS}-\textsc{ROLS} achieves state-of-the-art performances regarding exact model accuracy. It also shows high robustness to noise. Regarding statistical accuracy, which is not so relevant for the ground-truth problems, \textsc{RILS}-\textsc{ROLS} also performs quite well. 
	
\end{itemize}

\begin{table}[!h]
	\caption{Comparison without noise}\label{tab:comp_noise0}
	\centering
	\begin{tabular}{l|rrr|rrr} \hline
		& \multicolumn{3}{c|}{Exact model percentage} & \multicolumn{3}{c}{$R^2 > 0.999$ percentage}\\ \hline
		Method & Feynman & Strogatz & Total & Feynman & Strogatz & Total \\ \hline
		\textsc{RILS-ROLS}&\bf{57.84}&\bf{83.57}&\bf{60.62}&82.59&90&83.38\\
		\textsc{AI-Feynman}&55.78&27.14&52.65&78.51&35.71&73.83\\
		\textsc{GP-GOMEA}&26.83&29.46&27.12&71.55&71.43&71.54\\
		\textsc{AFP-FE}&26.98&20&26.23&59.05&28.57&55.77\\
		\textsc{ITEA}&22.41&7.14&20.77&27.59&21.43&26.92\\
		\textsc{AFP}&21.12&15.18&20.48&44.83&25&42.69\\
		\textsc{DSR}&19.72&19.64&19.71&25&14.29&23.85\\
		\textsc{Operon}&16.55&11.43&16&86.21&\bf{92.86}&86.92\\
		\textsc{gplearn}&16.27&8.93&15.48&32.76&7.14&30\\
		\textsc{SBP-GP}&12.72&11.61&12.6&73.71&78.57&74.23\\
		\textsc{EPLEX}&12.39&8.93&12.02&46.98&21.43&44.23\\
		\textsc{BSR}&2.48&0.89&2.31&10.78&21.43&11.92\\
		\textsc{FEAT}&0&0.89&0.1&39.66&42.86&40\\
		\textsc{FFX}&0&0&0&0&0&0\\
		\textsc{MRGP}&0&0&0&\bf{93.1}&89.29&\bf{92.69}\\
		\hline
	\end{tabular}
\end{table}


\begin{table}[!h]
	\caption{Comparison with noise level 0.001}\label{tab:comp_noise0001}
	\centering
	\begin{tabular}{l|rrr|rrr} \hline
		& \multicolumn{3}{c|}{Exact model percentage} & \multicolumn{3}{c}{$R^2 > 0.999$ percentage}\\ \hline
		Method & Feynman & Strogatz & Total & Feynman & Strogatz & Total \\ \hline
		\textsc{RILS-ROLS}&\bf{42.93}&\bf{35}&\bf{42.08}&80.26&65&78.62\\
		\textsc{AI-Feynman}&33.08&22.14&31.89&77.39&42.86&73.64\\
		\textsc{AFP-FE}&21.9&15.71&21.23&52.16&35.71&50.38\\
		\textsc{DSR}&19.14&19.29&19.15&25.86&14.29&24.62\\
		\textsc{AFP}&19.66&13.57&19&44.4&21.43&41.92\\
		\textsc{gplearn}&16.99&9.29&16.16&31.9&7.14&29.23\\
		\textsc{ITEA}&14.57&7.14&13.77&27.59&21.43&26.92\\
		\textsc{Operon}&13.19&5&12.31&85.78&\bf{92.86}&86.54\\
		\textsc{GP-GOMEA}&11.03&7.14&10.62&70.26&71.43&70.38\\
		\textsc{EPLEX}&9.57&9.29&9.54&47.84&25&45.38\\
		\textsc{SBP-GP}&0.78&0&0.69&75.43&64.29&74.23\\
		\textsc{BSR}&0.6&0.71&0.62&10.34&14.29&10.77\\
		\textsc{FFX}&0&0&0&0&0&0\\
		\textsc{FEAT}&0&0&0&42.24&50&43.08\\
		\textsc{MRGP}&0&0&0&\bf{92.24}&85.71&\bf{91.54}\\
		\hline
	\end{tabular}
\end{table}

\begin{table}[!h]
	\caption{Comparison with noise level 0.01}\label{tab:comp_noise001}
	\centering
	\begin{tabular}{l|rrr|rrr} \hline
		& \multicolumn{3}{c|}{Exact model percentage} & \multicolumn{3}{c}{$R^2 > 0.999$ percentage}\\ \hline
		Method & Feynman & Strogatz & Total & Feynman & Strogatz & Total \\ \hline
		\textsc{RILS-ROLS}&\bf{36.29}&\bf{22.14}&\bf{34.77}&64.91&46.43&62.92\\
		\textsc{AFP-FE}&20.78&13.57&20&52.59&32.14&50.38\\
		\textsc{DSR}&18.97&18.57&18.92&26.29&14.29&25\\
		\textsc{AFP}&16.9&11.43&16.31&42.67&21.43&40.38\\
		\textsc{gplearn}&16.87&9.29&16.05&29.13&10.71&27.13\\
		\textsc{AI-Feynman}&13.03&9.29&12.61&71.43&39.29&67.86\\
		\textsc{EPLEX}&8.71&4.29&8.23&56.03&21.43&52.31\\
		\textsc{ITEA}&7.84&6.43&7.69&27.59&21.43&26.92\\
		\textsc{GP-GOMEA}&5.09&1.43&4.69&73.71&71.43&73.46\\
		\textsc{Operon}&2.07&0.71&1.92&85.78&\bf{92.86}&86.54\\
		\textsc{BSR}&0.09&0&0.08&12.07&10.71&11.92\\
		\textsc{SBP-GP}&0&0&0&75&75&75\\
		\textsc{FEAT}&0&0&0&40.52&42.86&40.77\\
		\textsc{FFX}&0&0&0&2.59&3.57&2.69\\
		\textsc{MRGP}&0&0&0&\bf{91.81}&60.71&\bf{88.46}\\
		\hline
	\end{tabular}
\end{table}

\begin{figure}[!h]
	
	
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=190pt,height=150pt]{plots/time-avg-noise0.0.eps}
		\caption{No noise}
		\label{fig:noNoise-time}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.47\textwidth}
		
		\includegraphics[  width=190pt,height=150pt]{plots/time-avg-noise0.001.eps}
		\caption{Level of noise equal to 0.001}
		\label{fig:noise0.001-time}
	\end{subfigure}
	\centering
	\begin{subfigure}[b]{0.47\textwidth}
		\includegraphics[  width=190pt,height=150pt]{plots/time-avg-noise0.01.eps}
		\caption{Level of noise equal to 0.01.}
		\label{fig:noise0.01-time}
	\end{subfigure}
	\caption{Average runtime when exact solution is reached (considering only algorithms whose exact percentage is at least 5\% of all runs).}
	\label{fig:time-avg-exact-noise}
\end{figure}

Figure~\ref{fig:time-avg-exact-noise} compares the methods w.r.t. average runtimes (in seconds) when exact solution is reached. We do not consider methods whose exact percentage is too low, i.e., bellow 5\%. Methods are labeled on $x$-axis, while respective average runtimes are shown on $y$-axis. (Note that $y$-axis is logarithmically scaled.) The following conclusions are drawn from these results.

\begin{itemize}
	\item When considering the instance problems with no-noise, the lowest average runtime is reported for \textsc{RILS}-\textsc{ROLS} algorithm (228 sec.). A slightly worse runtime is obtained by \textsc{DSR} algorithm (232 sec.). Note that the relative exact percentage produced by \textsc{RILS}-\textsc{ROLS} is far better from \textsc{DSR} (60.62\% versu 19.71\%). All remaining approaches deliver an order of magnitude larger average runtimes than that of \textsc{RILS}-\textsc{ROLS} and \textsc{DSR} methods..   
	\item  When considering instances with the noise level of 0.001, the lowest average runtime to find the exact model is again delivered by \textsc{RILS}-\textsc{ROLS} algorithm (108 sec.). The second best is produced by \textsc{DSR}, but this time significantly larger (622 sec.) compared to \textsc{RILS-ROLS}. 
	\item When considering the instance problems with the noise level of 0.01, the conclusion is similar to previous. 
\end{itemize}

\subsection{Statistical evaluation}

In order to check the statistical significance of the results w.r.t.\ competitor approaches, we employed the standard statistical methodology. To do so, we have compared the results of \textsc{RILS}-\textsc{ROLS} to 14 other approaches over all algorithm runs on the problem instances from the both ground-truth benchmark sets, \textsc{Feynman} and \textsc{Strogatz}. The successful run means that the corresponding algorithm found the exact model. Having 1300 runs involved, this means that the maximal number of successful runs is 1300. 

\emph{Important note. Exact model accuracy of \textsc{RILS-ROLS} is calculated by dividing the number of successful runs with 1300, i.e., the maximal possible number of successful runs. 
On the other hand, the exact model accuracy of methods revised in \texttt{SRBench} was calculated in a different way. First, the accuracy of each of 130 considered instances was calculated by dividing the number of successful runs with the number of runs that finished its execution. This means that, for example, if some instance was run 10 times, giving 4 successful models, but failing to finish 2 runs due to memory exhaustion or some other OS issue, the accuracy would be 50\% (4/8) instead of 40\% (4/10). Further, these individual accuracies were averaged. 
We do not believe this was the right way to calculate the accuracy, because, this allows one to have high-accuracy method with a low number of successful runs. For example, if method A finishes all of its runs, out of which 650 are successful, its total accuracy will be 50\%. On the other hand, one can have a method B that finishes instances execution in only 4 out of 10 runs, giving 2 successful models each time -- its per-instances accuracy will be 50\%, and after averaging across all instances it will be also 50\%. So, methods A and B will have the same overall accuracy, although method A reached 650 correct solutions, while method B reached only 260.} 

First, the omnibus rank-based Friedmanâ€™s test for all approaches was conducted. 
The number of ranks is 1300, i.e., for each run of each method, the separate rank is calculated. Rank is based on the value of indicator variable, which takes 1 if the method delivers correct model in a given run, otherwise it takes value of 0, no matter if method finished and delivered incorrect model, or method did not finish at all.   Further, the ranks are averaged and used to test the null hypothesis, which states that there is no statistical differences between the average ranks of all competitors. 
In the case in which the null hypothesis $H_0$ was rejected, pairwise comparisons were further performed by using the Nemenyi post-hoc test~\cite{pohlert2014pairwise}. The outcome is represented by means of critical difference (CD) plots. In each CD plot, the 15 competitors are placed on the horizontal axis according to their average ranking. Thereafter, the CD score is computed for a significance level of 0.05. If the difference is small enough, meaning that no statistical difference is detected, a horizontal bar linking statistically equal approaches is drawn. This analysis is done for all three levels of noise, and the corresponding CD plots are shown in Figures~\ref{fig:CDplots-no-noise}-\ref{fig:CDplots-noise0.01}. 


\begin{figure}[!h]
	\centering
		\includegraphics[width=350pt]{plots/Feynman_strogatz_noise_0_0-expanded.pdf}
		\caption{CD plots for no-noise scenario}
		\label{fig:CDplots-no-noise}
\end{figure}
	
\begin{figure}[!h]
	
	\includegraphics[width=350pt]{plots/Feynman_strogatz_noise_0_001-expanded.pdf}
	\caption{CD plots for noise level 0.001}
	\label{fig:CDplots-noise0.001}
\end{figure}
	
	\begin{figure}[!h]
		\includegraphics[width=350pt]{plots/Feynman_strogatz_noise_0_01-expanded.pdf}
		\caption{CD plots for noise level 0.01. }
		\label{fig:CDplots-noise0.01}
	\end{figure}

The following conclusions may be drawn.

\begin{itemize}
	\item  Concerning the no-noise scenario, \textsc{RILS}-\textsc{ROLS} method returns the best average ranking in terms of the number of obtained exact models over all runs.  The second best average ranking is achieved by \textsc{AI-Feynman} algorithm. There is a statistically significant difference between these two approaches. The third best approach is \textsc{APF-FE},  which is far behind both \textsc{RILS}-\textsc{ROLS} and \textsc{AI-Feynman} w.r.t.\ delivered average ranking.  
	\item  For noise 0.001, the situation is similar. \textsc{RILS}-\textsc{ROLS} method again obtains the best average ranking. The second best ranking is obtained by \textsc{AI-Feynman} algorithm. There is a statistically significant difference between the results obtained by these two approaches. The third best approach is \textsc{AFP-FE} which performs statistically worse than \textsc{AI-Feynman}.  
	\item  When noise level is 0.01, \textsc{RILS-ROLS} method is significantly better than other methods. The second best is \textsc{AFP-FE}, followed by \textsc{DSR}, and \textsc{AFTP}. The latter three approaches perform statistically equal. \textsc{AI-Feynman} is fourth, and on pair with \textsc{gplearn}. Note that the average ranking of \textsc{AI-Feynman} method declined significantly with high noise, which is not the case with \textsc{RILS}-\textsc{ROLS} method.   
	
\end{itemize}

\subsection{Scalability of \textsc{RILS}-\textsc{ROLS} algorithm}\label{sec:scalability-rils-rols}

In this section we study the scalability of our method in terms of the size of  instance problems and  the presence of different noise levels in the input data.  First, for the shake of simplicity of presenting,  we split the instances from \textsc{Random} benchmark sets into three parts as follows. 
\begin{itemize}
	\item \textit{Small--sized instances}: any instance  from \textsc{Random} benchmark whose   corresponding exact model  tree representation  has from 3 to 7 nodes belongs to this sub-set.
	\item \textit{Medium--sized instances}:  any instance  from \textsc{Random} benchmark whose   corresponding exact model tree representation  has from 8 to 11 nodes belongs to this sub-set.
	\item \textit{Large--sized instances}: any instances from \textsc{Random} benchmark whose   corresponding exact model  tree representation  has from 12 to 15 nodes belongs to this sub-set. 
\end{itemize}

The results are displayed in Figure~\ref{fig:compExact_noise_size}, grouped with accordance to the above (three) formed sub-groups ($x$-axis); for each of them relative exact percentage rate   of \textsc{RILS}--\textsc{ROLS} algorithm is shown ($y$-axis). The following conclusions may be drawn from there. 

\begin{itemize}
	\item   As expected, the exact percentage success rate is the highest for the small--sized instance problems; it is sightly bellow 90\% (over all XXx runs) in case of no-noisy data. For the noisy data, the exact success rate is getting decreased as the level of noise increases. For example, on the small-sized problem instances with the noise level  of 0.0001, and 0.01, the exact percentage achieved by \textsc{RILS}--\textsc{ROLS} algorithm is still reasonably high, at about 55\%~ and 34 \%, respectively. 
	\item For the medium-sized instance problems where noise is not presented, the exact percentage rate of \textsc{RILS}-\textsc{ROLS} is slightly bellow 50\%. It also decreases by increasing the level of noise in the target data; for the highest level of noise (of 0.01), the success rate of \textsc{RILS}-\textsc{ROLS} method is just at about 6\%. 
	\item For the large-sized instance problems where no noise is utilized, the exact percentage rate of \textsc{RILS}--\textsc{ROLS} algorithm is at about 25\%, which means that the increase in the size of exact models affects the algorithm's performance, but not dramatically.  
	
	\item From the above, we can conclude that the size of exact models (solutions) and the increase of   level of noise both contribute to overall performance of our \textsc{RILS}--\textsc{ROLS} method. However, the exact percentage rate of the algorithm seems to decrease linearly  in the size of (exact) model, regardless of the noise level values.
	
	\item Concerning  the percentage of  runs over all random instances for which \textsc{RILS-ROLS} terminates by producing an accurate model, that is a model with $R^2 > 0.999$, for the small--sized problem instances where no noise is included, \textsc{RILS}-\textsc{ROLS} delivers  the perfect score almost. In the presence of a low noise level (equals to 0.001), this percentage  is still holding high, at about 95\%. In case when the high level of noise (equals to 0.01) is presented in the input data, these percentages stagnate, but still are at a respectable 66\%. %small-sized instances it reaches a high--precision of $R^2=0.999$ upon termination.   
	As  pointed out already, for a half of these cases the respectable exact models were found by the algorithm.
	
	\item Concerning  the medium-sized instance problems when the input data are free of the noise,   the final outcome reached the high precision in 65\% of all algorithm runs.    The noisy the data are, these percentages, as expected, drop off; however, it is not so dramatically. For the level of noise of 0.01, the high--accuracy of the final solution is reported on $\approx$30\% of all runs.
	
	\item Concerning  the large-sized instance problems with no noise, the  final solution reached the high precision in 60\% of all runs. Not so surprisingly, the hardest case to obtain the desired high--accuracy (that is, ensuring $R^2> 0.999$ for  the  outcome) is in the presence of a high level of noise (of 0.01), where the algorithm was successful on about 20\% of all runs concerning the large--sized instance problems. 
	
\end{itemize}


\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel=\textbf{model size},
			ylabel=\textbf{exact percentage},
			xmin=0, xmax=4,
			ymin=0, ymax=100,
			xtick={1,2,3},
			xticklabels={small,medium,large},   % <---
			ytick={0,10,...,100}
			]
			\addplot[smooth,mark=*,blue] plot coordinates {
				(1,85)
				(2,42.5)
				(3,21.05)
			};
			\addlegendentry{$\alpha=0.00$}
			
			\addplot[smooth,color=red,mark=x]
			plot coordinates {
				(1,51.67)
				(2,13.75)
				(3,5.26)
			};
			\addlegendentry{$\alpha=0.001$}
			
			\addplot[smooth,color=green,mark=o]
			plot coordinates {
				(1,31.67)
				(2,5)
				(3,4.21)
			};
			\addlegendentry{$\alpha=0.01$}
		\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Exact solution percentages for varying levels of noise and formulae sizes}
	\label{fig:compExact_noise_size}
\end{center}

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel=\textbf{model size},
			ylabel=\textbf{$R^2 > 0.999$ percentage},
			xmin=0, xmax=4,
			ymin=20, ymax=100,
			xtick={1,2,3},
			xticklabels={small,medium,large},   % <---
			ytick={20,30,...,100}
			]
			\addplot[smooth,mark=*,blue] plot coordinates {
				(1,98.33)
				(2,62.5)
				(3,58.95)
			};
			\addlegendentry{$\alpha=0.00$}
			
			\addplot[smooth,color=red,mark=x]
			plot coordinates {
				(1,93.33)
				(2,53.75)
				(3,40)
			};
			\addlegendentry{$\alpha=0.001$}
			
			\addplot[smooth,color=green,mark=o]
			plot coordinates {
				(1,65)
				(2,31.25)
				(3,20)
			};
			\addlegendentry{$\alpha= 0.01$}
		\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Percentages of solutions having $R^2 > 0.999$ for varying levels of noise and formulae sizes}
	\label{fig:compR2_noise_size}
\end{center}

In the rest of the section,  we investigate scalability of our \textsc{RILS}--\textsc{ROLS} algorithm in terms of the number of variables and sizes of corresponding exact models of the problem instances varying levels of noise. There are three bar plots generated per each level of noise, shown by Figures~\ref{fig:compExact_noise_varcnt}--\ref{fig:compExact_noise_varsizes}. The instances are grouped according to the different variable count/size of respective exact models ($x$-axis) that correspond to them. For each group, the exact percentages obtained by \textsc{RILS}--\textsc{ROLS} algorithm group are shown on $y$-axis.

The following conclusions can be drawn from Figure~\ref{fig:compExact_noise_varcnt}. 

\begin{itemize}
	\item For the no-noisy data, the exact percentage rate of \textsc{RILS}--\textsc{ROLS} algorithm  slightly decrease as the expected number of variables increases in the exact models. 
	\item The exact percentages drop off with the presence of a larger noise level. As expected, the higher the noise level, the smaller the exact percentages are.
	\item Interestingly, in the presence of noise in the data, the highest exact percentages are not obtained for the problem instances which have exact models consisted of a single variable, but two or three. We argue it by the fact that the early phase of the algorithm will more likely trying to include additional variables in current solution at the cost of increasing the model accuracy to ensure improving the fitness value over iterations. For the no--noisy data, OLS method will more likely find a small-sized high-accuracy model (in our experiments that happened on $\approx$ 50\% runs) that fits to the (no-noisy) input data than the case of noisy data. When the data is noisy, the decision of adding more variables at the beginning is likely to be forced than just dealing with short-size solutions. %By the progression of the algorithm, it gets harder improving the best fitness value by adding variables, and other operations are more frequently involved than in previous iterations. %This conclusion also correlates to the shape of fitness function given by Equation~(\ref{eq:fitness}) where the first two product terms there are already small (close to 1).      \fxnote{TODO: need to be further expanded...}
	%Obtaining an accurate model with a few variables in the presence of a high level of nose in the data 
\end{itemize}

\begin{figure}[!ht]
	
	
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[  width=180pt,height=140pt]{plots/numvars_vs_exact_correct_no_noise.eps}
		\caption{No noise}
		\label{fig:noNoise}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		
		\includegraphics[  width=180pt,height=140pt]{plots/numvars_vs_exact_correct_noise0_001.eps}
		\caption{Level of noise equal to 0.001}
		\label{fig:noise0.001}
	\end{subfigure}
	\centering
	\begin{subfigure}[b]{0.40\textwidth}
		\includegraphics[  width=180pt,height=140pt]{plots/numvars_vs_exact_correct_noise0_01.eps}
		\caption{Level of noise equal to 0.01.}
		\label{fig:noise0.01}
	\end{subfigure}
	\caption{Exact solution percentages for varying levels of noise and variable counts.}
	\label{fig:compExact_noise_varcnt}
\end{figure}


The  following conclusions may be drawn from Figure~\ref{fig:compExact_noise_varsizes}.
\begin{itemize}
	\item The exact percentages are decreased with increase in  the size of exact models, in case of any level of noise utilized on the target values.  %within the input data.
	\item The highest exact percentages, which are larger than 70\%, are noticed in case of the small-sized problem instances  (the size of models ranges from 3 to 7). However, for the same sizes, for noisy data, these percentages are significantly lower; for example, in case of the instances whose exact models are of size 7 when utilizing the level of noise of 0.001 and 0.01, the exact percentages obtained are 20\% and 10\%, respectively. 
	
	\item   For the problem instances with the largest size of (exact) models in case of the no noisy data, exact percentages are never lower than 20\%. The opposite holds for noisy data. 
\end{itemize}

\begin{figure}[!ht]
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[  width=180pt,height=140pt]{plots/numsize_vs_exact_correct_no_noise.eps}
		\caption{No noise}
		\label{fig:size-no-noise}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		
		\includegraphics[  width=180pt,height=140pt]{plots/numsize_vs_exact_correct_noise0_001.eps}
		\caption{Level of noise equal to 0.001}
		\label{fig:size-noise0.001}
	\end{subfigure}
	\centering
	\begin{subfigure}[b]{0.40\textwidth}
		\includegraphics[  width=180pt,height=140pt]{plots/numsize_vs_exact_correct_noise0_01.eps}
		\caption{Level of noise equal to 0.01.}
		\label{fig:size-noise0.01}
	\end{subfigure}
	\caption{Exact solution percentages for varying levels of noise and exact model sizes.}
	\label{fig:compExact_noise_varsizes}
\end{figure}

When one concerns of the runtime analysis of \textsc{RILS}--\textsc{ROLS} algorithm on the \textsc{Random} benchmark set w.r.t\ number of variables, the results are displayed in Figure~\ref{fig:runtime_rils_rols}. The instances are grouped w.r.t.\ number of variables involved into corresponding exact models ($x$-axis). The following conclusions may be drawn from there.  

\begin{itemize}
	\item As one could expect, average runtime is getting increased with the increase of the number of variables in exact models. 
	\item Note that the average runtime in case of the instances whose exact models have the largest number of variables (5) is just about 200$s$.
	\item The two above-mentioned points indicate efficacy of our method especially in case of the instances with small-to-medium sized exact models in the presence of small--to--middle noise levels included in the input data. In these cases, the algorithm is able to deliver a reasonable exact percentage rate, which are always larger than 50\%. Note that models which have more compact representations (thus, small in their size) are desirable in many areas of natural sciences as they are easy interpret-able than models represented by a more complex mathematical formula.   
\end{itemize}

\begin{figure}[!ht]
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[  width=180pt,height=140pt]{plots/numsize_vs_time_no_noise.eps}
		\caption{No noise}
		\label{fig:time-no-noise}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		
		\includegraphics[  width=180pt,height=140pt]{plots/vars_vs_time_noise0_001.eps}
		\caption{Level of noise equal to 0.001}
		\label{fig:time-noise0.001}
	\end{subfigure}
	\centering
	\begin{subfigure}[b]{0.40\textwidth}
		\includegraphics[  width=180pt,height=140pt]{plots/vars_vs_time_noise0_01.eps}
		\caption{Level of noise equal to 0.01.}
		\label{fig:time-noise0.01}
	\end{subfigure}
	\caption{Average runtime comparisons for varying levels of noise and variable counts.}
	\label{fig:runtime_rils_rols}
\end{figure}


\section{Conclusions and future work}\label{sec:conclusions}

In this paper, we dealt with solving the well--known Symbolic regression (SR)  problem. SR problem is a generalization of more interdisciplinary known problems such as linear and logistic regression.    
We proposed a meta-heuristic approach, called \textsc{RILS}-\textsc{ROLS}, which is built upon the Iterated local search (ILS) scheme. The first crucial concept within this scheme is utilizing the ordinary least square method to efficiently determine best-fitting internal coefficients to the candidate solution. % Thus, the search process is primarily focused on considering already promising solutions in the continuous part of the search space rather than applying enhanced heuristics to search for those coefficients. 
Another key aspect is the utilization of a carefully constructed fitness function in the search, which appeared as a combination of three important scores: RMSE, $R^2$  scores and the size of the model (solution).  Last but not least, a carefully constructed local search is applied systematically exploring solutions obtained as 1--perturbations of the considered solution, represented as trees.  1--perturbations of a solution are based on applying the six per-node change operations on the solution's tree representation. 

\textsc{RILS-ROLS} method is compared to the 14 other competitor algorithms from the literature on the two ground--truth benchmark sets from the literature, namely \textsc{Feynman} and \textsc{Strogatz}.  Additionally, we introduced a non-biased benchmark set of randomly generated instances, labelled by \textsc{Random}.  The experimental evaluation confirmed the efficiency of our method which produced the best average ranking results in terms of exact percentage when compared to all other competitors on the two ground-truth benchmarks in the presence of all three different levels of noise in the input data. In all cases, new best exact percentages are obtained by our \textsc{RILS-ROLS} method.   More in details,
our algorithm was able to obtain the right model on 60.62\%, 42.08\%, and 34.77\% runs over all considered problem instances in the presence of different noise levels: 0.0, 0.001, and 0.01, respectively. For the shake of comparisons, the second best approach \textsc{AI-Feynman}, when the noise level of 0.0 and 0.001 is included, obtains the exact percentages of 52.65\%, and 31.89\%, respectively. In case of the highest noise level, the second best approach was \textsc{Afp-Fe}, obtaining an exact percentage rate of 20\%. Additionally, the scalability and robustness of \textsc{RILS}-\textsc{ROLS} method are checked and confirmed on the unbiased \textsc{Random} benchmark set in terms of a few different criteria such as the size of the model, the number of variables included in exact models, and obtained average runtimes. 

In future work, one could think of constructing a hybrid of \textsc{RILS-ROLS} method with some other meta-heuristics to further boost the quality of the obtained results. For example, replacing ILS scheme of \textsc{RILS-RILS} with a more general Variable neighbourhood search (VNS) scheme is a reasonable option to give it a try. VNS   utilizes a more stronger and systematic system of solutions diversification and thus escaping from local optima could be much easier and the convergence much faster.  Additionally, as proof of the efficiency and usefulness of our algorithm, this tool could be applied to solving problems from various problem domains out of the area of computer science, for example from physics or chemistry, to help for setting up reasonable hypotheses or to obtain valuable insights on obtained experimental evaluations.  

\section*{Appendix -- \textsc{RILS}-\textsc{ROLS} Python package}\label{sec:appendix-1}

\textsc{RILS-ROLS} algorithm is available for install on the well-known Python package repository \url{https://pypi.org}, so it can be easily installed by typing commands:
\begin{python} 
	pip install rils-rols
\end{python}
\textsc{RILS-ROLS} project page is at \url{https://pypi.org/project/rils-rols}. Here, one can find a minimal working example on how-to-use \textsc{RILS-ROLS}:

\begin{python}
	from rils_rols.rils_rols import RILSROLSRegressor
	from math import sin, log
	
	regr = RILSROLSRegressor()
	
	# toy dataset 
	X = [[3, 4], [1, 2], [-10, 20], [10, 10], [100, 100], [22, 23]]
	y = [sin(x1)+2.3*log(x2) for x1, x2 in X]
	
	# RILSROLSRegressor inherits BaseEstimator (sklearn)
	regr.fit(X, y)
	
	# this prints out the learned model
	print("Final model is "+str(regr.model))
	
	# applies the model to a list of input vectors
	X_test = [[4, 4], [3, 3]]
	y_test = regr.predict(X_test)
	print(y_test) 
\end{python}

Python sources, experimental results and all other \textsc{RILS-ROLS} resources can be found under the project GitHub page \url{https://github.com/kartelj/rils-rols}. 
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{backmatter}

\section*{Acknowledgements}%% if any
Text for this section\ldots

\section*{Funding}%% if any
Text for this section\ldots

\section*{Abbreviations}%% if any
Text for this section\ldots

\section*{Availability of data and materials}%% if any
Text for this section\ldots

\section*{Ethics approval and consent to participate}%% if any
Text for this section\ldots

\section*{Competing interests}
The authors declare that they have no competing interests.

\section*{Consent for publication}%% if any
Text for this section\ldots

\section*{Authors' contributions}
Text for this section \ldots

\section*{Authors' information}%% if any
Text for this section\ldots

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file (bmc-mathphys, vancouver, spbasic).
\bibliography{bib}      % Bibliography file (usually '*.bib' )
 
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files
 

 
 

\section*{Additional Files}
  \subsection*{Additional file 1 --- Sample additional file title}
    Additional file descriptions text (including details of how to
    view the file, if it is in a non-standard format or the file extension).  This might
    refer to a multi-page table or a figure...
 

\end{backmatter}
\end{document}
