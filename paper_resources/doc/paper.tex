\documentclass[a4paper,12pt]{elsarticle}
% vim: tw=0 wm=0

\setcounter{tocdepth}{3}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{comment}
\usepackage{placeins}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{array}
\usepackage[pdfencoding=auto,psdextra]{hyperref}
\usepackage{booktabs}
\usepackage{bookmark}% faster updated bookmarks
\usepackage{hypcap} % fix the links
\evensidemargin\oddsidemargin
\usepackage{graphicx}
\pagestyle{plain}
\usepackage{xcolor}
\newcommand\ToDo[1]{\textcolor{red}{#1}}
\usepackage{tabularx}
\usepackage{xspace}
\usepackage{color}
\usepackage{epsfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{changes}
\usepackage{tikz}
\usepackage{fullpage}
\usepackage{calc}
\usetikzlibrary{positioning,shadows,arrows,trees,shapes,fit}
\usepackage{blindtext}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}

\usepackage{numprint}
\npdecimalsign{.}
\npthousandsep{}
\usepackage{pythonhighlight}


\usepackage[draft,nomargin,inline]{fixme}
\fxsetface{inline}{\itshape}
\fxsetface{env}{\itshape}
%\fxuselayouts{margin}
%\fxuselayouts{inline}
\fxusetheme{color}

\usepackage{url}
\newcommand{\keywords}[1]{\par\aDSvspace\baselineskip
	\noindent\keywordname\enspace\ignorespaces#1}

\usepackage{tikz}
\usetikzlibrary{positioning}
\definecolor{canaryyellow}{rgb}{1.0, 0.94, 0.0}
\definecolor{brightgreen}{rgb}{0.4, 1.0, 0.0}
\definecolor{jazzberryjam}{rgb}{0.65, 0.04, 0.37}

%defining of command

\newcommand\floor[1]{\lfloor#1\rfloor}
\newcommand\ceil[1]{\lceil#1\rceil}
\newcommand\str[1]{\texttt{#1}}
\newcommand\pL[1][]{\ensuremath{p^{\mathrm{L}#1}}}
\newcommand\pR[1][]{\ensuremath{p^{\mathrm{R}#1}}}
\newcommand\qL{\ensuremath{q^\mathrm{L}}}
\newcommand\qR{\ensuremath{q^\mathrm{R}}}
\newcommand\pLH{\ensuremath{\hat{p}^\mathrm{L}}}
\newcommand\pRH{\ensuremath{\hat{p}^\mathrm{R}}}
\newcommand{\Vext}{\ensuremath{V_\mathrm{{ext}}}}
\newcommand\UB{\ensuremath{\mathrm{UB}}}
\newcommand\Sigmand{\ensuremath{\Sigma^\mathrm{nd}}}
\newcommand{\mdmwnpp}{MDMWNPP\xspace}

\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\setlength{\leftmarginii}{1.8ex}
\raggedbottom
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

% scaling factor for tables
\newcommand\tabscale{0.8}
\newtheorem{definition}{Definition}
\newtheorem{Lemma}{Lemma}

\begin{document}
	
	%\setlength{\parindent}{0pt}  % disallow indentations
	%\numberwithin{table}{1}
	%\mainmatter  % start of an individual contribution
	
	% first the title is needed
	\title{\textsc{RILS}-\textsc{ROLS} : Robust Symbolic Regression via Iterated Local Search and Ordinary Least Squares}
	
	\author[1]{Aleksandar Kartelj}
	\author[2]{Marko Djukanovi\'c}
	\address[1]{$kartelj@matf.bg.ac.rs$, \\  Faculty of Mathematics, University of Belgrade, Serbia}
	\address[2]{$ marko.djukanovic@pmf.unibl.org$,\\   Faculty of Natural Sciences and Mathematics, University of Banja Luka, Bosnia and Herzegovina}
	\begin{abstract}
		In this paper, we solve the well-known symbolic regression problem. This problem has been intensively studied in the literature and has a wide range of applications. The symbolic regression models could provide insights towards establishing physical theory behind experimentally observed data. Practical applications include revealing complex ecological dynamics in ecology, discovering mutations effects on protein stability in biology, modeling analytic representations of the exciton binding energy in energy science, finding models for band gaps of NaCl-type compounds
		in material science, just to name a few. 
		
		We propose a new meta-heuristic-based approach, called \textsc{RILS}-\textsc{ROLS} to solve this problem. \textsc{RILS}-\textsc{ROLS} is based on the following elements: ($i$) iterated local search is the method backbone, mainly solving combinatorial and some continuous aspects of the problem; ($ii$) ordinary least square method is focused on the continuous aspect of the search space -- it efficiently determines the best--fitting coefficients of linear combinations within solution equations;  ($iii$) a novel fitness function combines three important model quality measures: $R^2$ score, RMSE score, and the size of the model (or model complexity). 
		
		The experiments are conducted on the two well-known ground-truth benchmark sets called \textsc{Feynman} and \textsc{Strogatz}. \textsc{RILS}-\textsc{ROLS} was compared to 14 other competitors from the literature with  presence of different levels of the Gaussian white noise in the input data. Our method outperformed all 14 competitors with respect to the true symbolic model accuracy percentage under all level of noise. 
		In addition to evaluation on known ground-truth datasets, a new randomly generated set of problem instances is introduced. The goal of \textsc{Random} dataset was to test the scalability of our method with respect to incremental equation sizes and different levels of noise. \textsc{RILS}-\textsc{ROLS} can be considered as a new state-of-the-art method for solving the problem of symbolic regression when ground-truth equations are known.
	\end{abstract}
	\maketitle
	
	
	\section{Introduction}\label{sec:introduction}
	
	The problem of symbolic regression (SR)~\cite{billard2002symbolic} has attracted many researchers over the last decade. SR can be seen as a generalization of more specific variants of regression in which the functional form is fixed, e.g. the well-known linear regression, polynomial regression~\cite{stimson1978interpreting}, etc. All regression models have the same goal: given a set of $n$-dimensional input data and its corresponding continuous output target variable, the aim is to find a  mathematical expression (function) of $n$ (input) variables that best \emph{fits} the target variable.  %This computationally intensive task is in general provenly NP--hard~\cite{virgolin2022symbolic}. 
	In the case of linear regression, the model is always a linear combination of input variables. This is in general not good enough, since target variable might be dependent on some nonlinear function among input variables. Unlike linear regression, SR allows the search over much larger space of possible mathematical formulas to find the best-fitting ones, i.e. those able to predict the target variable from the input variables. The basis of constructing explicit formula is in elementary operations like addition and multiplication, as well as polynomial, trigonometric, exponential, and others.  
	%Application: https://towardsdatascience.com/real-world-applications-of-symbolic-regression-2025d17b88ef
	
	SR models tend to be interpretable, unlike artificial neural networks. Coefficients inside SR formulas might indicate the absolute or relative importance of certain input variables. Moreover, relations among variables might be described in an elegant way. For example, the appearance of an exponential function can be associated with some physical phenomenon such as the intensity of radiation or acceleration over time, see~\cite{udrescu2020ai}. Additionally, the SR models often have high generalization power unlike some models with fixed functional forms such as polynomial regression. 
	
	Practical applications of SR in chemical and biological sciences are listed in~\cite{weng2020simple}. In particular, this paper describes the discovery of a series of new oxide perovskite catalysts with improved activities. The application of SR to discovering physical laws from a distorted video is studied in~\cite{udrescu2021symbolic}. Revealing complex ecological dynamics by SR is presented in~\cite{chen2019revealing}. Paper \cite{louis2021reviewing} shows how SR is used to model the effects of the mutations on protein stability, in the domain of fundamental and applied biology. One recent study \cite{liu2021machine} is concerned of auto-discovering conserved quantities using trajectory data from unknown dynamical systems. Paper~\cite{liang2019phillips} shows the application of SR to model analytic representations of the exciton binding energy. The use of SR in material science is described in~\cite{wang2019symbolic,wang2022symbolic,burlacu2022symbolic,kabliman2021application}. SR application to wind speed forecasting is given in~\cite{abdellaoui2021symbolic}. 
	
	There are many different ways to tackle the SR. Most of them are based on the machine learning techniques, genetic programming (GP) or some other meta-heuristics. Among the first GP methods to tackle SR is the one of Raidl~\cite{raidl1998hybrid} based on a hybrid variant of genetic programming. A differential evolution algorithm was proposed by Cerny et al.~\cite{cerny2008using}. Age-fitness Pareto Optimization approach is proposed by Smidt and Lipson~\cite{schmidt2010age}.  Application of the artificial bee colony programming to solve SR is proposed by Karaboga et al.~\cite{karaboga2012artificial}. 
	Application of local-based heuristics for solving SR is reported by Commenda in his PhD thesis~\cite{kommenda2018local}. A GP-based approach, the gene-pool optimal mixing evolutionary algorithm (\textsc{GOMEA}) is studied by Virgolin et al.~\cite{virgolin2021improving}.  Another evolutionary algorithm, the interaction-transformation EA (\textsc{ITEA}) has been proposed by de Franca et al.~\cite{de2021interaction}. Simulated annealing to solve SR is proposed by Kantor~\cite{kantor2021simulated}. A variable neighbourhood programming approach to solve SR  is proposed by Elleurich et al.~\cite{elleuch2020variable}; this technique is initially proposed in~\cite{elleuch2016variable}. 
	Kommenda et al.~\cite{kommenda2020parameter} proposed a method called \textsc{Operon} algorithm, which uses nonlinear least squares for parameter identification of SR models further integrated into a local search mechanism in tree-based GP. The C++ implementation of \textsc{Operon} is discussed in~\cite{burlacu2020operon}. The method that utilizes the Taylor polynomials to approximate the symbolic equation that fits the dataset, called Taylor genetic programming, is proposed in~\cite{he2022taylor}. A GP approach that uses the idea of semantic back-propagation (Sbp-Gp) is proposed in~\cite{virgolin2019linear}.  Empirical analysis of variance between many GP-based methods for SR is discussed in~\cite{kammerer2021empirical}. It is also worth to mention the GP-based Eurequa commercial solver~\cite{schmidt2009distilling, schmidt2011machine} that uses age-fitness pareto optimization with co-evolved fitness estimation. This solver is nowadays accepted as the gold standard of symbolic regression.   
	
	Method based on Bayesian symbolic regression (\textsc{BSR}) is proposed in~\cite{jin2019bayesian} -- this method belongs to a family of Markov Chain Monte Carlo algorithms (MCMC). Deep Symbolic Regression (\textsc{DSR}), an RNN approach, is proposed in~\cite{petersen2019deep} -- it utilizes the policy gradient search. This mechanism of search is further investigated in~\cite{landajuela2021improving}. A fast neural network approach, called \textsc{OccamNet},  is proposed in~\cite{costa2020fast}.  A deep reinforcement learning approach enhanced with genetic programming is proposed in~\cite{mundhenk2021symbolic}. 
	
    Powerful hybrid techniques to solve SR are also well studied; among them, we emphasize the \textsc{Eplex} solver from~\cite{la2019probabilistic,la2016epsilon} and \textsc{AI Feynman} algorithm from~\cite{udrescu2020ai}, a physics-inspired divide-and-conquer method combined with the neural network fitting. The latter is one of the most efficient methods for physically-inspired models. We also mention the Fast Function Extraction (\textsc{FFX}) algorithm developed by McConaghy~\cite{mcconaghy2011ffx}, which is a non-evolutionary method combined with a machine learning technique called path-wise-regularized learning, which quickly prunes a huge set of candidate basis functions down to compact models.
	
	A short overview of the most important literature methods to solve SR is given in Table~\ref{tab:gp-based}.  
	
	\begin{table}[!h]
		\centering
		\scalebox{0.7}{
			\begin{tabularx}{550pt}{l  l  X}  
				Algorithm          &   Paper/year &   Short details   \\ \hline
				\textsc{GP}        &    \cite{koza1994genetic} (1994)    & Application of GP to SR \\ 
				\textsc{Hybrid-GP}  &   \cite{raidl1998hybrid} (1998)   & GP-based method; solutions are locally optimized with OLS to find optimum coefficients for top-level terms \\ 
				\textsc{DFE}        & \cite{cerny2008using} (2008)   & Differential evolution algorithm \\
				\textsc{APF-FE}                & \cite{schmidt2009distilling, schmidt2011machine} (2009, 2011) & Age-fitness Pareto optimization approach using co-evolved fitness estimation   \\
				APF            &      \cite{schmidt2010age} (2010)        &  Age-fitness Pareto optimization approach                        \\
				\textsc{FFX}  & \cite{mcconaghy2011ffx} (2011)  & The fast function extraction algorithm --  non-evolutionary technique based on a machine learning technique called path-wise regularized learning \\
				\textsc{ABCP} &  \cite{karaboga2012artificial} (2012)  & Artificial bee colony programming  approach \\
				\textsc{EPLEX}          &      \cite{la2016epsilon} (2016)         &   A parent selection method called $\epsilon$-lexicase selection  \\ 
				\textsc{MRGP} & \cite{arnaldo2014multiple} (2014) & It decouples and
				linearly combines a program's subexpressions via multiple regression on the target variable  \\
				Local optimization NLS    & \cite{kommenda2018local} (2018)   & Constants optimization in GP by nonlinear least squares \\
				\textsc{FEAT} & \cite{la2018learning} (2018) & Features are represented as networks of multi-type expression trees comprised of activation functions; differentiable features are trained via gradient descent \\
				\textsc{SBP-GP} &  \cite{virgolin2019linear} (2019)  & The idea of semantic back-propagation utilized in GP \\
				\textsc{BSR} & \cite{jin2019bayesian} (2019) & ML-based approach; Bayesian symbolic regression  \\ 
				\textsc{DSR}  & \cite{petersen2019deep} (2019) & Deep symbolic regression based on a RNN approach further utilizing the policy gradient search \\
				\textsc{Operon} & \cite{kommenda2020parameter} (2020) &  Utilizing nonlinear least squares for parameter identification of SR models with LS \\
				\textsc{VNP}    & \cite{elleuch2020variable} (2020) & VNS based GP approach \\
				\textsc{OccamNet} & \cite{costa2020fast} (2020) &   A fast neural network approach; the model defines a probability distribution over a non-differentiable function space; it samples functions and updates the weights with back-propagation  based on cross-entropy matching in an EA strategy	 \\
				
				\textsc{AI-Feynman} & \cite{udrescu2020ai} (2020) & A physics-inspired divide-and-conquer method; it also uses neural network fitting \\
				\textsc{GOMEA}  & \cite{virgolin2021improving} (2021)    & A model-based
				EA framework called gene-pool optimal mixing evolutionary algorithm \\
				\textsc{ITEA} & \cite{de2021interaction} (2021)   & EA based approach called the interaction-transformation EA   \\
				\textsc{SA} & \cite{kantor2021simulated} (2021) &  Simulated annealing approach \\
				
				\textsc{DRLA} & \cite{mundhenk2021symbolic} (2021)  &    A deep reinforcement learning approach enhanced with genetic programming \\
				\textsc{Taylor-GP} &  \cite{he2022taylor} (2022)  &  Taylor polynomials approximations  \\ \hline
				
				
		\end{tabularx} }
		\caption{SR methods overview.}
		\label{tab:gp-based}
	\end{table}
	
	
   The SR researches lack uniform, robust, and transparent
	benchmarking standards. Recently, La Cava et al.~\cite{la2021contemporary} proposed an
	open-source, reproducible benchmarking platform for SR, called SRBench. The authors extended  PMLB~\cite{olson2017pmlb},  a repository of standardized regression problems, with 130 SR datasets for which exact (\emph{ground-truth}) models are known; see more in the aforementioned paper. In their extensive experimental evaluation, 14
	symbolic regression methods and 7 machine learning methods are compared on the set of 252 diverse regression problems. The remaining 122 SR datasets belong to a class of so-called \emph{black-box} problems for which the \emph{ground-truth} is unknown. One of the most interesting conclusions from La Cava et al. research is that algorithms specialize in either solving \emph{ground-truth} problems, or \emph{black-box} problems, but not both. 
	
	In this work we present a novel approach to solve \emph{ground-truth}-based SR, which combines the popular iterated local search meta-heuristic (ILS)~\cite{lourencco2003iterated,lourencco2019iterated} with the ordinary least square method (OLS)~\cite{leng2007ordinary}. ILS mostly handles combinatorial (discrete) aspects of search space, while OLS helps in the process of a coefficient determination, so it handles some continuous parts of the search space. As will be shown later, the proposed method has shown its robustness w.r.t. introduced noise, therefore we called the method \textsc{RILS}-\textsc{ROLS}  (with letters R corresponding to regression and robust). Additionally, in order to navigate the search toward exact model (and not only accurate one) the algorithm is equipped with a carefully constructed fitness function that utilizes three important model characteristics: RMSE and $R^2$ scores which participate into models accuracy, and a weighted solution size that penalizies too complex models. 
	
	The summary of main contributions is as follows: 
	
	\begin{enumerate}
		\item The proposed method outperforms 14 comparison methods on two \emph{ground-truth} benchmark sets from literature -- these methods and benchmarks are used in SRBench platform.  
		
		\item The method shows its high robustness, which is proved by its comparison to the other algorithms in the presence of different levels of Gaussian white noise in the input data. 
		
		\item The method is very efficient, taking in average less than 4 minutes to reach the exact solution, on the problem instances where it was possible. 
		
		\item A new set of unbiased instances is introduced -- it consists of randomly generated formulae of various sizes and number of input variables. This set was employed to analyze the effect of the model size and the level of noise on the solving difficulty. 
	\end{enumerate}
	
	
	\section{Problem definition and search space}
	\label{sec:search-space}
	In this section we formally define the SR problem.
	
	\begin{definition}
		Given is a dataset $D = \{(\mathbf{x_i}, y_i)\}_{i=1}^n$, where $\mathbf{x_i} \in \mathbb{R}^d$ represents i-th feature (input) vector while $y_i \in \mathbb{R}$ is its corresponding target (output) variable. Suppose that there exists an analytical model of the form $f(\mathbf{x})= g^*(\mathbf{x}, \theta^*) + \epsilon $ that is a generator of all observations from $D$.  
		The goal of SR is to learn a mapping $\tilde{f}(\mathbf{x})=  \tilde{g}(\mathbf{x}, \tilde{\theta})  \colon \mathbb{R}^d \mapsto \mathbb{R}$  estimated by searching through the space of (mathematical) expressions  $\tilde{g}$ and parameters $\tilde{\theta}$ where  $\epsilon$ is observed white noise within the given input data. 
		
	\end{definition}
	
	Koza~\cite{koza1994genetic} introduced the problem of SR as a specific application of genetic programming. GP can be used to optimize nonlinear structures such as computer programs. In particular, the programs are represented by syntax trees consisting of functions/operations over input features and constants. As an example of a function represented by a syntax tree, see Figure~\ref{fig:syntax-tree-example}. In essence, all valid syntax trees form the solution search space of SR. That is, each sample model $\tilde{f}$ may be seen as a point in the search space, represented by respective syntax tree. The solution accuracy can be computed on the basis of historical data $D$ and the chosen error measure such as $MSE$, $RMSE$, $R^2$, some combination of them, etc. Interestingly, the SR search space nature is twofold: discrete and continuous. It is primarily modeled as a problem of discrete (combinatorial) optimization, since the number of possible solution functional forms is countable. However, it may also include elements solved by means of continuous (global) optimization, e.g., constants (coefficients) fitting. It is common to use the set of the following elementary mathematical functions: $\sqrt{x}, x^2 $, $\sin$, $\cos$, $\log$, $\exp$, $\arcsin$, $\arccos$, $a^x$, and a set of standard arithmetic operators: $+$, $-$, $\cdot$, and $/$. 
	
	\begin{figure}[!ht]
		\centering
		\begin{tikzpicture}
			[font=\small, edge from parent, 
			every node/.style={top color=white, bottom color=blue!25, 
				rectangle,rounded corners, minimum size=6mm, draw=blue!75,
				very thick, drop shadow, align=center},
			edge from parent/.style={draw=blue!50,thick},
			level 1/.style={sibling distance=3cm},
			level 2/.style={sibling distance=1.2cm}, 
			level 3/.style={sibling distance=1cm}, 
			level distance=2cm,
			]
			\node (A) {$\cdot$} 
			child { node (B) {$\sin$}
				%child { node {x} 
					%edge from parent node[left=.5em,draw=none] {} }
				child { node {$x$}}
			}
			child {node (C) {$\cdot$}
				%child { node {x}
					%	child { node {C1a}}
					%}
				child { node {$x$}}
				child { node {$/$}
					child { node {$x$}}
					child { node {$y$} %edge from parent node[right=.5em,draw=none] {$\frac{a}{b}$}
					}
				}
			};
			%	child { node {+} 
				%	child { node {2}}
				%	child { node {$x$}}
				%};
			
		\end{tikzpicture}
		
		\caption{Syntax tree representation for the expression $\sin{x} \cdot   ( x \cdot  x / y  )  = \frac{x^2 \sin x }{y}$}
		\label{fig:syntax-tree-example}
	\end{figure}
	
	%\section{Literature review}\label{sec:lit-rev}
	
	
	\section{The proposed RILS-ROLS method}\label{sec:rils}
 
 	Our method relies on the following operations: $+$, $-$, $\cdot$, $/$, $\sqrt{x}$, $x^2 $, $\sin$, $\cos$, $\log$, $\exp$, $a^x$, where $a^x$ is only consequently used -- it is never explicitly introduced in the search space. Beside this, the following set of constants enters the search space explicitly: $-1$, $0$, $1$, $2$, $\pi$ and $10$. 
	Before we give details on our   method for solving SR, we will explain two building blocks of \textsc{RILS-ROLS}: 1) iterated local search (ILS) and 2) ordinary least squares (OLS). 
	
	\subsection{Iterated local search}
	ILS~\cite{lourencco2003iterated} is an efficient meta-heuristics that iteratively generates a sequence of solutions produced by the (embedded) heuristic, such as local search (LS) or randomized greedy heuristics. When the search gets \emph{stuck} in local optimum, a \emph{perturbation} is performed -- this step is usually non-deterministic. This simple idea was proposed by Baxter~\cite{baxter1981local} in early 1980s, and, since then, has been  re-invented by many researches under different names. Some of the following names were used: iterated descent search~\cite{baum1998iterated}, large-step
	Markov chains~\cite{martin1991large}, chained local optimization~\cite{martin1996combining}, or, in some cases, combinations of previous~\cite{applegate2003chained}. The most popular version of ILS is shown in Algorithm~\ref{alg:ils}. (Note that we use this version of ILS as the backbone of our \textsc{RILS}-\textsc{ROLS} algorithm.)
	
	\begin{algorithm}
		\begin{algorithmic}[1] 
			\State \textbf{Input}: problem instance
			\State \textbf{Output}: feasible solution 
			\State $s \gets$ \texttt{Initialize}$()$
			\State  $s \gets$ \texttt{LocalSearch}($s$)
			\While{\emph{stopping criteria are not met}}
			\State  $s' \gets$ \texttt{Perturbation}($s$)
			\State  $s' \gets$ \texttt{LocalSearch}($s'$)
			\State  $ s \gets$ \texttt{AcceptanceCriterion}($s, s'$)
			\EndWhile
			\State \Return $s$
		\end{algorithmic}
		\caption{General ILS method.}
		\label{alg:ils}
	\end{algorithm}  
	
	An initial solution may be generated randomly or by using a greedy heuristic, which is afterwards, improved by local search. At each iteration, ILS applies three steps. First, the current incumbent solution $s$ is perturbed -- it is partially randomized, yielding new solution $s'$. Next, the solution $s'$ is potentially improved by a LS procedure. Thirdly, the newly obtained solution $s'$  possibly becomes new incumbent -- this is decided upon the acceptance criterion. Sometimes, ILS incorporates a mechanism of tabu list, which prevents the search getting back into already visited solutions.  		 
	
	\subsection{Ordinary least square method}\label{sec:ols}
	Ordinary least square method (OLS) is a linear regression technique. It is based on applying the least-square method to minimize the square residual (error) sum  between actual and predicted values (given by the model). More precisely, given previously introduced dataset $D$ of $n$ points $(\mathbf{x_i}, y_i)$ where each $\mathbf{x_i}$ is $d$-dimensional, the task is to determine linear mapping $\tilde{y} = \mathbf{k} \mathbf{x} + b$, that is coefficients (line slope) $\mathbf{k} = (k_1, \ldots, k_d)$ and $b$ (intercept), so that $ \sum_{i}^{n} (\tilde{y}_i - y_i)^2 $ is minimized. This sum is also know as the sum of squared errors (SSE). There are many methods to minimize SSE. One of the analytical approaches is the calculus-oriented -- it takes into account partial derivatives of SSE w.r.t. $k_j,\; j \in \{1, ..., d\}$: 
	
	$$  \frac{\partial}{\partial k_j} \sum_{i}^{n} (\tilde{y}_i - y_i)^2 = \frac{\partial}{\partial k_j} \sum_{i=1}^{n} ( \mathbf{k}\mathbf{x_i}+b  - y_i)^2 =  \sum_{i=1}^{n} 2x_{ij}(\mathbf{k}\mathbf{x_i} + b - y_i).$$ 
  
	Similarly, by taking into account  partial derivation w.r.t. $b$, we have: 
	$$  \frac{\partial}{\partial b} \sum_{i=1}^{n} (\tilde{y}_i - y_i)^2  = \frac{\partial}{\partial b} \sum_{i=1}^{n} ( \mathbf{k}\mathbf{x_i}+b  - y_i)^2 = \sum_{i=1}^{n} -2  (y_i - \tilde{y}_i). $$
	
	Finally, by setting each partial derivative to zero, we get the system of $d+1$ linear equations with $d+1$ unknowns ($\mathbf{k}=(k_1, ..., k_d)$ and $b$): 
	\begin{align*}
		& \sum_{i=1}^{n} -2x_{i1} (y_i - \tilde{y}_i) = 0 \\
		& \vdots \\
		& \sum_{i=1}^{n} -2x_{id} (y_i - \tilde{y}_i) = 0 \\
		&\sum_{i=1}^{n} -2  (y_i - \tilde{y}_i) = 0.
	\end{align*}
	%The above system can be transformed to 
	%\begin{align*}
	%	 & \sum y_i \cdot x_i   = b\sum x_i \cdot x_i  +  k \cdot \sum x_i \\
	%	 & \sum y_i  = k \sum x_i + b \cdot d. 
	%  \end{align*}

%https://www.thekerneltrip.com/machine/learning/computational-complexity-learning-algorithms/
%https://math.stackexchange.com/questions/84495/computational-complexity-of-least-square-regression-operation %https://levelup.gitconnected.com/train-test-complexity-and-space-complexity-of-linear-regression-26b604dcdfa3
To solve this system one can use Cholesky  factorization or LU decomposition for matrix inversion, and Winograd or Strassen algorithm for matrix multiplication, depending on a solver of linear equations chosen as well as the sparsity of the matrix associated to the linear system~\cite{krishnamoorthy2013matrix}. For example, by using Cholesky decomposition, solving the system requires a matrix inversions and matrix multiplications. In that case the computation cost of $O(d^3)$ is required to find an inverse of the matrix associated to the linear system, which is the most consuming part of the algorithm. Thus, OLS using close-form matrix techniques performs in $O(d^3)$ time complexity.  %, where $d$ stands for the number of input features. 
It is worth mentioning that when the number of input features becomes large, these closed-form matrix techniques are usually replaced by iterative ones, such as gradient descent (GD)~\cite{andrychowicz2016learning}. When using GD the complexity becomes $O(n^2d )$.
% The learning rate in GD has to be carefully chosen, as it dictates how quick the algorithm is and how fast it converges. 

\subsection{\textsc{RILS}-\textsc{ROLS}  method}
Now we will explain the proposed \textsc{RILS}-\textsc{ROLS} method in detail. The overall method scheme is given in Algorithm~\ref{alg:rilsrols}.   

\begin{algorithm}
	\footnotesize
	\hspace*{\algorithmicindent} \textbf{Input}: input training dataset $D_{tr}$ \\
	\hspace*{\algorithmicindent} \textbf{Control parameters}: size penalty $penalty_{size}$, error tolerance $tolerance_{error}$ \\
	\hspace*{\algorithmicindent} \textbf{Output}: best symbolic formula solution $bs$
	\begin{algorithmic}[1] 
		\Procedure{\textsc{RILS}-\textsc{ROLS} }{$D_{tr}$}
		\State $n_{tr} \gets |D_{tr}|$
		\State $sample_{size} \gets $ \texttt{InitialSampleSize}($n_{tr}$)
		\State $D_{tr}' \gets$ \texttt{Sample}($D_{tr}, sample_{size}$)
		\State $s \gets NodeConstant(0)$ 
		\State $s_{fit} \gets$ \texttt{Fitness}($s, D_{tr}'$)
		\State $bs, bs_{fit} \gets s, s_{fit}$ \label{line:solSet}
		\State $start_{tried}, perturbations_{tried} \gets \emptyset, \emptyset$
		\While{\emph{stopping criteria is not met}}
		\State $start_{tried} \gets start_{tried} \cup \{s\}$
		\State $s_{perturbations} \gets $ \texttt{All1Perturbations}($s$) \label{line:sPert}
		\State $s_{perturbations} \gets $ \texttt{FitOLS}($s_{perturbations}$, $D_{tr}'$)
		\State $s_{perturbations} \gets $ \texttt{OrderByR2}($s_{perturbations}$) \label{line:orderR2}
		\State $improved \gets false $
		\For{$p \in s_{perturbations}$}
		\If{$p \in perturbations_{tried}$}
		\State \textbf{continue}
		\EndIf
		\State $p \gets $ \texttt{Simplify}($p$) \label{line:simp}
		\State $p \gets $ \texttt{LocalSearch}($p$, $D_{tr}'$) \label{line:ls}
		\State $p_{fit} \gets$ \texttt{Fitness}($p$, $D_{tr}'$)
		\If{$p_{fit} < bs_{fit}$} \label{line:avoid1}
		\State $bs, bs_{fit}, improved \gets p, p_{fit}, true$ // new best solution
		\State \textbf{break}
		\EndIf \label{line:avoid2}
		\State $perturbations_{tried} \gets perturbations_{tried} \cup \{p\}$
		\EndFor
		\If{improved}
		\State $s \gets bs$ \label{line:improved}
		\Else
		\State $start_{candidates} \gets $ \texttt{All1Perturbations}($bs$)
		\If{$start_{candidates} \setminus start_{tried} = \emptyset$} // all 1-perturbations around $bs$ tried
		\State $s' \gets $ \texttt{RandomPick}($start_{candidates}$)  \label{line:pert21}
		\State $start_{candidates} \gets $ \texttt{All1Perturbations}($s'$) \label{line:pert22} // 2-permutations of \textit{bs}
		\EndIf
		\State $s \gets $ \texttt{RandomPick}($start_{candidates} \setminus start_{tried}$) \label{line:randPick}
		\If{not improved for too many iterations}
		\State $sample_{size} \gets$ \texttt{IncreaseSampleSize}($sample_{size}, n_{tr}$) \label{line:sampleAdj1}
		\State $D_{tr}' \gets$ \texttt{Sample}($D_{tr}, sample_{size}$)\label{line:sampleAdj2}
		\EndIf
		\EndIf
		\If{$R^2$ almost 1 and RMSE almost 0 w.r.t. $tolerance_{error}$}
		\State \textbf{break} // early exit
		\EndIf
		\EndWhile
		\State $bs \gets $ \texttt{Simplify}($bs$)
		\State $bs \gets $ \texttt{RoundModelCoefficients}($bs$)
		\State \Return $bs$
		\EndProcedure
	\end{algorithmic}
	\caption{\textsc{RILS}-\textsc{ROLS}  method.}
	\label{alg:rilsrols}
\end{algorithm}  

\textsc{RILS}-\textsc{ROLS}  algorithms receives training dataset $D_{tr}$ as input. In addition, it has two control parameters: $penalty_{size}$ and $tolerance_{error}$. The first one quantifies the importance of solution expression complexity in the overall solution quality measure (more on this in Section~\ref{sec:fitness}). The second parameter is related to the expected noise level in data -- higher noise means that tolerance to errors should be higher.
$D_{tr}$ can have very large number of records, so the first step of \textsc{RILS}-\textsc{ROLS}  is to choose a random sample $D_{tr}' \subseteq D_{tr}$ of size $sample_{size}$. Initial the sample size is chosen as $\max(0.01 \cdot n_{tr}, 100)$. The size of sample is later dynamically adjusted through the algorithm's iterations -- when there are no solution improvements for some number of iterations, the sample size is doubled (lines~\ref{line:sampleAdj1}-\ref{line:sampleAdj2} of Algorithm~\ref{alg:rilsrols}).

As previously stated, solution is usually represented by means of a tree. We use a simple solution initialization -- tree root node is set to zero constant. We interchangeably use two solution variables: ($i$) $s$ denotes starting (or working) solution and ($ii$) $bs$ stands for the best solution so far, also known as the incumbent solution. Solution quality is measured by evaluating the fitness function (more about it in the subsequent Section~\ref{sec:fitness}). Before entering the main loop, the best solution \emph{bs} is set to the initial solution (line~\ref{line:solSet}). 


Main loop iterates as long as none of termination criteria is met: ($i$) maximal running time has been reached; ($ii$) maximal number of fitness calculations has been made; ($iii$) best solution is sufficiently good, 
w.r.t. its $R^2$ and $RMSE$ scores. More precisely, if $R^2$ is sufficiently close to 1 and, at the same time, $RMSE$ is sufficiently close to 0, the algorithm stops prematurely -- this significantly reduces the running times for the majority of tested instances.
The sufficiency is controlled with parameter $tolerance_{error}$ -- smaller value means that $R^2$ and $RMSE$ should be closer to 1 and 0, respectively. 


One of the first steps in the main loop is to generate perturbations near the starting solution $s$ (line~\ref{line:sPert}). 
As the name of this procedure (\texttt{All1Perturbations}) suggests, perturbation step is local -- meaning the closeness of starting solution $s$ and any of perturbations is 1 (we call it 1-perturbation sometimes). The precise way of generating perturbation is described separately, in Section~\ref{sec:pertGen}. 
 

Candidate perturbations are being improved by performing OLS coefficient fitting (procedure \texttt{FitOLS}). This means that coefficients in any of linear combinations of current solution are being set by applying  ordinary least square method, already described in Section~\ref{sec:ols}. 
After this step, perturbations are usually better suited to given sample data $D_{tr}'$. 
Further, these perturbations are sorted w.r.t. $R^2$ metric in the descending order (line~\ref{line:orderR2}). 

Now, the algorithm enters the internal loop -- it iterates over the ordered perturbations and aims to find the one which improves the best solution $bs$. But, before comparing candidate perturbation solution $p$ with $bs$, $p$ is first simplified (line~\ref{line:simp}), after which the local search is performed (line~\ref{line:ls}).
Solution simplification is done in a symbolical fashion by popular \texttt{SymPy} Python package~\cite{sympy}.
Local search tries to find local optima expressions close to the given $p$ -- explained in detail in Section~\ref{sec:ls}.  
Finally, the fitness function value of $p$ is compared to the fitness function value of $bs$. If fitness of $p$ is better, $bs$ is updated correspondingly and the internal loop, that goes across ordered perturbations, is immediately terminated (it works in a  \emph{first-improvement} strategy). Otherwise, the next perturbation is probed. 
Note that the probed perturbations are stored in a set denoted by $perturbations_{tried}$. The goal is to avoid checking the same perturbation multiple times (lines \ref{line:avoid1}-\ref{line:avoid2}), i.e. $perturbations_{tried}$ serves as a kind of tabu list, which is known from the Tablu search meta-heuristic, see~\cite{glover1998tabu}.    


If some of $s_{perturbations}$ around starting solution $s$ yielded an improvement, $bs$ becomes the starting solution $s$ in the next iteration of the main loop (line~\ref{line:improved}). 
Otherwise, it makes no sense to set starting solution to $bs$, as nothing changed -- the search is being  \emph{trapped} in a local optimum. Randomness is introduced in order to avoid this undesired situation. First, a set of local perturbations around $bs$ is generated ($start_{candidates}$) in the same manner as before (procedure \texttt{All1Perturbations}). If at least one of these was not previously used as a starting solution ($start_{tried}$), a single perturbation from the $start_{candidates} \setminus start_{tried}$ is randomly picked (line~\ref{line:randPick}). There is a minor chance that $start_{candidates} \setminus start_{tried} = \emptyset$. When that happens, the set of starting solution candidates is equal to perturbations of some randomly selected perturbation of $bs$ (lines \ref{line:pert21}-\ref{line:pert22}) -- which effectively means that the perturbations with distance 2 from $bs$ are used. There is a very small chance that these 2-perturbations will have an empty set difference with $start_{tried}$ set of starting solutions. Therefore, 3-perturbations are not considered in our algorithm. 

Before returning the final symbolical model, \textsc{RILS}-\textsc{ROLS}  performs the final symbolical simplification and rounding of model coefficients. The latter is sensitive to control parameter $tolerance_{error}$ -- a smaller value means the information loss during rounding is smaller, i.e. a higher number of significant digits is preserved. Note that confidence in the symbolical model is reduced when expected noise in data is higher -- the rule of thumb is: for higher expected noise $tolerance_{error}$ should be higher. 

%\fxnote{Dati neku smislenu procjenu kompleksnosti algoritma, ako to nije pretesko. Aca: zasad bih ovo izbegao, nezgodno je. Ako budu trazili recenzenti, uradicemo.}

\subsubsection{Fitness function}\label{sec:fitness}

Symbolic regression objective is to determine the expression fitting available data. It is also allowed to obtain some equivalent expression, since there are multiple ways to express some symbolical equation. Although logically sound and intuitive, this objective is not quantifiable during the solution search/training phase, because the goal expression is not known at that point -- only target values for some of the input data are known. Thus, various numerical metrics are being used in literature to guide the symbolic regression search process. The most popular are the coefficient of determination, also known as $R^2$, and the mean squared error (MSE) or root mean squared error (RMSE). Also, the important aspect of the solution quality is the solution expression complexity, which may correspond to the model size of its tree representation. This follows the Occam's razor principle~\cite{costa2020fast}~ that simpler solution  is more likely to be a correct one. 
The search process of \textsc{RILS}-\textsc{ROLS}  is guided by the non-linear combination of $R^2$, RMSE and solution expression size (complexity) presented in Equation~\ref{eq:fitness}. 
 
\begin{equation}
	\label{eq:fitness}
	fit(s) = (2-R^2(s)) \cdot (1+RMSE(s)) \cdot (1+penalty_{size} \cdot size(S))
\end{equation}

Since the presented fitness function needs to be minimized, the following conclusions may be drawn:
\begin{itemize}
	\item higher $R^2$ is preferred -- ideally, when $R^2(s)=1$, the effect of term $2-R^2(s)$ is neutralized; %since 1 is neutral for multiplication;
	\item lower RMSE is preferred -- ideally, when $RMSE(s)=0$, the whole term $(1+RMSE(s))$ becomes 1;
	\item since $penalty_{size} > 0$, larger expressions tend to have higher fitness (which follows the Occam's razor principle); therefore, simpler solutions are favorable. 
\end{itemize}

The size of expression is calculated by counting all nodes in the expression tree -- this includes leaves (variables and constants) and internal nodes (operations). 

\subsubsection{Expression caching}

In order to speed-up the fitness evaluation, we employ \emph{expression caching}. This means that values attached to expression trees or subtrees are persisted in key-value structure, such that the key is tree (subtree) textual representation, while the value is the $|D_{tr}'|$-size  vector of corresponding expression values on the sample training dataset $D_{tr}'$. (Of course, once the $D_{tr}'$ changes, which happens not that frequently, the whole cache is cleared.) 
Caching is performed in a partial way -- when determining the value of a given expression $T$, it is not required to find the exact \emph{hit} inside the cache. So, if some subexpression (subtree) of $T$ is present in the cache, its value will be reused and further combined to calculate the whole fitness function value. 

For example, let  $T = y^2 \cdot (x+y)/z - \sin(x+y)$ be an expression, and $D_{tr}'=\{([1, 2, 5], 7), ([3, 4, 3], 5), ([4, 5, 3], 6), ([6, 7, 4], 3), ([3, 3, 6], 2)\}$ be a sample training dataset (here, input feature vector is labeled by $[x, y, z]$); let expression cache consisting of the following key-value entries $cache=\{(x+y, [3, 7, 9, 13, 6]), (y^2, [4, 16, 25, 49, 9])\}$.  
Expression $T$ does not need to be fully evaluated since some of its parts are inside the cache: $y^2$ and $x+y$. (Note that single variables do not need to enter the cache, since they are already available as columns of  $D_{tr}'$.)
Each newly evaluated (sub)expression (except constant or variable alone) enters the cache. In this example, the new entries will correspond to keys $(x+y)/z$, $y^2 \cdot (x+y)/z$, $\sin(x+y)$ and $y^2 \cdot (x+y)/z - \sin(x+y)$.  
The maximal number of cache entries is set to 5000. Once this number is reached, the cache is cleared. 

\subsubsection{Perturbations}\label{sec:pertGen}

Perturbations allow the algorithm to escape from local optima. As previously described, perturbations are performed in two occasions: ($i$) during the exhaustive examination of neighboring solutions around the starting solution, ($ii$) during selection of the next starting solution, a non-exhaustive case.  
In both cases, the same Algorithm~\ref{alg:pert} is used. 

\begin{algorithm}
	\hspace*{\algorithmicindent} \textbf{Input}: solution $s$ \\
	\hspace*{\algorithmicindent} \textbf{Output}: local perturbations (1-perturbations) of solution $s$ -- $s_{perturbations}$
	\begin{algorithmic}[1] 
		\Procedure{All1Perturbations}{$s$}
		\State $s_{perturbations} \gets \emptyset$ \label{line:pertInit}
		\State $s \gets$ \texttt{NormalizeConstants}($s$)
		\State $s \gets$ \texttt{Simplify}($s$)
		\State $s_{subtrees} \gets$ \texttt{SubTrees}($s$)
		\For{$n \in s_{subtrees}$} \label{line:forSSStart}
		\State $n_{perturbations} \gets$ \texttt{All1PerturbationsAroundNode}($s$, $n$)
		\State $s_{perturbations} \gets s_{perturbations} \cup n_{perturbations}$	
		\EndFor \label{line:forSSEnd}
		\State \Return $s_{perturbations}$
		\EndProcedure
	\end{algorithmic}
	\caption{Generation of all 1-perturbations of a given solution.}
	\label{alg:pert}
\end{algorithm}  

Initially, the set of perturbations $s_{perturbations}$ is empty (line~\ref{line:pertInit} of Algorithm~\ref{alg:pert}).


This is followed by constant normalization during which coefficients that enter multiplication, division, addition or subtraction are set to 1, while those entering the power function are rounded to integer, with exception of square root, which is kept intact. For example, for expression $3.3\cdot(x+45.1\cdot y^{3.2})\cdot 81\cdot x/\sqrt{y}$ the normalized version is $1\cdot (x+1\cdot y^3)\cdot 1\cdot x/\sqrt{y}$. The reason for performing normalization is reducing the search space of possible perturbations. This reduction is reasonable, since normalization preserves the essential functional form. (Note that coefficients get tuned later: the linear coefficient during the OLS phase, and the remaining during local search.)


After performing the normalization process, the expression is simplified -- getting compact expression is more likely after normalization than before. The previous expression will take form $(x+y^3)\cdot x/\sqrt{y}$. In this particular case, the simplification will usually only remove unnecessary coefficients, but in general it can also perform some non-trivial symbolic simplification. 


Perturbations are generated by making simple changes on the per-node level of $s$ expression tree.
Depending on the structure of the expression tree (note that expression does not have to have unique tree representation), the set of subtrees of the previous expression $(x+y^3)\cdot x/\sqrt{y}$ might be $\{(x+y^3)\cdot x/\sqrt{y}, (x+y^3), x/\sqrt{y}, x, y^3, \sqrt{y}, y\}$. %This set of subtrees is obtained if left subtree of the whole expression is $(x+y^3)$ while the right one is $x/\sqrt{y}$.  
Further, the set of perturbations around each subtree $n$ is generated (lines \ref{line:forSSStart}-\ref{line:forSSEnd} in Algorithm~\ref{alg:pert}).   


Algorithm~\ref{alg:pertNode} shows how perturbations around given subtree are generated.

\begin{algorithm}	
	\hspace*{\algorithmicindent} \textbf{Input}: solution $s$, node $n$\\
	\hspace*{\algorithmicindent} \textbf{Output}: 1-perturbations of solution $s$ around node $n$
	\begin{algorithmic}[1]
		\Procedure{All1PerturbationsAroundNode}{$s$, $n$}
		\State $n_{perturbations} \gets \emptyset$
		\If{$n = s$} \label{alg:alg4-case-1}
		\State $n_{perturbations} \gets n_{perturbations} \cup$ \texttt{NodeChanges}($n$)
		\EndIf
		\If{$n.arity \geq 1$}\label{alg:alg4-case-2}
		\State $n_{changes} \gets$ \texttt{NodeChanges}($n.left$)
		\For{$nc \in n_{changes}$}
		\State $new \gets$ \texttt{Replace}($s$, $n.left$, $nc$)
		\State $n_{perturbations} \gets n_{perturbations} \cup \{new\}$
		\EndFor
		\EndIf
		\If{$n.arity = 2$}\label{alg:alg4-case-3}
		\State $n_{changes} \gets$ \texttt{NodeChanges}($n.right$)
		\For{$nc \in n_{changes}$}
		\State $new \gets$ \texttt{Replace}($s$, $n.right$, $nc$)
		\State $n_{perturbations} \gets n_{perturbations} \cup \{new\}$
		\EndFor
		\EndIf
		\State \Return $n_{perturbations}$
		\EndProcedure
	\end{algorithmic}
	\caption{Generation of 1-perturbations of a given solution around given node.}
	\label{alg:pertNode}
\end{algorithm}

It can be seen that there are three possibly overlapping cases when performing perturbations on the per-node level. 

\emph{Case 1}. 
Observed node $n$ is the whole tree $s$ (see line ~\ref{alg:alg4-case-1} in Algorithm~\ref{alg:pertNode}). Following on  the previous exemplary expression tree, this means that multiplication node that connects $(x+y^3)$ and $x/\sqrt{y}$ is to be changed. For example, multiplication can be replaced by addition, which forms a 1-perturbation expression (tree) $(x+y^3)+ x/\sqrt{y}$. 


\emph{Case 2}. 
Node $n$ has arity of at least 1 (see line ~\ref{alg:alg4-case-2} in Algorithm~\ref{alg:pertNode}). This means that the left subtree exists, so the left subtree node is to be changed. 
For example, if $n=x+y^3$, the overall perturbation might be $(x/y^3)+x/\sqrt{y}$ (addition is replaced by division). 
Another example would be the case of unary operation, e.g., when $n=\sqrt{y}$. In that case, some of possible perturbations could be $(x+y^3)\cdot x/\sqrt{\ln{y}}$ (application of logarithm to left subtree $y$) or $(x+y^3)\cdot x/\sqrt{x}$ (changing variable $y$ to $x$), etc.

\emph{Case 3}. 
Node $n$ is binary operation, meaning the right subtree must exist (Line ~\ref{alg:alg4-case-3} in Algorithm~\ref{alg:pertNode}). 
The analogous idea is applied as in \emph{Case 2}. 

The algorithm allows the following set of carefully chosen per-node changes (method named \texttt{NodeChanges} in Algorithm~\ref{alg:pertNode}): 

\begin{enumerate}
	\item Any node to any of its subtrees (excluding itself). For example, if $(x+y^3)$ is changed to $x$, the perturbation is $(x+y^3)\cdot x/\sqrt{y} \rightarrow x\cdot x/\sqrt{y}$. 
	\item Constant to variable. For example,  $(1+y^3)\cdot x/\sqrt{y} \rightarrow (x+y^3)\cdot x/\sqrt{y}$.
	\item Variable to unary operation applied to that variable. For example,  $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+y^3)\cdot \ln{x}/\sqrt{y}$.
	\item Unary operation to another unary operation. For example,  $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+y^3)\cdot x/\sin{y}$.
	\item Binary operation to another binary operation. For example,  $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+y^3)\cdot (x + \sqrt{y})$. 
	\item Variable or constant enter the binary operation with arbitrary variable. For example,  $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+x/y^3)\cdot x/\sqrt{y}$. 
\end{enumerate}

Method named \texttt{Replace}$(s, n, nc)$ inside Algorithm~\ref{alg:pertNode} is simply used to replace the node $n$ with node $nc$ inside the given expression tree $s$.    
%\fxnote{Nekako zbunjuje u Algoritmu 4, pojava NodeChanges)() i Replace(). Mislim, razumijem iz opisa sta one rade, ali nisam siguran da treba imenovati metode, ako ih barem u kontekstu opisa ne pomenemo direktno po nazivu. ACA: za NodeChanges stoji opis operacija iznad, sad sam ubacio i u tekst da je to sustina metode NodeChanges. Za replace sam takodje dao objasnjenje.}


%\fxnote{Dati kompleksnost velicine skupa perturbacija u odnosu na velicinu drveta i dopustene promjene. ACA: tesko je ovo izracunati, u principu za svaki cvor, dakle, celo drvo ili bilo koje pod-drvo ide ovaj spisak gore, dakle, neki red velicine d*C(T), gde je C(T) broj poddrveca iz datog drveta T, a d je neka konstanta, tj. broj mogucih perturbacija jednog poddrveta. Sad koliko ima poddrveca, moze se izracunati ovako nekako, ali ne bih to ukljucivao osim ako nam ne zatraze. Dakle, za drvo T broj poddrveca $C(T) = C(T.left)+C(T.right)+1$ (ovo jedan za bas to celo drvo u datom rekurzivnom koraku), sad kad je drvo T od N cvorova balansirano, dobija se diferencna jednacina $C(N)=2*C(N/2)+1$, a njeno resenje je O(N) mislim. Kad se raspise dobija se $C(N) = 2*2*C(N/4)+2 = 2*2*2*C(N/8)+3 = ... = 2^(\log_2(N))*C(1)+\log_2(N) = N + \log_2(N) = O(N)$ posto je $C(1)=1$, to je list drveta}

\subsubsection{Local search}\label{sec:ls}

Perturbations are further improved by means of local search procedure (Algorithm~\ref{alg:ls}). 

\begin{algorithm}
	\hspace*{\algorithmicindent} \textbf{Input}: perturbation $p$, sample training dataset $D_{tr}'$ \\
	\hspace*{\algorithmicindent} \textbf{Output}: local optimum $bp$ in the vicinity of perturbation $p$
	\begin{algorithmic}[1] 
		\Procedure{LocalSearch}{$p$, $D_{tr}'$}
		\State $bp, bp_{fit} \gets p,\ $\texttt{Fitness}$(p,D_{tr}')$ 
		\State $improved \gets true$
		\While{$improved$}
		\State $improved \gets false$
		\State $bp_{candidates} \gets \emptyset$
		\State $bp_{subtrees} \gets $\texttt{SubTrees}($bp$)
		\For{$n \in bp_{subtrees}$}
		\State $n_{candidates} \gets $ \texttt{All1PerturbationsAroundNodeExtended}($bp$, $n$)
		\For{$new \in n_{candidates}$}
		\State $new \gets$ \texttt{FitOLS}($new$, $D_{tr}'$)
		\State $new_{fit} \gets$ \texttt{Fitness}($new$, $D_{tr}'$)
		\If{$new_{fit} < bp_{fit}$}
		\State $bp, bp_{fit} \gets new, new_{fit}$
		\State $improved \gets true$
		\EndIf
		\EndFor
		\EndFor
		\EndWhile
		\State \Return $bp$
		\EndProcedure
	\end{algorithmic}
	\caption{Local search procedure.}
	\label{alg:ls}
\end{algorithm}  

For a given perturbation $p$, local search systematically explores extended set of 1-perturbations around $p$. It relies on the \emph{best-improvement} strategy, meaning that all 1-perturbations (for all subtrees) are considered. Before checking if the candidate solution ($new$) is better than the actual best $bp$, the OLS coefficient fitting (\texttt{FitOLS}) takes place.  
The set of used 1-perturbations is increased in comparison to those used previously. Namely, in addition to those six possible types of node changes, the following four are added:

\begin{enumerate}
	\setcounter{enumi}{6}
	\item Any node to any constant or variable. For example, $(x+y^3)\cdot x/\sqrt{y} \rightarrow y \cdot x / \sqrt{y}$ or  $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+\pi) \cdot x / \sqrt{y}$.
	\item Any node to unary operation applied to it. For example, $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+y^3)\cdot x/\ln{\sqrt{y}}$.
	\item Any node to binary operation applied to that node and some variable or constant. For example, $(x+y^3)\cdot x/\sqrt{y} \rightarrow (x+y^3)\cdot x/\sqrt{y} - x$.
	\item Constant to its multiple, where possible multipliers are $\{$0.01, 0.1, 0.2, 0.5, 0.8, 0.9, 1.1, 1.2, 2, 5, 10, 20, 50, 100$\}$. For example, $(1+y^3)\cdot x/\sqrt{y} \rightarrow (1.2+y^3)\cdot x/\sqrt{y}$.
\end{enumerate}

Change under number 10 is very important -- it performs the general coefficient tuning, unlike OLS that considers only coefficients in linear combinations.  

\section{Experimental evaluation}\label{sec:experiments}

Our \textsc{RILS}--\textsc{ROLS} algorithm is implemented in Python 3.9.0. All experiments concerning our method are conducted in the single-core mode, on a PC with Intel i9-9900KF CPU @3.6GHz, 64GB RAM, under Windows 10 Pro OS. The RAM consumption was very small (up to few hundred megabytes) -- beside memory space needed to load input dataset, the only considerable amount is reserved for expression cache. 


The following 13 algorithms are compared to our approach: \textsc{AI-Feynman}, \textsc{GOMEA}, \textsc{Afp-Fe}, \textsc{Itea}, \textsc{Afp}, \textsc{Dsr}, \textsc{Operon}, the \textsc{Gplearn} from python package \textsc{gplearn}~\cite{stephens2016genetic}, \textsc{Sbp-Gp}, \textsc{Eplex}, \textsc{Bsr}, \textsc{Feat}, \textsc{Ffx}, \textsc{Mrgp}. 

For the (13) competitors, the results are exported from SRBench \url{https://cavalab.org/srbench/results/}, reported in~\cite{la2021contemporary}. 
The maximum computation time allowed for each run of \textsc{RILS}-\textsc{ROLS} is set to 1 hour, while the maximal number of fitness function evaluations is set to 1 million. All other algorithms from SRBench used the time limit that was equal or bellow 8 hours runtime and the same fitness evaluation limit of 1 million (see~\cite{la2021contemporary}). We did not use the full time limit of 8 hours because we empirically concluded that there were almost no improvements after 1 hour \textsc{RILS}-\textsc{ROLS} runtime.

All non-deterministic algorithms (including \textsc{RILS}-\textsc{ROLS}) were run ten times per each problem instance, where each run used different setting of random number generator seed. 

\subsection{Datasets and SRBench}

SRBench (\url{https://cavalab.org/srbench/}) is an open-source benchmarking project which merges a large set of diverse benchmark datasets, contemporary SR methods as well as ML methods around a shared model evaluation and an environment for analysis, see~\cite{la2021contemporary}. SRBench considers two types of symbolic regression problems: 1) \emph{ground-truth} problems, for which the exact model is known, and 2) \emph{black-box} problems, for which the exact model is not known. In our work, we consider only the former one, since \textsc{RILS-ROLS} was not designed to solve \emph{black-box} problems. There are two groups of instances in SRBench \emph{ground-truth} problem set:
\begin{itemize}
	\item \textsc{Feynman} instances are inspired by physics and formulas/models that describes various natural laws.  
	There are 116 instances, where each one consists of $10^5$  samples (see \cite{udrescu2020ai} for more details). Some exact models (equations) of \textsc{Feynman} instances are listed in Table~\ref{tab:Feynamn-Eq}.  
	
	\begin{table}[!ht]
		\centering
		\begin{tabular}{l}   \hline
			$x = \sqrt{x_1^2 + x_2^2 - 2 x_1 x_2 \cos(\theta_1 - \theta_2)}$ \\
			$ \theta_1 = \arcsin(n \sin \theta_2)$ \\
			$E =  \frac{m c^2 }{1 - \frac{v^2}{c^2}}$ \\
			$\omega = \frac{1 + \frac{v}{c}}{ \sqrt{1 - \frac{v^2}{c^2}}} \omega_0$ \\ \hline
			
		\end{tabular}
		\caption{Some Feynman equations.}
		\label{tab:Feynamn-Eq}
	\end{table}
	
	
	\item \textsc{Strogatz} instances are introduced in \cite{la2016inference}. 
	Each instance represents a 2-state system of first-order, ordinary differential equations. 
	The aim of each problem is to predict the rate of change of the subsequent state. These equations describe natural processes modeled by non-linear dynamics exhibiting chaos.  The equations for some of the datasets that belong \textsc{Strogatz} are shown in Table~\ref{table:strogatz-ODEs}. In overall, there are 14 \textsc{Strogatz} instances. 
	
	%table, an instance -- example
	
	\begin{table}
		\centering
		\begin{tabular}{ll} \\ \hline
			Bacterial representation &   $x' = 20 - x - \frac{x \cdot y}{1 + 0.5 x^2 }$ \\ 
			&   $y' = 10 - \frac{x \cdot y}{1 + 0.5 x^2  }$ \\ \hline
			Shear Flow               &  $\theta' = \cot(\phi)\cos(\theta)$ \\
			&  $ \phi'  = ( \cos^2(\phi) + 0.1 \cdot \sin^2 (\phi)) \sin(\theta) $ \\ \hline
		\end{tabular}
		\caption{Some Strogatz ODE problems.}
		\label{table:strogatz-ODEs}
	\end{table}
	
\end{itemize}

\fxnote{ACA: do ovde sam pregledao}
The above-mentioned benchmark sets may be biased since the models which describe physical laws usually impose   various symmetries, periodicity w.r.t. some variables, internal separability on some variables etc. In that way, the search process and the search space can be pre-defined and thus significantly reduced. Thereby, some methods may get significant benefit with this regard, while some others may be in a subordinate situation. 
In order to permit any kind of favoritism caused by physical lows behind the input data, we generate a set of randomly generated (non-biased) SR problem instances; this benchmark set is therefore labelled by \textsc{Random}. These instances are generated according the following procedure. \fxnote{Opisati proceduru generisanja instanci + kombinaciju parametara za instance + koliko instanci, itd.} 

Another purpose of having these instances is  systematically  providing conclusions on robustness and scalability of the RILS-ROLS method w.r.t.\ various instance size parameters. 


\subsection{Parameter tuning}
The \textsc{RILS}-\textsc{ROLS} algorithm employs two parameters that need to be tuned, $penalty_{size}$ and $tolerance_{error}$. The first parameter is fixed empirically to 0.001(?), according to the preliminary results.  The second parameter is tuned by monitoring the two errors the algorithm produced on a randomly chosen training set of xx problem instances. Among the following values \{ x, xx, ... \} we chose $tolerance_{error}=?$ for which the largest average $R^2$ and the smallest average RMSE error on all problem instances from the training set are produced. %Note that the training set is chosen randomly with a cardinalty of xx instances. 

\fxnote{ drugi parametar manje vise zavistan od ocekivanog 
	nivoa suma -- ne znam kako obrazloziti njegov tuning (mozda prema greski na trening skupu pa izabrati onaj za koji su R2 i RMSE bolji...). }

\subsection{Comparison with other methods on the   ground--truth benchmark sets}
In this section, we evaluate our \textsc{RILS-ROLS} algorithm and compare it to the 13 other competitors from the literature. The numerical results are reported in terms of the three Tables~\ref{tab:comp_noise0}--\ref{tab:comp_noise001}. The results for the two ground-truth benchmark sets, \textsc{Feynman} and \textsc{Storgatz}, are given w.r.t.\   the following level of noise: 0.0 (no-noisy data), 0.001 (low-level noise), and 0.01 (high-level noise), respectively. In details, the white Gaussian noise is added to the target ($y$-axis) values as a fraction of the signal RMS value, applied also in~\cite{la2021contemporary}, i.e. for target noise level $\alpha$ we have
$$ y_{noise} = y + \epsilon, \epsilon \sim \mathcal{N}\left(0, \alpha \sqrt{\frac{1}{n} \sum _{i=1}^n{y_i^2}}\right),$$
where $n$ relates to the number of items in the input data.

Each table consists of three blocks. The performances of all (14) algorithms are displayed in respective rows.  
The first block gives the label of the algorithm  whose performance is reported. The second block reports by two columns the relative percentages of all (10) algorithm runs on all problem instances for which the exact model has been found by respective algorithm  on \textsc{Feynman} and \textsc{Strogatz} benchmark set, respectively. The third column of this block provides the overall   relative exact percentage of success of the algorithm over all runs on all problem instances from the both benchmark sets. The third block displays the results of the respective algorithm in terms of   final $R^2$ score which indicate  on accuracy of found solution. The solution is considered accurate if it     produced an $R^2$ score larger that  0.999.  Note that the termination of our algorithm may not always be triggered by reaching an appropriate $R^2$ score, but also due to the reached time limit. These kind of results are of a particular interest as they may point out on the presence of strong over-fitting of algorithms in contrast to the primary goal, that is finding the right model. This block consists of three columns that provide the relative percentage of algorithm runs on the instances where he algorithm is able to reach the desired accuracy (fulfilling the condition $R^2 > 0.999$ upon termination) on the both ground--truth benchmark sets separately, and the overall percentage ov all (10) runs on all considered problem instances.   %) w.r.t. the criterion of reaching the final accuracy of 0.999.

%Note that when the algorithm terminates with $R^2=1$ and $RMSE=0$, the obtained model and the exact model are always match (but also checked manually). However, in case when the algorithms deliver an error $R^2 \geq 0.999$, but $R^2 \neq 1$, these cases are all  manually checked by comparing weather the produced model match the exact model. 

In short, the exact results, thus the number of algorithm runs over all instances (in percentages) for which the output of respective algorithm is able to find the exact model, are displayed in the second block of each  table. The numbers of runs   (in percentages) for which an algorithm  delivers a precise enough model (i.e. the outcome satisfying $R^2>  0.999$) are displayed in the third block of each of the tables. 

The following conclusions may be drawn from the numerical results: 

\begin{itemize}
	\item Concerning the exact percentages in case when no noise is utilized in the input data, the best performing algorithm is our \textsc{RILS-ROLS}, able to find the solution that match to the right model on 57.84\% runs over all problems instances of the benchmark set \textsc{Feynman}; it was even more impressive in solving problem instances from benchmark set \textsc{Strogatz}, being successful on 83.57\% runs. The second best performing  approach is \textsc{AI-Feynman}, successful in 55.78\%   runs over all problem instances from \textsc{Feynman} set, and just 27.14\% runs over the problem instances from benchmark set   \textsc{Strogatz}, respectively. All other approaches are performing much worse, and none of them is able to delver the exact solution for more than 30\% runs over the instances of any of the two considered ground-truth benchmark sets. 
	\item   Concerning the exact results in presence of the   noise  level of 0.001, our \textsc{RILS-ROLS} is still performing very well, having found an exact model for 42.08\% runs considering all problem instances from the both ground-truth benchmark sets. The second best approach is \textsc{AI-Feynman} which is successful on 31.89\% runs over all problem instances; as  already noticed, this algorithm performs better when applied on the instances from \textsc{Feynman} benchmark set than it is the case of \textsc{Strogatz} benchmark set. Approach  \textsc{AFP-FE} shows to be robust w.r.t. noise so its performances only slightly deteriorate in comparison to the no-noise scenario. This puts it on the third place with overall accuracy of 21.23\%. Other approaches reach   accuracy bellow 20\%. 
	\item  Concerning the exact results when the presence of noise is large with  the noise level of 0.01,  our \textsc{RILS-ROLS} is still performing robustly, having found an exact model for  34.77\% runes over all problem instances considering the both ground-truth benchmark sets. The second best is   \textsc{AFP-FE} reaching 20\% correct solutions. \textsc{AI-Feynman} performances are degraded significantly under this level of noise, so it is ranked only 6th overall, reaching the accuracy of 12.61\%.    
	
	\item When comparing performances of the algorithms in terms of algorithm runs for which the obtained final model fulfills an accuracy of $R^2 > 0.999$, in case when no noise is integrated, our \textsc{RILS}-\textsc{ROLS} is successful with the rate of success of 83.38\% (third place). However, in contrast with the exact model percentages comparisons from above, there are a few methods reaching   better percentages of success than our algorithm: \textsc{MRGP}, and \textsc{Operon}, with 92.69\%, and 86.92\%, respectively. On the other hand, the exact model percentages of these two approaches are rather low (0\% and 16\%, respectively). These two algorithms are therefore more appropriate in scenarios when the underlying (ground-truth) model is unknown, so-called black-box regression. 
	
	\item  In the presence of the noise level of 0.001, our \textsc{RILS-ROLS} method has 78.62\% of a success rate (ranked on the third place).  As before, \textsc{MRGP} and \textsc{Operon} are showing good performances. 
	
	\item   When noise level is 0.01, the situation is almost the same, \textsc{MRGP} and \textsc{Operon} are the best and second best with 88.46\% and 86.54\% accuracy, respectively. \textsc{GP-GOMEA} now reaches third place with 73.46\%, while the proposed \textsc{RILS-ROLS} is fifth with 62.92\% $R^2$ accuracy.
	
	\item Following the above conclusions, our \textsc{RILS}-\textsc{ROLS} algorithm is more robust than the other competitors, which is confirmed by the produced results w.r.t.\ different levels of noise in the input data. Moreover, it can be considered as new state-of-the-art algorithm when one solves the problems of SR with known ground-truth models. 
	
\end{itemize}

\begin{table}[!htb]
	\caption{Comparison without noise}\label{tab:comp_noise0}
	\centering
	\begin{tabular}{l|rrr|rrr} \hline
		& \multicolumn{3}{c|}{Exact model percentage} & \multicolumn{3}{c}{$R^2 > 0.999$ percentage}\\ \hline
		Method & Feynman & Strogatz & Total & Feynman & Strogatz & Total \\ \hline
		\textsc{RILS-ROLS}&\bf{57.84}&\bf{83.57}&\bf{60.62}&82.59&90&83.38\\
		\textsc{AI-Feynman}&55.78&27.14&52.65&78.51&35.71&73.83\\
		\textsc{GP-GOMEA}&26.83&29.46&27.12&71.55&71.43&71.54\\
		\textsc{AFP-FE}&26.98&20&26.23&59.05&28.57&55.77\\
		\textsc{ITEA}&22.41&7.14&20.77&27.59&21.43&26.92\\
		\textsc{AFP}&21.12&15.18&20.48&44.83&25&42.69\\
		\textsc{DSR}&19.72&19.64&19.71&25&14.29&23.85\\
		\textsc{Operon}&16.55&11.43&16&86.21&\bf{92.86}&86.92\\
		\textsc{gplearn}&16.27&8.93&15.48&32.76&7.14&30\\
		\textsc{SBP-GP}&12.72&11.61&12.6&73.71&78.57&74.23\\
		\textsc{EPLEX}&12.39&8.93&12.02&46.98&21.43&44.23\\
		\textsc{BSR}&2.48&0.89&2.31&10.78&21.43&11.92\\
		\textsc{FEAT}&0&0.89&0.1&39.66&42.86&40\\
		\textsc{FFX}&0&0&0&0&0&0\\
		\textsc{MRGP}&0&0&0&\bf{93.1}&89.29&\bf{92.69}\\
		\hline
	\end{tabular}
\end{table}


\begin{table}[!htb]
	\caption{Comparison with noise level 0.001}\label{tab:comp_noise0001}
	\centering
	\begin{tabular}{l|rrr|rrr} \hline
		& \multicolumn{3}{c|}{Exact model percentage} & \multicolumn{3}{c}{$R^2 > 0.999$ percentage}\\ \hline
		Method & Feynman & Strogatz & Total & Feynman & Strogatz & Total \\ \hline
		\textsc{RILS-ROLS}&\bf{42.93}&\bf{35}&\bf{42.08}&80.26&65&78.62\\
		\textsc{AI-Feynman}&33.08&22.14&31.89&77.39&42.86&73.64\\
		\textsc{AFP-FE}&21.9&15.71&21.23&52.16&35.71&50.38\\
		\textsc{DSR}&19.14&19.29&19.15&25.86&14.29&24.62\\
		\textsc{AFP}&19.66&13.57&19&44.4&21.43&41.92\\
		\textsc{gplearn}&16.99&9.29&16.16&31.9&7.14&29.23\\
		\textsc{ITEA}&14.57&7.14&13.77&27.59&21.43&26.92\\
		\textsc{Operon}&13.19&5&12.31&85.78&\bf{92.86}&86.54\\
		\textsc{GP-GOMEA}&11.03&7.14&10.62&70.26&71.43&70.38\\
		\textsc{EPLEX}&9.57&9.29&9.54&47.84&25&45.38\\
		\textsc{SBP-GP}&0.78&0&0.69&75.43&64.29&74.23\\
		\textsc{BSR}&0.6&0.71&0.62&10.34&14.29&10.77\\
		\textsc{FFX}&0&0&0&0&0&0\\
		\textsc{FEAT}&0&0&0&42.24&50&43.08\\
		\textsc{MRGP}&0&0&0&\bf{92.24}&85.71&\bf{91.54}\\
		\hline
	\end{tabular}
\end{table}


\begin{table}[!htb]
	\caption{Comparison with noise level 0.01}\label{tab:comp_noise001}
	\centering
	\begin{tabular}{l|rrr|rrr} \hline
		& \multicolumn{3}{c|}{Exact model percentage} & \multicolumn{3}{c}{$R^2 > 0.999$ percentage}\\ \hline
		Method & Feynman & Strogatz & Total & Feynman & Strogatz & Total \\ \hline
		\textsc{RILS-ROLS}&\bf{36.29}&\bf{22.14}&\bf{34.77}&64.91&46.43&62.92\\
		\textsc{AFP-FE}&20.78&13.57&20&52.59&32.14&50.38\\
		\textsc{DSR}&18.97&18.57&18.92&26.29&14.29&25\\
		\textsc{AFP}&16.9&11.43&16.31&42.67&21.43&40.38\\
		\textsc{gplearn}&16.87&9.29&16.05&29.13&10.71&27.13\\
		\textsc{AI-Feynman}&13.03&9.29&12.61&71.43&39.29&67.86\\
		\textsc{EPLEX}&8.71&4.29&8.23&56.03&21.43&52.31\\
		\textsc{ITEA}&7.84&6.43&7.69&27.59&21.43&26.92\\
		\textsc{GP-GOMEA}&5.09&1.43&4.69&73.71&71.43&73.46\\
		\textsc{Operon}&2.07&0.71&1.92&85.78&\bf{92.86}&86.54\\
		\textsc{BSR}&0.09&0&0.08&12.07&10.71&11.92\\
		\textsc{SBP-GP}&0&0&0&75&75&75\\
		\textsc{FEAT}&0&0&0&40.52&42.86&40.77\\
		\textsc{FFX}&0&0&0&2.59&3.57&2.69\\
		\textsc{MRGP}&0&0&0&\bf{91.81}&60.71&\bf{88.46}\\
		\hline
	\end{tabular}
\end{table}

%time-avg-noise0.0.eps

\begin{figure}[!ht]
	
	
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[  width=220pt,height=150pt]{plots/time-avg-noise0.0.eps}
		\caption{No noise}
		\label{fig:noNoise-time}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		
		\includegraphics[  width=220pt,height=150pt]{plots/time-avg-noise0.001.eps}
		\caption{Level of noise equal to 0.001}
		\label{fig:noise0.001-time}
	\end{subfigure}
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[  width=220pt,height=150pt]{plots/time-avg-noise0.01.eps}
		\caption{Level of noise equal to 0.01.}
		\label{fig:noise0.01-time}
	\end{subfigure}
	\caption{Average runtimes on the instance problems where the exact solution has  been reached (considered only those algorithms whose the exact percentage is at least 5\% of all runs).}
	\label{fig:time-avg-exact-noise}
\end{figure}

In Figure~\ref{fig:time-avg-exact-noise}, average runtimes of the algorithms on those instance problems from the both ground-truth benchmark sets for which respective exact model has  been found, are compared. The algorithms which obtained an exact percentage of at least 5\% (considering all 1300 runs) are considered as relevant  and therefore included here, while others are just omitted.  The runtimes of each algorithm  are averaged w.r.t\  different levels of noise, and presented by a bar plot. Algorithms are labeled on $x$-axis, while respective average runtimes (oer all runs) are shown on $y$-axis. Note that $y$-axis is logarithmically scaled. The following conclusions are drawn from here.

\begin{itemize}
	\item When considering the instance problems with no-noisy input data, the lowest average runtime  is reported for \textsc{RILS}-\textsc{ROLS} algorithm (228$s$). A slightly worse runtime is obtained by \textsc{Dsr} algorithm (232$s$).  However, note that the relative exact percentage produced by \textsc{RILS}--\textsc{ROLS} is far better from \textsc{Dsr} (60.62\% vs. 19.71\%). 	All remaining approaches deliver an order of magnitude larger average runtimes than that of the \textsc{RILS}--\textsc{ROLS} method in case of runs when the exact model has been reached.   
	\item  When considering the instance problems with the noise level of 0.001,    the lowest average runtime  to find the exact model is again delivered by \textsc{RILS}-\textsc{ROLS} algorithm (108$s$). The second best average runtime is produced by \textsc{Dsr}, but this time significantly larger (622$s$) from the former algorithm. 
	\item When considering the instance problems with the noise level of 0.01, a similar conclusion may be drawn as that from above. 
\end{itemize}



\subsection{Statistical evaluation}

In order to check the statistical significance of the results w.r.t.\ obtained exact models and their differences between the competitor approaches, we employed the statistical methodology comparing the results of \textsc{RILS}--\textsc{ROLS} to 13 other approaches over all algorithm runs on the problem instances from the both ground-truth benchmark sets, \textsc{Feynman} and \textsc{Strogatz}. Note that an indicator random variable is involved for each run of each algorithm. More precisely, the score that comes into the statistical observation is an indicator (0 or 1) indicating weather or not an algorithm was able to return the exact model at specific run. More precisely, we involved 1300 results into our statistical evaluation per each algorithm. Note that  results of some of the 13 competitors are not known from SRBench \url{https://cavalab.org/srbench/results/} due to   unknown reasons (most likely algorithm is terminated running out of memory or due to   chosen algorithm configuration settings). For these runs, we simply indicate a 0 as their score.   

Initially, Friedman’s test was separately executed for all competitor approaches.      In those cases in which the null hypothesis $H_0$ was rejected ($H_0$ states that there are no statistical differences between the obtained results of all competitors) pairwise comparisons are further performed by using the Nemenyi post-hoc test~\cite{pohlert2014pairwise}. The outcome is represented by means of critical difference (CD) plots. In each CD plot, the (14) competitor approaches are placed on the horizontal axis according to their average ranking. Thereafter, the CD score is computed for a significance level of 0.05. If the difference is small enough, meaning that no statistical difference is detected, a horizontal bar linking statistically equal approaches is drawn.   Three (one for each level of noise) CD plots are shown in Figure~\ref{fig:cd-plots-exact-pcts} per each level of noise. The following conclusions may be drawn from there.

\begin{itemize}
	\item  Concerning the instance problems where no noise is included, \textsc{RILS}-\textsc{ROLS} method returns the best average ranking in terms of the number of obtained exact models over all runs.  The second best average ranking is achieved by \textsc{AI-Feynman} algorithm. There is  statistically significant difference between these two approaches w.r.t.\ number of found exact solutions. The third best approach is \textsc{Apf-Fe},  which is far behind \textsc{RILS}-\textsc{ROLS} w.r.t.\ delivered average ranking.  
	\item    Concerning the instance problems with the included  level of noise of 0.001, \textsc{RILS}-\textsc{ROLS} method again obtains the best average ranking in terms of the found exact solutions.   The second best ranking is obtained by \textsc{AI-Feynman} algorithm. There is again a statistically significant difference between the results obtained by these two approaches. The third best approach is \textsc{Afp-Fe} which performs statistically worse than \textsc{AI-Feynman}.  
	\item  Concerning the instance problems with the included noise level of  0.01, again, \textsc{RILS-ROLS} method produces the best average ranking according to positively evaluated runs, i.e. those runs with found  exact models; the second best approach w.r.t.\ average ranking is \textsc{Afp-Fe}, followed by \textsc{Dsr}, and \textsc{Afp}. The  latter three approaches perform statistically equal, but significantly worse than \text{RILS}-\textsc{ROLS} method. 	The fourth best approach according to delivered average ranking is \textsc{AI-Feynman} which is on pair with \textsc{gplearn}. Note that the average ranking of \textsc{AI-Feynman} method declines by increasing  the level of noise in the input data. Thus, despite being efficient when comes to the no-noisy data,  \textsc{AI-Feynman} shows to be very sensitive w.r.t.  increase of the level of noise, which is not that dramatically conspicuous with our \textsc{RILS}-\textsc{ROLS} method.   
	
\end{itemize}


\begin{figure}[!ht]
	 	\centering
	\begin{subfigure}[b]{0.6\textwidth} 
		\includegraphics[  width=340pt,height=180pt]{plots/Feynman_strogatz_noise_0_0-expanded.pdf}
		\caption{No noise}
		\label{fig:CDplots-no-noise}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.6\textwidth}
		
		\includegraphics[  width=340pt,height=180pt]{plots/Feynman_strogatz_noise_0_001-expanded.pdf}
		\caption{Level of noise equal to 0.001}
		\label{fig:CDplots-noise0.001}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.6\textwidth}
		\includegraphics[  width=340pt,height=180pt]{plots/Feynman_strogatz_noise_0_01-expanded.pdf}
		\caption{Level of noise equal to 0.01. }
		\label{fig:CDplots-noise0.01}
	\end{subfigure}
	\caption{CD plots comparisons in terms of exact percentages over all problem instances from the both ground-truth benchmark sets,  varying levels of noise.  }
	\label{fig:cd-plots-exact-pcts}
\end{figure}


\subsection{Scalability of \textsc{RILS}-\textsc{ROLS} algorithm}\label{sec:scalability-rils-rols}

In this section we study the scalability of our method in terms of the size of  instance problems and  the presence of different noise levels in the input data.  First, for the shake of simplicity of presenting,  we split the instances from \textsc{Random} benchmark sets into three parts as follows. 
\begin{itemize}
	\item \textit{Small--sized instances}: any instance  from \textsc{Random} benchmark whose   corresponding exact model  tree representation  has from 3 to 7 nodes belongs to this sub-set.
	\item \textit{Medium--sized instances}:  any instance  from \textsc{Random} benchmark whose   corresponding exact model tree representation  has from 8 to 11 nodes belongs to this sub-set.
	\item \textit{Large--sized instances}: any instances from \textsc{Random} benchmark whose   corresponding exact model  tree representation  has from 12 to 15 nodes belongs to this sub-set. 
\end{itemize}

The results are displayed in Figure~\ref{fig:compExact_noise_size}, grouped with accordance to the above (three) formed sub-groups ($x$-axis); for each of them relative exact percentage rate   of \textsc{RILS}--\textsc{ROLS} algorithm is shown ($y$-axis). The following conclusions may be drawn from there. 

\begin{itemize}
	\item   As expected, the exact percentage success rate is the highest for the small--sized instance problems; it is sightly bellow 90\% (over all XXx runs) in case of no-noisy data. For the noisy data, the exact success rate is getting decreased as the level of noise increases. For example, on the small-sized problem instances with the noise level  of 0.0001, and 0.01, the exact percentage achieved by \textsc{RILS}--\textsc{ROLS} algorithm is still reasonably high, at about 55\%~ and 34 \%, respectively. 
	\item For the medium-sized instance problems where noise is not presented, the exact percentage rate of \textsc{RILS}-\textsc{ROLS} is slightly bellow 50\%. It also decreases by increasing the level of noise in the target data; for the highest level of noise (of 0.01), the success rate of \textsc{RILS}-\textsc{ROLS} method is just at about 6\%. 
	\item For the large-sized instance problems where no noise is utilized, the exact percentage rate of \textsc{RILS}--\textsc{ROLS} algorithm is at about 25\%, which means that the increase in the size of exact models affects the algorithm's performance, but not dramatically.  
	
	\item From the above, we can conclude that the size of exact models (solutions) and the increase of   level of noise both contribute to overall performance of our \textsc{RILS}--\textsc{ROLS} method. However, the exact percentage rate of the algorithm seems to decrease linearly  in the size of (exact) model, regardless of the noise level values.
	
	\item Concerning  the percentage of  runs over all random instances for which \textsc{RILS-ROLS} terminates by producing an accurate model, that is a model with $R^2 > 0.999$, for the small--sized problem instances where no noise is included, \textsc{RILS}-\textsc{ROLS} delivers  the perfect score almost. In the presence of a low noise level (equals to 0.001), this percentage  is still holding high, at about 95\%. In case when the high level of noise (equals to 0.01) is presented in the input data, these percentages stagnate, but still are at a respectable 66\%. %small-sized instances it reaches a high--precision of $R^2=0.999$ upon termination.   
	As  pointed out already, for a half of these cases the respectable exact models were found by the algorithm.
	
	\item Concerning  the medium-sized instance problems when the input data are free of the noise,   the final outcome reached the high precision in 65\% of all algorithm runs.    The noisy the data are, these percentages, as expected, drop off; however, it is not so dramatically. For the level of noise of 0.01, the high--accuracy of the final solution is reported on $\approx$30\% of all runs.
	
	\item Concerning  the large-sized instance problems with no noise, the  final solution reached the high precision in 60\% of all runs. Not so surprisingly, the hardest case to obtain the desired high--accuracy (that is, ensuring $R^2> 0.999$ for  the  outcome) is in the presence of a high level of noise (of 0.01), where the algorithm was successful on about 20\% of all runs concerning the large--sized instance problems. 
	
\end{itemize}

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel=\textbf{model size},
			ylabel=\textbf{exact percentage},
			xmin=0, xmax=4,
			ymin=0, ymax=100,
			xtick={1,2,3},
			xticklabels={small,medium,large},   % <---
			ytick={0,10,...,100}
			]
			\addplot[smooth,mark=*,blue] plot coordinates {
				(1,85)
				(2,42.5)
				(3,21.05)
			};
			\addlegendentry{$\alpha=0.00$}
			
			\addplot[smooth,color=red,mark=x]
			plot coordinates {
				(1,51.67)
				(2,13.75)
				(3,5.26)
			};
			\addlegendentry{$\alpha=0.001$}
			
			\addplot[smooth,color=green,mark=o]
			plot coordinates {
				(1,31.67)
				(2,5)
				(3,4.21)
			};
			\addlegendentry{$\alpha=0.01$}
		\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Exact solution percentages for varying levels of noise and formulae sizes}
	\label{fig:compExact_noise_size}
\end{center}

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
			xlabel=\textbf{model size},
			ylabel=\textbf{$R^2 > 0.999$ percentage},
			xmin=0, xmax=4,
			ymin=20, ymax=100,
			xtick={1,2,3},
			xticklabels={small,medium,large},   % <---
			ytick={20,30,...,100}
			]
			\addplot[smooth,mark=*,blue] plot coordinates {
				(1,98.33)
				(2,62.5)
				(3,58.95)
			};
			\addlegendentry{$\alpha=0.00$}
			
			\addplot[smooth,color=red,mark=x]
			plot coordinates {
				(1,93.33)
				(2,53.75)
				(3,40)
			};
			\addlegendentry{$\alpha=0.001$}
			
			\addplot[smooth,color=green,mark=o]
			plot coordinates {
				(1,65)
				(2,31.25)
				(3,20)
			};
			\addlegendentry{$\alpha= 0.01$}
		\end{axis}
	\end{tikzpicture}
	\captionof{figure}{Percentages of solutions having $R^2 > 0.999$ for varying levels of noise and formulae sizes}
	\label{fig:compR2_noise_size}
\end{center}

In the rest of the section,  we investigate scalability of our \textsc{RILS}--\textsc{ROLS} algorithm in terms of the number of variables and sizes of corresponding exact models of the problem instances varying levels of noise. There are three bar plots generated per each level of noise, shown by Figures~\ref{fig:compExact_noise_varcnt}--\ref{fig:compExact_noise_varsizes}. The instances are grouped according to the different variable count/size of respective exact models ($x$-axis) that correspond to them. For each group, the exact percentages obtained by \textsc{RILS}--\textsc{ROLS} algorithm group are shown on $y$-axis.

The following conclusions can be drawn from Figure~\ref{fig:compExact_noise_varcnt}. 

\begin{itemize}
	\item For the no-noisy data, the exact percentage rate of \textsc{RILS}--\textsc{ROLS} algorithm  slightly decrease as the expected number of variables increases in the exact models. 
	\item The exact percentages drop off with the presence of a larger noise level. As expected, the higher the noise level, the smaller the exact percentages are.
	\item Interestingly, in the presence of noise in the data, the highest exact percentages are not obtained for the problem instances which have exact models consisted of a single variable, but two or three. We argue it by the fact that the early phase of the algorithm will more likely trying to include additional variables in current solution at the cost of increasing the model accuracy to ensure improving the fitness value over iterations. For the no--noisy data, OLS method will more likely find a small-sized high-accuracy model (in our experiments that happened on $\approx$ 50\% runs) that fits to the (no-noisy) input data than the case of noisy data. When the data is noisy, the decision of adding more variables at the beginning is likely to be forced than just dealing with short-size solutions. %By the progression of the algorithm, it gets harder improving the best fitness value by adding variables, and other operations are more frequently involved than in previous iterations. %This conclusion also correlates to the shape of fitness function given by Equation~(\ref{eq:fitness}) where the first two product terms there are already small (close to 1).      \fxnote{TODO: need to be further expanded...}
	%Obtaining an accurate model with a few variables in the presence of a high level of nose in the data 
\end{itemize}

\begin{figure}[!ht]
	
	
	\begin{subfigure}[b]{0.40\textwidth}
		\includegraphics[  width=180pt,height=140pt]{plots/numvars_vs_exact_correct_no_noise.eps}
		\caption{No noise}
		\label{fig:noNoise}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.4\textwidth}
		
		\includegraphics[  width=180pt,height=140pt]{plots/numvars_vs_exact_correct_noise0_001.eps}
		\caption{Level of noise equal to 0.001}
		\label{fig:noise0.001}
	\end{subfigure}
	\centering
	\begin{subfigure}[b]{0.40\textwidth}
		\includegraphics[  width=180pt,height=140pt]{plots/numvars_vs_exact_correct_noise0_01.eps}
		\caption{Level of noise equal to 0.01.}
		\label{fig:noise0.01}
	\end{subfigure}
	\caption{Exact solution percentages for varying levels of noise and variable counts.}
	\label{fig:compExact_noise_varcnt}
\end{figure}

The  following conclusions may be drawn from Figure~\ref{fig:compExact_noise_varsizes}.
\begin{itemize}
	\item The exact percentages are decreased with increase in  the size of exact models, in case of any level of noise utilized on the target values.  %within the input data.
	\item The highest exact percentages, which are larger than 70\%, are noticed in case of the small-sized problem instances  (the size of models ranges from 3 to 7). However, for the same sizes, for noisy data, these percentages are significantly lower; for example, in case of the instances whose exact models are of size 7 when utilizing the level of noise of 0.001 and 0.01, the exact percentages obtained are 20\% and 10\%, respectively. 
	
	\item   For the problem instances with the largest size of (exact) models in case of the no noisy data, exact percentages are never lower than 20\%. The opposite holds for noisy data. 
\end{itemize}

\begin{figure}[H]
	\begin{subfigure}[b]{0.40\textwidth}
		\includegraphics[  width=180pt,height=140pt]{plots/numsize_vs_exact_correct_no_noise.eps}
		\caption{No noise}
		\label{fig:size-no-noise}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.4\textwidth}
		
		\includegraphics[  width=180pt,height=140pt]{plots/numsize_vs_exact_correct_noise0_001.eps}
		\caption{Level of noise equal to 0.001}
		\label{fig:size-noise0.001}
	\end{subfigure}
	\centering
	\begin{subfigure}[b]{0.40\textwidth}
		\includegraphics[  width=180pt,height=140pt]{plots/numsize_vs_exact_correct_noise0_01.eps}
		\caption{Level of noise equal to 0.01.}
		\label{fig:size-noise0.01}
	\end{subfigure}
	\caption{Exact solution percentages for varying levels of noise and exact model sizes.}
	\label{fig:compExact_noise_varsizes}
\end{figure}

When one concerns of the runtime analysis of \textsc{RILS}--\textsc{ROLS} algorithm on the \textsc{Random} benchmark set w.r.t\ number of variables, the results are displayed in Figure~\ref{fig:runtime_rils_rols}. The instances are grouped w.r.t.\ number of variables involved into corresponding exact models ($x$-axis). The following conclusions may be drawn from there.  

\begin{itemize}
	\item As one could expect, average runtime is getting increased with the increase of the number of variables in exact models. 
	\item Note that the average runtime in case of the instances whose exact models have the largest number of variables (5) is just about 200$s$.
	\item The two above-mentioned points indicate efficacy of our method especially in case of the instances with small-to-medium sized exact models in the presence of small--to--middle noise levels included in the input data. In these cases, the algorithm is able to deliver a reasonable exact percentage rate, which are always larger than 50\%. Note that models which have more compact representations (thus, small in their size) are desirable in many areas of natural sciences as they are easy interpret-able than models represented by a more complex mathematical formula.   
\end{itemize}

\begin{figure}[H]
	\begin{subfigure}[b]{0.40\textwidth}
		\includegraphics[  width=180pt,height=140pt]{plots/numsize_vs_time_no_noise.eps}
		\caption{No noise}
		\label{fig:time-no-noise}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.4\textwidth}
		
		\includegraphics[  width=180pt,height=140pt]{plots/vars_vs_time_noise0_001.eps}
		\caption{Level of noise equal to 0.001}
		\label{fig:time-noise0.001}
	\end{subfigure}
	\centering
	\begin{subfigure}[b]{0.40\textwidth}
		\includegraphics[  width=180pt,height=140pt]{plots/vars_vs_time_noise0_01.eps}
		\caption{Level of noise equal to 0.01.}
		\label{fig:time-noise0.01}
	\end{subfigure}
	\caption{Average runtime comparisons for varying levels of noise and variable counts.}
	\label{fig:runtime_rils_rols}
\end{figure}

\begin{comment}
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				xlabel=$variable\;count\;in\;exact\;models$,
				ylabel=$exact\;percentage$,
				xmin=0, xmax=5,
				ymin=0, ymax=100,
				xtick={1,2,3,4},
				xticklabels={1,2,3,4},   % <---
				ytick={0,10,...,100}
				]
				\addplot[smooth,mark=*,blue] plot coordinates {
					(1,43.33)
					(2,43.33)
					(3,46.67)
					(4,46.67)
				};
				\addlegendentry{No noise}
				
				\addplot[smooth,color=red,mark=x]
				plot coordinates {
					(1,3.33)
					(2,3.33)
					(3,26.67)
					(4,26.67)
				};
				\addlegendentry{Noise level 0.001}
				
				\addplot[smooth,color=green,mark=o]
				plot coordinates {
					(1,3.33)
					(2,3.33)
					(3,3.33)
					(4,16.67)
				};
				\addlegendentry{Noise level 0.01}
			\end{axis}
		\end{tikzpicture}
		\captionof{figure}{Exact solution percentages for varying levels of noise and variable counts (only sizes ranging from 7 to 12)}
		\label{fig:compExact_noise_varcnt}
	\end{center}
\end{comment}
%\begin{center}
%	\begin{tikzpicture}
	%		\begin{axis}[
		%			xlabel=$variable\;count$,
		%			ylabel=$exact\;percentage$,
		%			xmin=0, xmax=5,
		%			ymin=0, ymax=100,
		%			xtick={1,2,3,4},
		%			xticklabels={1,2,3,4},   % <---
		%			ytick={0,10,...,100}
		%			]
		%			\addplot[smooth,mark=*,blue] plot coordinates {
			%				(1,83.33)
			%				(2,66.67)
			%				(3,73.33)
			%				(4,46.67)
			%			};
		%			\addlegendentry{No noise}
		%			
		%			\addplot[smooth,color=red,mark=x]
		%			plot coordinates {
			%				(1,56.67)
			%				(2,63.33)
			%				(3,66.67)
			%				(4,46.67)
			%			};
		%			\addlegendentry{Noise level 0.001}
		%			
		%			\addplot[smooth,color=green,mark=o]
		%			plot coordinates {
			%				(1,23.33)
			%				(2,43.33)
			%				(3,53.33)
			%				(4,26.67)
			%			};
		%			\addlegendentry{Noise level 0.01}
		%		\end{axis}
	%	\end{tikzpicture}
%	\captionof{figure}{Percentages of solutions having $R^2 > 0.999$ for varying levels of noise and variable counts (only sizes ranging from 7 to 12)}
%	\label{fig:compR2_noise_varcnt}
%\end{center}





\section{Conclusions and future work}\label{sec:conclusions}

In this paper, we dealt with solving the well--known Symbolic regression (SR)  problem. SR problem is a generalization of more interdisciplinary known problems such as linear and logistic regression.    
We proposed a meta-heuristic approach, called \textsc{RILS}-\textsc{ROLS}, which is built upon the Iterated local search (ILS) scheme. The first crucial concept within this scheme is utilizing the ordinary least square method to efficiently determine best-fitting internal coefficients to the candidate solution. % Thus, the search process is primarily focused on considering already promising solutions in the continuous part of the search space rather than applying enhanced heuristics to search for those coefficients. 
Another key aspect is the utilization of a carefully constructed fitness function in the search, which appeared as a combination of three important scores: RMSE, $R^2$  scores and the size of the model (solution).  Last but not least, a carefully constructed local search is applied systematically exploring solutions obtained as 1--perturbations of the considered solution, represented as trees.  1--perturbations of a solution are based on applying the six per-node change operations on the solution's tree representation. 

\textsc{RILS-ROLS} method is compared to the 14 other competitor algorithms from the literature on the two ground--truth benchmark sets from the literature, namely \textsc{Feynman} and \textsc{Strogatz}.  Additionally, we introduced a non-biased benchmark set of randomly generated instances, labelled by \textsc{Random}.  The experimental evaluation confirmed the efficiency of our method which produced the best average ranking results in terms of exact percentage when compared to all other competitors on the two ground-truth benchmarks in the presence of all three different levels of noise in the input data. In all cases, new best exact percentages are obtained by our \textsc{RILS-ROLS} method.   More in details,
our algorithm was able to obtain the right model on 60.62\%, 42.08\%, and 34.77\% runs over all considered problem instances in the presence of different noise levels: 0.0, 0.001, and 0.01, respectively. For the shake of comparisons, the second best approach \textsc{AI-Feynman}, when the noise level of 0.0 and 0.001 is included, obtains the exact percentages of 52.65\%, and 31.89\%, respectively. In case of the highest noise level, the second best approach was \textsc{Afp-Fe}, obtaining an exact percentage rate of 20\%. Additionally, the scalability and robustness of \textsc{RILS}-\textsc{ROLS} method are checked and confirmed on the unbiased \textsc{Random} benchmark set in terms of a few different criteria such as the size of the model, the number of variables included in exact models, and obtained average runtimes. 

In future work, one could think of constructing a hybrid of \textsc{RILS-ROLS} method with some other meta-heuristics to further boost the quality of the obtained results. For example, replacing ILS scheme of \textsc{RILS-RILS} with a more general Variable neighbourhood search (VNS) scheme is a reasonable option to give it a try. VNS   utilizes a more stronger and systematic system of solutions diversification and thus escaping from local optima could be much easier and the convergence much faster.  Additionally, as proof of the efficiency and usefulness of our algorithm, this tool could be applied to solving problems from various problem domains out of the area of computer science, for example from physics or chemistry, to help for setting up reasonable hypotheses or to obtain valuable insights on obtained experimental evaluations.  


\newpage
%\section*{References}
\bibliographystyle{abbrv}	
\bibliography{bib}	

\newpage
\appendix

\section{Overview of \textsc{RILS}-\textsc{ROLS} python package}\label{sec:appendix-1}

\textsc{RILS-ROLS} algorithm is available for install on the well-known Python package repository \url{https://pypi.org}, so it can be easily installed by typing commands:
\begin{python} 
	pip install rils-rols
\end{python}
\textsc{RILS-ROLS} project page is at \url{https://pypi.org/project/rils-rols}. Here, one can find a minimal working example on how-to-use \textsc{RILS-ROLS}:

\begin{python}
from rils_rols.rils_rols import RILSROLSRegressor
from math import sin, log

regr = RILSROLSRegressor()

# toy dataset 
X = [[3, 4], [1, 2], [-10, 20], [10, 10], [100, 100], [22, 23]]
y = [sin(x1)+2.3*log(x2) for x1, x2 in X]

# RILSROLSRegressor inherits BaseEstimator (sklearn)
regr.fit(X, y)

# this prints out the learned model
print("Final model is "+str(regr.model))

# applies the model to a list of input vectors
X_test = [[4, 4], [3, 3]]
y_test = regr.predict(X_test)
print(y_test) 
\end{python}

Python sources, experimental results and all other \textsc{RILS-ROLS} resources can be found under the project GitHub page \url{https://github.com/kartelj/rils-rols}. 

%\section{Additional statistical analysis}\label{sec:appendix-2}



%\begin{figure}[H]
%	\begin{subfigure}[b]{0.40\textwidth}
%		\includegraphics[  width=220pt,height=140pt]{plots/Feynman__strogatz_noise_0_0acc.pdf}
%		\caption{No noise}
%		\label{fig:CDplotsR2-no-noise}
%	\end{subfigure}
%	\hfill
%	\begin{subfigure}[b]{0.4\textwidth}
%		
%		\includegraphics[  width=220pt,height=140pt]{plots/Feynman__strogatz_noise_0_001acc.pdf}
%		\caption{Level of noise equal to 0.001}
%		\label{fig:CDplotsR2-noise0.001}
%	\end{subfigure}
%	\centering
%	\begin{subfigure}[b]{0.40\textwidth}
%		\includegraphics[  width=220pt,height=140pt]{plots/Feynman__strogatz_noise_0_01acc.pdf}
%		\caption{Level of noise equal to 0.01.}
%		\label{fig:CDplotsR2-noise0.01}
%	\end{subfigure}
%	\caption{CD plots comparisons in terms of obtained $R^2$ score over all problem instances from both ground-truth benchmark sets,  varying levels of noise.}
%	\label{fig:cd-plotsR2-varying_noise}
%\end{figure}

\end{document}
